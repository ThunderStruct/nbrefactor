{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seNHGHcuX8gP"
      },
      "source": [
        "\n",
        "## Imports\n",
        "\n",
        "<!---  \n",
        "$ignore-module=True\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWh-pq6pYj3j"
      },
      "outputs": [],
      "source": [
        "\"\"\" Imports\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import sys\n",
        "import abc\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "import psutil\n",
        "import shutil\n",
        "import inspect\n",
        "import logging\n",
        "import textwrap\n",
        "import argparse\n",
        "import itertools\n",
        "import subprocess\n",
        "from typing import *\n",
        "from enum import Enum, auto\n",
        "from datetime import datetime\n",
        "from copy import copy, deepcopy\n",
        "from functools import partial, lru_cache\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from matplotlib.path import Path\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import FancyBboxPatch, Circle, PathPatch\n",
        "\n",
        "from google.colab import runtime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGTtRW5oFjgM"
      },
      "source": [
        "\n",
        "## Utilities\n",
        "\n",
        "<!---\n",
        "$module=utilities\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ItaEcTkN7bY"
      },
      "source": [
        "\n",
        "#### Config\n",
        "\n",
        "<!---  \n",
        "$file=config.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avsW6rCrN9iB"
      },
      "outputs": [],
      "source": [
        "\"\"\" Static Global Configuration\n",
        "\"\"\"\n",
        "\n",
        "class Config:\n",
        "\n",
        "    # BASE_PATH = ''\n",
        "    BASE_PATH = '/content/gdrive/MyDrive/PhD/NASKit/'\n",
        "\n",
        "    EXPERIMENT_NAME = 'ext'\n",
        "\n",
        "    MOUNT_GDRIVE = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TodxubsVFmhK"
      },
      "source": [
        "### Argument Parser\n",
        "\n",
        "<!---  \n",
        "$file=params.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppeC7uO2FlfP"
      },
      "outputs": [],
      "source": [
        "\"\"\" Argument Parsing\n",
        "\"\"\"\n",
        "\n",
        "class Params:\n",
        "\n",
        "    PARSER = None\n",
        "    ARGS = None\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize():\n",
        "\n",
        "        Params.PARSER = argparse.ArgumentParser()\n",
        "        Params.__set_args()\n",
        "\n",
        "        Params.ARGS = Params.PARSER.parse_args(args=[])\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def __set_args():\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        Params.PARSER.add_argument('--num-vertices', '-nv',\n",
        "                                   dest='num_vertices',\n",
        "                                   nargs='?',\n",
        "                                   const=8,\n",
        "                                   default=8,\n",
        "                                   type=int,\n",
        "                                   choices=range(1, 10),\n",
        "                                   help=(\n",
        "                                       'Number of vertices in generated '\n",
        "                                       'architectures'\n",
        "                                       )\n",
        "        )\n",
        "\n",
        "        Params.PARSER.add_argument('--arch-encoding', '-ae',\n",
        "                                   dest='arch_encoding',\n",
        "                                   nargs='?',\n",
        "                                   const='multi-branch',\n",
        "                                   default='multi-branch',\n",
        "                                   type=str,\n",
        "                                   choices=['single-branch', 'multi-branch'],\n",
        "                                   help=(\n",
        "                                      'Search space encoding; '\n",
        "                                      'single-branch = path-based encoding, '\n",
        "                                      'multi-branch = adjacency-matrix encoding'\n",
        "                                      )\n",
        "        )\n",
        "\n",
        "        Params.PARSER.add_argument('--nas-epochs', '-ne',\n",
        "                                   dest='nas_epochs',\n",
        "                                   nargs='?',\n",
        "                                   const=50,\n",
        "                                   default=50,\n",
        "                                   type=int,\n",
        "                                   choices=range(1, 150),\n",
        "                                   help=(\n",
        "                                       'Number of epochs for the NAS '\n",
        "                                       'optimization loop (# of evaluated '\n",
        "                                       'architectures)'\n",
        "                                       )\n",
        "        )\n",
        "\n",
        "        Params.PARSER.add_argument('--train-epochs', '-te',\n",
        "                                   dest='train_epochs',\n",
        "                                   nargs='?',\n",
        "                                   const=5,\n",
        "                                   default=5,\n",
        "                                   type=int,\n",
        "                                   choices=range(1, 200),\n",
        "                                   help=(\n",
        "                                       'Number of training epochs per '\n",
        "                                       'architecture'\n",
        "                                       )\n",
        "        )\n",
        "\n",
        "        Params.PARSER.add_argument('--learning-rate', '-lr',\n",
        "                                   dest='learning_rate',\n",
        "                                   nargs='?',\n",
        "                                   const=0.001,\n",
        "                                   default=0.001,\n",
        "                                   type=float,\n",
        "                                   choices=range(0, 1),\n",
        "                                   help=(\n",
        "                                       'Learing rate for generated '\n",
        "                                       'architectures'\n",
        "                                       )\n",
        "        )\n",
        "\n",
        "        Params.PARSER.add_argument('--results-filepath', '-rf',\n",
        "                                   dest='results_filepath',\n",
        "                                   type=str,\n",
        "                                   nargs='?',\n",
        "                                   default='./results/',\n",
        "                                   const='./results/',\n",
        "                                   help=(\n",
        "                                       'Filepath and filename to save the NAS '\n",
        "                                       'results to'\n",
        "                                       )\n",
        "        )\n",
        "\n",
        "        Params.PARSER.add_argument('--save-training-logs', '-stl',\n",
        "                                   dest='save_training_logs',\n",
        "                                   nargs='?',\n",
        "                                   const=True,\n",
        "                                   default=True,\n",
        "                                   type=bool,\n",
        "                                   help=(\n",
        "                                       'Sets whether or not to save training '\n",
        "                                       'history log files'\n",
        "                                       )\n",
        "        )\n",
        "\n",
        "        Params.PARSER.add_argument('--verbose', '-v',\n",
        "                                   dest='verbose',\n",
        "                                   nargs='?',\n",
        "                                   const=True,\n",
        "                                   default=True,\n",
        "                                   type=bool,\n",
        "                                   help=(\n",
        "                                       'Sets whether or not to log debug and '\n",
        "                                       'warning details'\n",
        "                                       )\n",
        "        )\n",
        "\n",
        "\n",
        "    def get_args(*args):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        if not args or len(args) == 0:\n",
        "            return vars(Params.ARGS)\n",
        "\n",
        "        ret_dict = {}\n",
        "\n",
        "        for key, val in vars(Params.ARGS).items():\n",
        "            if key in args:\n",
        "                ret_dict[key] = val\n",
        "\n",
        "        return ret_dict\n",
        "\n",
        "\n",
        "Params.initialize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLQup1Bw7ibQ"
      },
      "source": [
        "### Logger\n",
        "\n",
        "<!---  \n",
        "$file=logger.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJeA1NNW7ket"
      },
      "outputs": [],
      "source": [
        "\"\"\"NASKit's Custom Logger and Formatter\n",
        "\"\"\"\n",
        "\n",
        "class _ColoredFormatter(logging.Formatter):\n",
        "    \"\"\"\n",
        "    Custom color formatter for the :class:`~Logger` class\n",
        "    \"\"\"\n",
        "\n",
        "    ANSI_COLORS = {\n",
        "        'none': '\\x1b[0m',\n",
        "        'black': '\\x1b[30m',\n",
        "        'red': '\\x1b[31m',\n",
        "        'green': '\\x1b[32m',\n",
        "        'yellow': '\\x1b[33m',\n",
        "        'blue': '\\x1b[34m',\n",
        "        'magenta': '\\x1b[35m',\n",
        "        'cyan': '\\x1b[36m',\n",
        "        'white': '\\x1b[37m',\n",
        "\n",
        "        'lavender': '\\x1b[38;5;147m',\n",
        "        'pink': '\\x1b[38;5;201m'\n",
        "    }\n",
        "\n",
        "    def __init__(self, fmt=None, datefmt=None, style='%'):\n",
        "        super(_ColoredFormatter, self).__init__(fmt, datefmt, style)\n",
        "\n",
        "    def colorize(self, message, color, start_idx, end_idx):\n",
        "        if color in self.ANSI_COLORS:\n",
        "            # reset the color after colorizing the given message\n",
        "            return f\"{self.ANSI_COLORS[color]}{message[start_idx:end_idx]}\" + \\\n",
        "            f\"{self.ANSI_COLORS['none']}{message[end_idx:]}\"\n",
        "        else:\n",
        "            return message\n",
        "\n",
        "    def format(self, record):\n",
        "\n",
        "        formatted = ''\n",
        "        # if the message starts with \\n, shift the whole record down 1 line\n",
        "        if record.msg[0] == '\\n' or record.msg[0:3] == ': \\n':\n",
        "            formatted = '\\n' + super(_ColoredFormatter,\n",
        "                                     self).format(record).replace('\\n', '', 1)\n",
        "        else:\n",
        "            formatted = super(_ColoredFormatter, self).format(record)\n",
        "\n",
        "        levelname = record.levelname\n",
        "        header_idx = formatted.index(levelname)\n",
        "        end_idx = header_idx + formatted[header_idx:].index(' ')\n",
        "\n",
        "        if record.levelno == logging.ERROR:\n",
        "            return self.colorize(formatted, 'red', 0, end_idx)\n",
        "        elif record.levelno == logging.WARNING:\n",
        "            return self.colorize(formatted, 'yellow', 0, end_idx)\n",
        "        elif record.levelno == logging.INFO:\n",
        "            return self.colorize(formatted, 'blue', 0, end_idx)\n",
        "        elif record.levelno == logging.DEBUG:\n",
        "            return self.colorize(formatted, 'yellow', 0, end_idx)\n",
        "        elif record.levelno == logging.CRITICAL:\n",
        "            return self.colorize(formatted, 'pink', 0, end_idx)\n",
        "        else:\n",
        "            return formatted\n",
        "\n",
        "class Logger:\n",
        "    \"\"\"\n",
        "    Responsible for formatting and managing all logging-related calls\n",
        "    \"\"\"\n",
        "\n",
        "    def setup_logger(colored=True):\n",
        "        \"\"\"\n",
        "        Initializes the `logger` member once. All arguments default to a \\\n",
        "        predefined style\n",
        "\n",
        "        Args:\n",
        "            colored (optional, bool): whether or not the logger distinguishes \\\n",
        "            levels with predefined colors\n",
        "        \"\"\"\n",
        "\n",
        "        if hasattr(Logger, '__logger') and hasattr(Logger, '__progress_logger'):\n",
        "            return\n",
        "\n",
        "        handler = logging.StreamHandler()\n",
        "        prg_handler = logging.StreamHandler()\n",
        "\n",
        "        # init standard logger with function name from inspect\n",
        "        Logger.__logger = logging.Logger('std')\n",
        "        # init progress logger (used for updates such as running loss per epoch\n",
        "        # during training, etc.)\n",
        "        Logger.__progress_logger = logging.Logger('PROGRESS')\n",
        "        # init file logging dict\n",
        "        Logger.__file_logger = {}\n",
        "\n",
        "        # standard logger formatting\n",
        "        fmt = '[%(asctime)s:%(msecs)03d] \\033[1m%(levelname)s\\033[0m%(message)s'\n",
        "        datefmt = '%d/%m %H:%M:%S'\n",
        "\n",
        "        # progress logger formatting\n",
        "        prg_fmt = (\n",
        "            '\\x1b[36m[%(asctime)s.%(msecs)03d] \\033[1m%(name)s'\n",
        "            '\\033[0m\\x1b[0m: %(message)s'\n",
        "        )\n",
        "        prg_datefmt = '%d/%m %H:%M:%S'\n",
        "        prg_formatter = logging.Formatter(prg_fmt, datefmt=prg_datefmt)\n",
        "        prg_handler.setFormatter(prg_formatter)\n",
        "\n",
        "        if colored:\n",
        "            handler.setFormatter(_ColoredFormatter(fmt, datefmt, '%'))\n",
        "        else:\n",
        "            formatter = logging.Formatter(fmt, datefmt=datefmt)\n",
        "            handler.setFormatter(formatter)\n",
        "\n",
        "        Logger.__logger.addHandler(handler)\n",
        "        Logger.__progress_logger.addHandler(prg_handler)\n",
        "\n",
        "\n",
        "    def debug(*msg, caller=True, line=True):\n",
        "        prepend = (\n",
        "            f' \\033[3m(caller: {inspect.stack()[1][3]}'\n",
        "            f'{\", line: \" + str(inspect.stack()[1][2]) if line else \"\"})'\n",
        "            '\\033[23m: ' if caller else ': '\n",
        "        )\n",
        "        if msg is None:\n",
        "            Logger.__logger.debug(prepend)\n",
        "            return\n",
        "        Logger.__logger.debug(prepend + ' '.join([repr(m) \\\n",
        "                                                  if not isinstance(m, str) \\\n",
        "                                                  else m for m in msg]))\n",
        "\n",
        "    def info(*msg, caller=False, line=False):\n",
        "        prepend = (\n",
        "            f' \\033[3m(caller: {inspect.stack()[1][3]}' + \\\n",
        "            f'{\", line: \" + str(inspect.stack()[1][2]) if line else \"\"})' + \\\n",
        "            ('\\033[23m: ' if caller else ': ')\n",
        "        )\n",
        "        Logger.__logger.info(prepend + ' '.join(msg))\n",
        "\n",
        "    def warning(*msg, caller=True, line=True):\n",
        "        prepend = (\n",
        "            f' \\033[3m(caller: {inspect.stack()[1][3]}' + \\\n",
        "            f'{\", line: \" + str(inspect.stack()[1][2]) if line else \"\"})' + \\\n",
        "            ('\\033[23m: ' if caller else ': ')\n",
        "        )\n",
        "        Logger.__logger.warning(prepend + ' '.join(msg))\n",
        "\n",
        "    def error(*msg, caller=True, line=True):\n",
        "        prepend = (\n",
        "            f' \\033[3m(caller: {inspect.stack()[1][3]}' + \\\n",
        "            f'{\", line: \" + str(inspect.stack()[1][2]) if line else \"\"})' + \\\n",
        "            ('\\033[23m: ' if caller else ': ')\n",
        "        )\n",
        "        Logger.__logger.error(prepend + ' '.join(msg))\n",
        "\n",
        "    def critical(*msg, caller=False, line=False):\n",
        "        prepend = (\n",
        "            f' \\033[3m(caller: {inspect.stack()[1][3]}' + \\\n",
        "            f'{\", line: \" + str(inspect.stack()[1][2]) if line else \"\"})' + \\\n",
        "            ('\\033[23m: ' if caller else ': ')\n",
        "        )\n",
        "        Logger.__logger.critical(prepend + ' '.join(msg))\n",
        "\n",
        "    def progress(*msg):\n",
        "        Logger.__progress_logger.info(' '.join(msg))\n",
        "\n",
        "    def separator(color='\\x1b[32m', length=80, symbol='▬'):\n",
        "        \"\"\"\n",
        "        Symbols could be any string, some suggestions are:\n",
        "        ● ■ ◆ ▬ | ~ - = + # & ^\n",
        "        \"\"\"\n",
        "        reset_color = '\\x1b[0m'\n",
        "\n",
        "        separator = f'\\n{color}{symbol * length}{reset_color}\\n'\n",
        "\n",
        "        sys.stdout.write(separator)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def success(*msg, color='\\x1b[32m'):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        reset_color = '\\x1b[0m'\n",
        "        content = str(' '.join(msg))\n",
        "\n",
        "        log = f'\\n{color}{content}{reset_color}\\n'\n",
        "\n",
        "        sys.stdout.write(log)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "class TrainingLogger:\n",
        "\n",
        "    __total_epochs = 0\n",
        "    __total_train_batches = 0\n",
        "    __total_val_batches = 0\n",
        "    __task_id = 0\n",
        "    __task_version = 0\n",
        "    __task_name = ''\n",
        "    __log_to_file = False\n",
        "    __log_file_content = []\n",
        "\n",
        "    def progress(*msg, is_last=False, delay=0.00):\n",
        "        \"\"\"\n",
        "        Logs without Python's :class:`logging` to apply carriage return when\n",
        "        applicable\n",
        "\n",
        "        Args:\n",
        "            carriage (optional, bool): whether or not to apply carriage \\\n",
        "            return, defaults to `False`\n",
        "            delay (optional, float): amount of time to sleep for readability \\\n",
        "            purposes when logs are processed too quickly. Defaults to 0.00\n",
        "        \"\"\"\n",
        "\n",
        "        current_datetime = datetime.now()\n",
        "        time_str = current_datetime.strftime('%d/%m %H:%M:%S.%f')[:-3]\n",
        "\n",
        "        text = f'\\x1b[36m[{time_str}]'\n",
        "        text += f' \\033[1mPROGRESS\\033[0m\\x1b[0m: {\"\".join(msg)}'\n",
        "\n",
        "        if TrainingLogger.__log_to_file is not None:\n",
        "            # re-init log, but stripped of ANSI colors for file-logging\n",
        "            file_text = f'[time_str] PROGRESS: {\"\".join(msg)}'\n",
        "            TrainingLogger.__log_file_content.append(file_text)\n",
        "\n",
        "        if delay > 0:\n",
        "            time.sleep(delay)\n",
        "\n",
        "        sys.stdout.write('\\r' + text + ('\\n' if is_last else ''))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def reset(task_id, task_version, task_name, total_epochs,\n",
        "              total_train_batches, total_val_batches, log_to_file):\n",
        "        TrainingLogger.__total_epochs = total_epochs\n",
        "        TrainingLogger.__total_train_batches = total_train_batches\n",
        "        TrainingLogger.__total_val_batches = total_val_batches\n",
        "        TrainingLogger.__task_id = task_id\n",
        "        TrainingLogger.__task_name = task_name\n",
        "        TrainingLogger.__task_version = task_version\n",
        "        TrainingLogger.__log_to_file = log_to_file\n",
        "\n",
        "    def log_training(epoch, batch, avg_loss, avg_acc):\n",
        "        task = f'Task ({TrainingLogger.__task_id} '\n",
        "        task += f'v.{TrainingLogger.__task_version}: '\n",
        "        task += f'\"{TrainingLogger.__task_name}\") | '\n",
        "        epochs = task + f'Epoch {epoch+1}/{TrainingLogger.__total_epochs}, '\n",
        "        epochs += f'Batch {batch+1}/{TrainingLogger.__total_train_batches}'\n",
        "        running_metrics = f'Loss: {avg_loss}, Acc.: {avg_acc}'\n",
        "        is_last = (batch == TrainingLogger.__total_train_batches - 1)\n",
        "        # running_metrics += '\\n' if is_last else ''\n",
        "        TrainingLogger.progress(epochs + ' - ' + running_metrics,\n",
        "                                is_last=is_last)\n",
        "\n",
        "    def log_validation(epoch, batch, avg_loss, avg_acc):\n",
        "        task = f'Task ({TrainingLogger.__task_id} '\n",
        "        task += f'v.{TrainingLogger.__task_version}: '\n",
        "        task += f'\"{TrainingLogger.__task_name}\") | '\n",
        "        epochs = task + f'Epoch {epoch+1}/{TrainingLogger.__total_epochs}, '\n",
        "        epochs += f'Batch {batch+1}/{TrainingLogger.__total_val_batches}'\n",
        "        running_metrics = f'Loss: {avg_loss}, Acc.: {avg_acc}'\n",
        "        is_last = (batch == TrainingLogger.__total_val_batches - 1)\n",
        "        # running_metrics += '\\n' if is_last else ''\n",
        "        TrainingLogger.progress(epochs + ' - ' + running_metrics,\n",
        "                                is_last=is_last)\n",
        "\n",
        "    def commit_file(filename, path='./training_logs/'):\n",
        "\n",
        "        dir_path = os.path.join(Config.BASE_PATH, path)\n",
        "        full_path = os.path.join(dir_path, filename)\n",
        "\n",
        "        ensure_dir(dir_path, True)\n",
        "\n",
        "        try:\n",
        "            with open(full_path, 'w') as file:\n",
        "                file.write('\\n'.join(TrainingLogger.__log_file_content))\n",
        "            # reset uncommitted content\n",
        "            TrainingLogger.__log_file_content = []\n",
        "        except Exception as e:\n",
        "            Logger.debug(f'Error writing log to {full_path}: {e}')\n",
        "\n",
        "\n",
        "Logger.setup_logger()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS1H6iOf37hA"
      },
      "source": [
        "\n",
        "### Functional Utils\n",
        "\n",
        "<!---  \n",
        "$module=functional\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu7YUBhTG8kg"
      },
      "source": [
        "#### Graph Utils\n",
        "\n",
        "<!---  \n",
        "$file=graph_utils.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBmBNkFS4YSS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# GRAPH-RELATED UTILITIES\n",
        "\n",
        "def predecessor_successor_lists(adj_matrix):\n",
        "    \"\"\"\n",
        "    Get a graph's predecessor and successor lists from an adjacency matrix.\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "        `adj_matrix`:\n",
        "            [[0, 1, 1, 0, 0],\n",
        "             [0, 0, 0, 1, 0],\n",
        "             [0, 0, 0, 1, 1],\n",
        "             [0, 0, 0, 0, 1],\n",
        "             [0, 0, 0, 0, 0]]\n",
        "\n",
        "        Predecessor list (inputs of every node):\n",
        "            [ [], [0], [0], [1, 2], [2, 3] ]\n",
        "\n",
        "        Successor list (outputs of every node):\n",
        "            [ [1, 2], [3], [3, 4], [4], [] ]\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (:class:`numpy.ndarray`): the graph's adjacency matrix\n",
        "    Returns:\n",
        "        tuple: a 2-element tuple consisting of a predecessor and a successor \\\n",
        "        list, respectively\n",
        "    \"\"\"\n",
        "\n",
        "    num_vertices = adj_matrix.shape[0]\n",
        "\n",
        "    pred = [[] for _ in range(num_vertices)]    # input connections per vertex\n",
        "    succ = [[] for _ in range(num_vertices)]    # output connections per vertex\n",
        "\n",
        "    # init input/output connections for every operation\n",
        "    for src in range(num_vertices):\n",
        "        for dst in range(num_vertices):\n",
        "            if adj_matrix[src, dst] == 1:\n",
        "                pred[dst].append(src)\n",
        "                succ[src].append(dst)\n",
        "\n",
        "    return pred, succ\n",
        "\n",
        "\n",
        "def path_exists(adj_matrix, src, dst):\n",
        "    \"\"\"\n",
        "    Uses DFS to check if a path exists from `src` to `dst` in a\n",
        "    given adjacency matrix\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (:class:`numpy.ndarray`): the graph's adjacency matrix\n",
        "        src (int): index of the source node\n",
        "        dst (int): index of the destination node\n",
        "    Returns:\n",
        "        bool: whether or not a path was found\n",
        "    \"\"\"\n",
        "\n",
        "    num_vertices = len(adj_matrix)\n",
        "    vis = [False] * num_vertices    # visited array\n",
        "\n",
        "    def dfs(node):\n",
        "        vis[node] = True\n",
        "        if node == dst:\n",
        "            # reached dst\n",
        "            return True\n",
        "\n",
        "        for neighbor, edge in enumerate(adj_matrix[node]):\n",
        "            if edge == 1 and not vis[neighbor] and dfs(neighbor):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    return dfs(src)\n",
        "\n",
        "\n",
        "def get_topological_orders(adj_matrix=None, pred=None):\n",
        "    \"\"\"\n",
        "    Computes the topological orders (depths) of each node in a graph /\n",
        "    adjacency matrix. This function follows the same logic behind\n",
        "    :func:`nx.topological_sort()`'s indexing.\n",
        "\n",
        "    Note: should not be used with super-graphs (i.e. non-single-rooted DAGs)\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (:class:`numpy.ndarray`): the graph's adjacency matrix\n",
        "    Returns:\n",
        "        :obj:`list`: the depth of each node in the given graph\n",
        "    \"\"\"\n",
        "    assert adj_matrix is not None or pred is not None, (\n",
        "        'You must provide the adj_matrix or pred list to compute '\n",
        "        'the topological orders'\n",
        "    )\n",
        "\n",
        "    if pred is None:\n",
        "        pred, _ = predecessor_successor_lists(adj_matrix)\n",
        "\n",
        "    depths = [-1 for _ in range(len(pred))]     # memoization list\n",
        "\n",
        "    def depth_dfs(node):\n",
        "        if depths[node] >= 0:\n",
        "            # memoized\n",
        "            return depths[node]\n",
        "\n",
        "        if not pred[node]:  # no predecessors\n",
        "            depths[node] = 0\n",
        "        else:\n",
        "            depths[node] = max(depth_dfs(prev_node) \\\n",
        "                               for prev_node in pred[node]) + 1\n",
        "        return depths[node]\n",
        "\n",
        "    # populate `depths`\n",
        "    for op, _ in enumerate(pred):\n",
        "        depth_dfs(op)\n",
        "\n",
        "    return depths\n",
        "\n",
        "\n",
        "def dag_is_rooted_tree(adj_matrix):\n",
        "    \"\"\"\n",
        "    Checks whether or not a given DAG is:\n",
        "        1. Single-rooted\n",
        "        2. Has no intermediary leaf nodes (single output)\n",
        "        3. Has at least 1 path from root to output\n",
        "\n",
        "    These criteria are used to define the validity of a CNN. Multi-head models\n",
        "    are explicitly defined during the generation of a network; this method is\n",
        "    exclusively used to enforce topologies that are generated randomly.\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (:class:`numpy.ndarray`): the graph's adjacency matrix\n",
        "\n",
        "    Returns:\n",
        "        bool: whether or not the DAG is a Rooted Tree\n",
        "    \"\"\"\n",
        "\n",
        "    pred, succ = predecessor_successor_lists(adj_matrix)\n",
        "\n",
        "    if not path_exists(adj_matrix, 0, len(pred) - 1):\n",
        "        # no path from root to output\n",
        "        return False\n",
        "\n",
        "    for op_idx, inputs in enumerate(pred):\n",
        "        if op_idx == 0 or op_idx == len(pred) - 1:\n",
        "            # skip input/output stems\n",
        "            continue\n",
        "\n",
        "        elif len(inputs) == 0 and len(succ[op_idx]) > 0:\n",
        "            # secondary root detected\n",
        "            return False\n",
        "\n",
        "        elif len(inputs) > 0 and len(succ[op_idx]) == 0:\n",
        "            # secondary leaf detected\n",
        "            return False\n",
        "\n",
        "    # validated\n",
        "    return True\n",
        "\n",
        "\n",
        "def trim_isolate_nodes(adj_matrix):\n",
        "    \"\"\"\n",
        "    Removes island/isolate nodes (those with no neighbors; i.e. nodes with\n",
        "    degree zero) from a give graph topology (adjacency matrix).\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (:class:`numpy.ndarray`): the graph's adjacency matrix\n",
        "\n",
        "    Returns:\n",
        "        (:class:`numpy.ndarray`): the isolates-free `adj_matrix`\n",
        "    \"\"\"\n",
        "\n",
        "    # identify neighborless nodes (rows and columns sum = 0)\n",
        "    row_sums = adj_matrix.sum(axis=1)\n",
        "    col_sums = adj_matrix.sum(axis=0)\n",
        "    rem_indices = np.where((row_sums == 0) & (col_sums == 0))[0]\n",
        "\n",
        "    if not rem_indices.size:\n",
        "        # no isolates\n",
        "        return adj_matrix\n",
        "\n",
        "    # delete rows/columns with no edges\n",
        "    ret_adj = np.delete(adj_matrix, rem_indices, axis=0)\n",
        "    ret_adj = np.delete(ret_adj, rem_indices, axis=1)\n",
        "\n",
        "    return ret_adj\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M5JeLamG_76"
      },
      "source": [
        "#### Module Utils\n",
        "\n",
        "<!---  \n",
        "$file=module_utils.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSIUytdCHEsn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def __init_kernel_based_op_params(kernel_size, stride, padding, dilation):\n",
        "\n",
        "    if (type(padding) is str and padding != 'same'):\n",
        "        raise ValueError('Invalid padding value')\n",
        "\n",
        "    k = (kernel_size, kernel_size) if type(kernel_size) is int else kernel_size\n",
        "    s = (stride, stride) if type(stride) is int else stride\n",
        "    d = (dilation, dilation) if type(dilation) is int else dilation\n",
        "\n",
        "    p = padding\n",
        "    if type(p) is int:\n",
        "        p = (p, p)\n",
        "    elif type(p) is tuple:\n",
        "        pass  # tuple padding is already in the right format\n",
        "    elif type(p) is str:\n",
        "        if p == 'same':\n",
        "            # calculate 'same' padding based on the kernel size and dilation\n",
        "            p = (((k[0] - 1) * d[0] + 1) // 2, ((k[1] - 1) * d[1] + 1) // 2)\n",
        "\n",
        "    return k, s, d, p\n",
        "\n",
        "\n",
        "def kernel_based_outshape(in_shape, kernel_size,\n",
        "                          filter_count, stride, padding, dilation, **kwargs):\n",
        "    \"\"\"\n",
        "    Calculate the output shape of any kernel-based operation (convolution,\n",
        "    pooling, etc.)\n",
        "\n",
        "    Args:\n",
        "        in_shape (:class:`torch.Tensor`): the input feature map's shape \\\n",
        "        (NxCxHxW)\n",
        "        kernel_size (int / tuple): the operation's filter size \\\n",
        "        (accepts `int` (`kernel_size` x `kernel_size`) or tuple (HxW))\n",
        "        filter_count (int): the number of filters in the operation\n",
        "        stride (int / tuple): the operation's filter stride \\\n",
        "        (accepts `int` or tuple (HxW))\n",
        "        padding (int / tuple / str): the output map's padding (accepts `int` \\\n",
        "        or `tuple` values (HxW), and `'same'`, which pads the output map to \\\n",
        "        maintain its input resolution) dilation (int / tuple): the operation's \\\n",
        "        filter dilation value (accepts `int` or tuple (HxW))\n",
        "    \"\"\"\n",
        "    in_height, in_width = in_shape[2:]\n",
        "\n",
        "    k, s, d, p = __init_kernel_based_op_params(kernel_size, stride,\n",
        "                                               padding, dilation)\n",
        "\n",
        "    dilated_kernel_height = k[0] + (k[0] - 1) * (d[0] - 1)\n",
        "    dilated_kernel_width = k[1] + (k[1] - 1) * (d[1] - 1)\n",
        "\n",
        "    # conv output dimensions\n",
        "    out_height = ((in_height + 2 * p[0] - dilated_kernel_height) // s[0] + 1)\n",
        "    out_width = ((in_width + 2 * p[1] - dilated_kernel_width) // s[1] + 1)\n",
        "\n",
        "    return (in_shape[0], filter_count, out_height, out_width)\n",
        "\n",
        "\n",
        "def kernel_based_validation(in_shape, out_shape, kernel_size,\n",
        "                            filter_count, stride, padding, dilation, **kwargs):\n",
        "    \"\"\"\n",
        "    Validates the hyperparameters of a kernel-based operation given the input\n",
        "    dimensions and intrinsic constraints (`padding` <= `kernel_size` / 2)\n",
        "    Args:\n",
        "        in_shape (:class:`torch.Tensor`): the input feature map's shape \\\n",
        "        (NxCxHxW)\n",
        "        kernel_size (int / tuple): the operation's filter size \\\n",
        "        (accepts `int` (`kernel_size` x `kernel_size`) or tuple (HxW))\n",
        "        filter_count (int): the number of filters in the operation\n",
        "        stride (int / tuple): the operation's filter stride \\\n",
        "        (accepts `int` or tuple (HxW))\n",
        "        padding (int / tuple / str): the output map's padding (accepts `int` \\\n",
        "        or `tuple` values (HxW), and `'same'`, which pads the output map to \\\n",
        "        maintain its input resolution) dilation (int / tuple): the operation's \\\n",
        "        filter dilation value (accepts `int` or tuple (HxW))\n",
        "    \"\"\"\n",
        "    assert len(in_shape) == 4 and len(out_shape) == 4, (\n",
        "        'Invalid input/output shape provided'\n",
        "    )\n",
        "\n",
        "    k, s, d, p = __init_kernel_based_op_params(kernel_size, stride,\n",
        "                                               padding, dilation)\n",
        "\n",
        "    # kernel size smaller than input shape\n",
        "    if k[0] > in_shape[2] or k[1] > in_shape[3]:\n",
        "        return False\n",
        "\n",
        "    # padding is at most half the kernel size\n",
        "    if p[0] > k[0] // 2 or p[1] > k[1] // 2:\n",
        "        return False\n",
        "\n",
        "    # stride and filter count > 0\n",
        "    if s[0] <= 0 or s[1] <= 0 or filter_count <= 0:\n",
        "        return False\n",
        "\n",
        "    # valid\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTy6nDJlyMiI"
      },
      "source": [
        "#### FLOPs Estimation\n",
        "\n",
        "<!---  \n",
        "$file=flops_estimation.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxa1SQS6yQ0U"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_conv_flops(in_shape, out_shape,\n",
        "                         kernel_size,\n",
        "                         separable=False):\n",
        "    \"\"\"\n",
        "    General formula to calculate MACs/FLOPs for convolutional operations\n",
        "\n",
        "    Ref:\n",
        "        Abstracted from http://bit.ly/411Uscw, but modified to account for \\\n",
        "        padding, dilation, etc., through inference from the output size\n",
        "\n",
        "        Standard Conv:\n",
        "        (2 * input_channels * (kernel_size^2) * output_height * output_width)\n",
        "        * output_channels\n",
        "\n",
        "        Separable Conv:\n",
        "        (2 x input_channels * (kernel_size^2) * output_height * output_width)\n",
        "        +\n",
        "        (2 * input_channels * output_channels)\n",
        "\n",
        "    Returns:\n",
        "        :obj:`float`: calculated FLOPs for the given conv. layer\n",
        "    \"\"\"\n",
        "\n",
        "    mac = in_shape[1] * (kernel_size**2) * out_shape[2] * out_shape[3]\n",
        "    flops = 2 * mac\n",
        "\n",
        "    if separable:\n",
        "        # add point-wise terms, as opposed to multiplying by out_channels\n",
        "        # (much less FLOPs)\n",
        "        mac_depthwise = in_shape[1] * (kernel_size**2) \\\n",
        "        * out_shape[2] * out_shape[3]\n",
        "        flops_depthwise = 2 * mac_depthwise\n",
        "\n",
        "        mac_pointwise = in_shape[1] * out_shape[1] * out_shape[2] * out_shape[3]\n",
        "        flops_pointwise = 2 * mac_pointwise\n",
        "\n",
        "        flops = flops_depthwise + flops_pointwise\n",
        "    else:\n",
        "        # multiply by out_channels --> standard conv\n",
        "        flops = flops * out_shape[1]\n",
        "\n",
        "    return flops\n",
        "\n",
        "\n",
        "def calculate_linear_flops(in_shape, out_shape):\n",
        "    \"\"\"\n",
        "    General formula to calculate MACs/FLOPs for linear operations\n",
        "\n",
        "    Ref:\n",
        "        http://bit.ly/411Uscw\n",
        "\n",
        "    Args:\n",
        "        in_shape (:obj:`tuple`): input shape for the linear operation\n",
        "        out_shape (:obj:`tuple`): output shape for the linear operation\n",
        "\n",
        "    Returns:\n",
        "        :obj:`float`: calculated FLOPs for the given linear layer\n",
        "    \"\"\"\n",
        "    mac = in_shape[1] * out_shape[1]\n",
        "\n",
        "    return 2 * mac\n",
        "\n",
        "\n",
        "def calculate_pool_flops(out_shape, flops_per_element):\n",
        "    \"\"\"\n",
        "    General formula to calculate MACs/FLOPs for pooling operations.\n",
        "\n",
        "    Args:\n",
        "        out_shape (:obj:`tuple`): output shape for the pooling operation\n",
        "        flops_per_element (:obj:`float`): number of FLOPs for each sliding \\\n",
        "        window iteration (differs for each type of pooling operation). \\\n",
        "        i.e. for MaxPool, each iteration comprises of 1 comparison operation, \\\n",
        "        while AveragePool requires multiple additions + divisions.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`float`: calculated FLOPs for the given pooling layer\n",
        "    \"\"\"\n",
        "\n",
        "    return flops_per_element * out_shape[1] * out_shape[2] * out_shape[3]\n",
        "\n",
        "\n",
        "def calculate_norm_flops(in_shape):\n",
        "    \"\"\"\n",
        "    General formula to calculate MACs/FLOPs for normalization operations.\n",
        "    This estimation assumes 8 FLOPs per element (mean, variance, scale, shift).\n",
        "\n",
        "    Args:\n",
        "        in_shape (:obj:`tuple`): input shape for the pooling operation\n",
        "\n",
        "    Returns:\n",
        "        :obj:`float`: calculated FLOPs for the given normalization layer\n",
        "    \"\"\"\n",
        "\n",
        "    return 8 * in_shape[1] * in_shape[2] * in_shape[3]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7_slAlgfPlZ"
      },
      "source": [
        "#### File Utils\n",
        "\n",
        "<!---  \n",
        "$file=file_utils.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btmW6anDfTM-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def recursive_dir_delete(path):\n",
        "    shutil.rmtree(path)\n",
        "\n",
        "\n",
        "def should_overwrite_path(path, force_overwrite=False):\n",
        "    if os.path.exists(path):\n",
        "        if not force_overwrite:\n",
        "            Logger.warning(f'Path \"{path}\" is not empty! ',\n",
        "                            'Set `force_overwrite` to True to overwrite the ',\n",
        "                            'existing data')\n",
        "            return False\n",
        "        else:\n",
        "            Logger.warning(f'Overwriting path \"{path}\"...')\n",
        "            recursive_dir_delete(path)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def ensure_dir(path, create_if_not_exists):\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        if create_if_not_exists:\n",
        "            os.makedirs(path)\n",
        "\n",
        "        return False\n",
        "\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5saaZDE2stXI"
      },
      "source": [
        "#### Torch Utils\n",
        "\n",
        "<!---  \n",
        "$file=torch_utils.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezsM8eOhsvfn"
      },
      "outputs": [],
      "source": [
        "def transforms_equal(compose_a, compose_b):\n",
        "    if len(compose_a.transforms) != len(compose_b.transforms):\n",
        "        return False\n",
        "\n",
        "    for idx, _ in compose_a.transforms:\n",
        "        t_a = compose_a.transforms[idx]\n",
        "        t_b = compose_b.transforms[idx]\n",
        "\n",
        "        if type(t_a) != type(t_b):\n",
        "            return False\n",
        "\n",
        "        for param in t_a.__dict__:\n",
        "            if param not in t_b.__dict__:\n",
        "                # parameters not identical\n",
        "                return False\n",
        "\n",
        "            # check equality only on public parameters\n",
        "            if param[0] != '_':\n",
        "                if t_a.__dict__[param] != t_b.__dict__[param]:\n",
        "                    return False\n",
        "\n",
        "    return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO75HmYD2Khy"
      },
      "source": [
        "#### Search Space Utils\n",
        "\n",
        "<!---  \n",
        "$file=search_space_utils.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kiJf1p42Ma1"
      },
      "outputs": [],
      "source": [
        "def uniform_operation_weights(op_weights_dict,\n",
        "                              topological_order, max_depth, prev_ops,\n",
        "                              all_sampled_ops, in_shape, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    The default hook used for operation-sampling. This function precedes other\n",
        "    custom-registered hooks.\n",
        "\n",
        "    Assign uniform probabilities across operation-groups (rather than\n",
        "    across all partials)\n",
        "\n",
        "    i.e. for 8 instances of Conv2D configurations, we consider them\n",
        "    as 1 operation and assign uniform probabilities within that group.\n",
        "    This reduces bias towards operations with a large number of\n",
        "    configurations\n",
        "\n",
        "    Args:\n",
        "        op_weights_dict (:obj:`dict`): a dict of the valid partial operations \\\n",
        "        and their corresponding weights (calculated from previous hooks if \\\n",
        "        applicable). The values in this dict are not guaranteed to sum to `1.0`.\n",
        "        topological_order (:obj:`int`): the current node's depth in the graph. \\\n",
        "        Could be used to add weight for certain operations in deeper sections \\\n",
        "        of the architecture\n",
        "        max_depth (:obj:`int`): max depth of the given graph\n",
        "        prev_ops (:obj:`list`): list the current node's direct predecessor(s)\n",
        "        all_sampled_ops (:obj:`list`): list of tuples of all sampled \\\n",
        "        operations thus far and their corresponding topological orders\n",
        "        in_shape (:obj:`tuple`): the expected input shape to this node\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple`: a tuple containing the set of operations (possibly \\\n",
        "        modified) and the computed weights' dict (`partial_op`:`float weight`)\n",
        "    \"\"\"\n",
        "\n",
        "    # Logger.debug('Hook: \\n',\n",
        "    #              # f'op_set: {operation_set}\\n',\n",
        "    #              f'weights: {weights}\\n',\n",
        "    #              f'topological_order: {topological_order}\\n',\n",
        "    #              f'max_depth: {max_depth}\\n',\n",
        "    #              f'prev_ops: {prev_ops}\\n',\n",
        "    #              f'in_shape: {in_shape}\\n')\n",
        "\n",
        "    func_groups = defaultdict(list)\n",
        "    for op_partial in op_weights_dict.keys():\n",
        "        func_groups[op_partial.func].append(op_partial)\n",
        "\n",
        "    op_groups = len(func_groups)\n",
        "    op_group_weight = 1 / op_groups\n",
        "\n",
        "    ret_w = {}\n",
        "\n",
        "    for op_cls, hyperparam_combs in func_groups.items():\n",
        "        partial_weight = op_group_weight / len(hyperparam_combs)\n",
        "        for op in hyperparam_combs:\n",
        "            ret_w[op] = partial_weight\n",
        "\n",
        "    return ret_w\n",
        "\n",
        "\n",
        "def incentivize_operation_overlap_hook_factory(overlapping_ops):\n",
        "    \"\"\"\n",
        "    A factory function that returns a hook that encourages the selection of\n",
        "    operations from `overlapping_ops`.\n",
        "\n",
        "    This is the default method to create overlaps in Task-Incremental Learning\n",
        "    scenarios.\n",
        "\n",
        "    Args:\n",
        "        overlapping_ops (:obj:`list`): a list of tuples comprising of 1. the \\\n",
        "        set of operations' partials used to compose a hook function to \\\n",
        "        incentivize intersecting with and 2. their corresponding topological \\\n",
        "        orders in the supergraph\n",
        "\n",
        "    Returns:\n",
        "        :obj:`callable`: constructed function to incentivize the given \\\n",
        "        operation set\n",
        "    \"\"\"\n",
        "\n",
        "    # decompose the incentive set and the corresponding topological orders\n",
        "    _incentive_set = {copy(t[0]) for t in overlapping_ops}\n",
        "    _depths = [t[1] for t in overlapping_ops]\n",
        "\n",
        "    def _hook(op_weights_dict, topological_order, max_depth,\n",
        "              prev_ops, all_sampled_ops, in_shape, *args, **kwargs):\n",
        "\n",
        "        min_overlap_depth = 1\n",
        "        sampled_ops_ids = []\n",
        "\n",
        "        for s_idx, (s_op, s_depth) in enumerate(all_sampled_ops):\n",
        "            # check currently used operations to ensure acyclic sampling\n",
        "            if 'id' not in s_op.keywords:\n",
        "                continue\n",
        "\n",
        "            sampled_ops_ids.append(s_op.keywords['id'])\n",
        "\n",
        "            for i_op, i_depth in zip(_incentive_set, _depths):\n",
        "                if s_op.keywords['id'] == i_op.keywords['id']:\n",
        "                    # overlapping operation; update min_overlap_depth to prevent\n",
        "                    # cycles\n",
        "                    min_overlap_depth = max(min_overlap_depth, s_depth)\n",
        "\n",
        "        ret_weights = deepcopy(op_weights_dict) # preserve referenced weights\n",
        "\n",
        "        # incentivize weight by sqrt(n)\n",
        "        incentive_factor = math.sqrt(len(ret_weights)) * 10\n",
        "\n",
        "        for op, op_weight in op_weights_dict.items():\n",
        "            for i_op, i_depth in zip(_incentive_set, _depths):\n",
        "                # filter by depth to prevent cycles\n",
        "                if i_depth >= min_overlap_depth:\n",
        "                    # more than or equal as we can use other nodes from the\n",
        "                    # same topological level\n",
        "\n",
        "                    # test equality for intersecting keys in the partial\n",
        "                    # wrapper (this mimics the equality of `op.signature`)\n",
        "                    if op.intersect_equals(i_op) and \\\n",
        "                    i_op.keywords['id'] not in sampled_ops_ids:\n",
        "\n",
        "                        i_op_id = i_op.keywords['id']\n",
        "                        # change op ID (replace dict key) and add incentive\n",
        "                        new_op = deepcopy(op)\n",
        "                        Logger.debug(ret_weights.keys())\n",
        "                        new_op.keywords['id'] = i_op.keywords['id']\n",
        "                        old_w = ret_weights[op]\n",
        "                        ret_weights[new_op] = old_w * incentive_factor\n",
        "                        del ret_weights[op]\n",
        "\n",
        "        return ret_weights\n",
        "\n",
        "    return _hook\n",
        "\n",
        "\n",
        "def spatial_ops_boost_hook(op_weights_dict,\n",
        "                           topological_order, max_depth, prev_ops,\n",
        "                           all_sampled_ops, in_shape, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    An optional hook to boost spatial operations' probability when the previous\n",
        "    operation(s) was non-spatial.\n",
        "\n",
        "    Args:\n",
        "        op_weights_dict (:obj:`dict`): a dict of the valid partial operations \\\n",
        "        and their corresponding weights (calculated from previous hooks if \\\n",
        "        applicable). The values in this dict are not guaranteed to sum to `1.0`.\n",
        "        topological_order (:obj:`int`): the current node's depth in the graph. \\\n",
        "        Could be used to add weight for certain operations in deeper sections \\\n",
        "        of the architecture\n",
        "        max_depth (:obj:`int`): max depth of the given graph\n",
        "        prev_ops (:obj:`list`): list the current node's direct predecessor(s)\n",
        "        all_sampled_ops (:obj:`list`): list of tuples of all sampled \\\n",
        "        operations thus far and their corresponding topological orders\n",
        "        in_shape (:obj:`tuple`): the expected input shape to this node\n",
        "\n",
        "    Returns:\n",
        "        :obj:`tuple`: a tuple containing the set of operations (possibly \\\n",
        "        modified) and the computed weights' dict (`partial_op`:`float weight`)\n",
        "    \"\"\"\n",
        "\n",
        "    ret_w = deepcopy(op_weights_dict)\n",
        "\n",
        "    if not any([True for op in prev_ops \\\n",
        "                if op.func.OPERATION_TYPE == OperationType.SPATIAL_OP]):\n",
        "        # no spatial ops preceding; boost spatial probs\n",
        "        incentive_factor = math.sqrt(len(ret_w)) * 10\n",
        "\n",
        "        for op, weight in ret_w.items():\n",
        "            if op.func.OPERATION_TYPE == OperationType.SPATIAL_OP:\n",
        "                ret_w[op] = weight * incentive_factor\n",
        "\n",
        "    return ret_w\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Environment Utils\n",
        "\n",
        "<!---  \n",
        "$file=env_utils.py\n",
        "-->"
      ],
      "metadata": {
        "id": "0Tz3gDoe4-3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpu_memory():\n",
        "\n",
        "    if shutil.which('nvidia-smi') is None:\n",
        "        # nvidia-smi not available\n",
        "        return {}\n",
        "\n",
        "    # execute nvidia-smi command in a subprocess\n",
        "    result = subprocess.run(['nvidia-smi',\n",
        "                             '--query-gpu=memory.total,memory.free,memory.used',\n",
        "                             '--format=csv,noheader,nounits'],\n",
        "                            stdout=subprocess.PIPE)\n",
        "    output = result.stdout.decode('utf-8')\n",
        "\n",
        "    # parse the output\n",
        "    memory_info = output.strip().split('\\n')\n",
        "    gpu_memory = {}\n",
        "    for info in memory_info:\n",
        "        total, free, used = re.split(r',\\s*', info)\n",
        "        gpu_memory = {\n",
        "            'gpu_mem_total_mb': int(total),\n",
        "            'gpu_mem_free_mb': int(free),\n",
        "            'gpu_mem_used_mb': int(used)\n",
        "        }\n",
        "\n",
        "    return gpu_memory\n",
        "\n",
        "\n",
        "def get_system_usage():\n",
        "    ret_dict = {}\n",
        "\n",
        "    # get current process\n",
        "    process = psutil.Process(os.getpid())\n",
        "\n",
        "    # get system ram usage\n",
        "    mb_factor = 1024 ** 2\n",
        "    ret_dict['sys_mem_free_mb'] = psutil.virtual_memory().available // mb_factor\n",
        "    ret_dict['sys_mem_used_mb'] = psutil.virtual_memory().used // mb_factor\n",
        "    ret_dict['sys_mem_total_mb'] = psutil.virtual_memory().total // mb_factor\n",
        "\n",
        "    # get process size\n",
        "    ret_dict['process_rss_mb'] = process.memory_info().rss // mb_factor\n",
        "\n",
        "    return {**ret_dict, **(get_gpu_memory())}\n",
        "\n"
      ],
      "metadata": {
        "id": "eEnhO6tl5Kag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pupkQu44AqZ3"
      },
      "source": [
        "#### Misc. Utils\n",
        "\n",
        "<!---  \n",
        "$file=misc_utils.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP2KWgwRAxBs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def flatten_dict(d, current_key='', sep='.'):\n",
        "\n",
        "    if d is None:\n",
        "        return {}\n",
        "\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f'{current_key}{sep}{k}' if current_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "\n",
        "    return dict(items)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwxWXihhsbJy"
      },
      "source": [
        "### Partial Wrapper\n",
        "\n",
        "<!---  \n",
        "$file=partial_wrapper.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbtkKGEXscpk"
      },
      "outputs": [],
      "source": [
        "class PartialWrapper(partial):\n",
        "    \"\"\"\n",
        "    A custom :class:`~functools.partial` subclass that supports equality.\n",
        "    `__eq__` is not supported by default, and the official suggested approach\n",
        "    is to sub-class `partial` (as per https://bugs.python.org/issue3564).\n",
        "\n",
        "    This wrapper is also hashable, making it usable as a\n",
        "    \"\"\"\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, PartialWrapper):\n",
        "            return NotImplemented\n",
        "\n",
        "        return (self.func, self.args, self.keywords) == \\\n",
        "         (other.func, other.args, other.keywords)\n",
        "\n",
        "    def __ne__(self, other):\n",
        "        return not self == other\n",
        "\n",
        "    def __str__(self):\n",
        "        return str((self.func, self.args, str(self.keywords) \\\n",
        "                     if self.keywords else None))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __hash__(self):\n",
        "        # hash the function + args + keywords (as an immutable set)\n",
        "        return hash((self.func, self.args, frozenset(self.keywords.items()) \\\n",
        "                     if self.keywords else None))\n",
        "\n",
        "\n",
        "    def intersect_equals(self, other):\n",
        "        if not isinstance(other, PartialWrapper):\n",
        "            raise NotImplemented\n",
        "\n",
        "        # compare func / positional arguments\n",
        "        if self.func != other.func or self.args != other.args:\n",
        "            return False\n",
        "\n",
        "        # get intersection of kwargs\n",
        "        intersection = set(self.keywords.keys()) & set(other.keywords.keys())\n",
        "\n",
        "        for key in intersection:\n",
        "            if self.keywords[key] != other.keywords[key]:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssIDkb9r-uhI"
      },
      "source": [
        "### Metadata\n",
        "\n",
        "<!---  \n",
        "$module=metadata\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUFY0-GJ-xBL"
      },
      "outputs": [],
      "source": [
        "class Metadata:\n",
        "\n",
        "    def __init__(self, params=None, **kwargs):\n",
        "        self.params = dict(params if params else {}, **kwargs)\n",
        "\n",
        "        for key, value in self.params.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "    def pretty_print(self):\n",
        "        dump_dict = deepcopy(self.params)\n",
        "        for key, val in dump_dict.items():\n",
        "            dump_dict[key] = str(val)\n",
        "\n",
        "        return json.dumps(dump_dict, indent=4, sort_keys=True)\n",
        "\n",
        "    def save(self, dir, filename):\n",
        "\n",
        "        dir_path = os.path.join(Config.BASE_PATH, dir)\n",
        "        full_path = os.path.join(dir_path, filename)\n",
        "\n",
        "        ensure_dir(dir_path, True)\n",
        "\n",
        "        dump_dict = deepcopy(self.params)\n",
        "        for key, val in dump_dict.items():\n",
        "            dump_dict[key] = str(val)\n",
        "        try:\n",
        "            with open(full_path, 'w') as f:\n",
        "                f.write(json.dumps(dump_dict))\n",
        "        except Exception as e:\n",
        "            Logger.debug(f'Error writing metadata log to {full_path}: {e}')\n",
        "\n",
        "    def __str__(self):\n",
        "        return ','.join([f'{k}={str(v)}' for k, v in self.params.items()])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, self.__class__):\n",
        "            return False\n",
        "\n",
        "        return self.params == other.params\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(str(self))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a6LRc4Xma53"
      },
      "source": [
        "#### Operation Metadata\n",
        "\n",
        "<!---  \n",
        "$file=operational_metadata.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO_gVlo2mer5"
      },
      "outputs": [],
      "source": [
        "class OperationMetadata(Metadata):\n",
        "\n",
        "    def __init__(self,\n",
        "                 op, id,\n",
        "                 in_shape=None,\n",
        "                 out_shape=None,\n",
        "                 is_partial=False,\n",
        "                 **hyperparameters):\n",
        "        \"\"\"\n",
        "        Not simply getting `**kwargs` as the strongly-typed args can be more\n",
        "        robustly handled in instantiation\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            'op': op,\n",
        "            'id': id\n",
        "        }\n",
        "\n",
        "        if in_shape is not None:\n",
        "            params['in_shape'] = in_shape\n",
        "        if out_shape is not None:\n",
        "            params['out_shape'] = out_shape\n",
        "\n",
        "        for k, v in hyperparameters.items():\n",
        "            params[k] = v\n",
        "\n",
        "        params['_is_partial'] = is_partial\n",
        "        super(OperationMetadata, self).__init__(params=params)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def init_from_partial(partial_wrapper):\n",
        "        \"\"\"\n",
        "        Primarily used to init :class:`~SearchSpaceMetadata` as all Search\n",
        "        Space operations are contained as :class:`~PartialWrapper` objects\n",
        "        until sampled.\n",
        "        \"\"\"\n",
        "        return OperationMetadata(op=partial_wrapper.func.__name__,\n",
        "                                 id=None, in_shape=None, out_shape=None,\n",
        "                                 is_partial=True,\n",
        "                                 **(partial_wrapper.keywords))\n",
        "\n",
        "\n",
        "    @property\n",
        "    def signature(self):\n",
        "        \"\"\"\n",
        "        Essentially the same as `__str__()`, but excluding the operation's UID.\n",
        "\n",
        "        This is used to check if two operations have the same \"signature\", or\n",
        "        properties and structure.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`str`: the operation's signature\n",
        "        \"\"\"\n",
        "\n",
        "        return ','.join([f'{k}={str(v)}' for k, v in self.params.items() \\\n",
        "                         if k != 'id'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnhFNByI2MeZ"
      },
      "source": [
        "#### Search Space Metadata\n",
        "\n",
        "<!---  \n",
        "$file=search_space_metadata.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsLXVUY3Bh6W"
      },
      "outputs": [],
      "source": [
        "class SearchSpaceMetadata(Metadata):\n",
        "\n",
        "    def __init__(self, type, num_vertices, encoding,\n",
        "                 operations_metadata, **kwargs):\n",
        "        \"\"\"\n",
        "        Not simply getting `**kwargs` as the strongly-typed args can be more\n",
        "        robustly handled in instantiation\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            'type': type,\n",
        "            'num_vertices': num_vertices,\n",
        "            'encoding': encoding,\n",
        "            'operations_metadata': operations_metadata\n",
        "        }\n",
        "\n",
        "        super(SearchSpaceMetadata, self).__init__(params=params, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VlRyMJtf9NK"
      },
      "source": [
        "#### Model Metadata\n",
        "\n",
        "<!---  \n",
        "$file=model_metadata.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7a6jsUcf_Hh"
      },
      "outputs": [],
      "source": [
        "class ModelMetadata(Metadata):\n",
        "    def __init__(self, id, version, wl_hash, model_hash, serialized_graph,\n",
        "                 mflops, total_params, learnable_params, adj_matrix, nodes,\n",
        "                 task_map, tasks_metadata, **kwargs):\n",
        "        \"\"\"\n",
        "        Not simply getting `**kwargs` as the strongly-typed args can be more\n",
        "        robustly handled in instantiation\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            'id': id,\n",
        "            'version': version,\n",
        "            'wl_hash': wl_hash,\n",
        "            'model_hash': model_hash,\n",
        "            'serialized_graph': serialized_graph,\n",
        "            'mflops': mflops,\n",
        "            'total_params': total_params,\n",
        "            'learnable_params': learnable_params,\n",
        "            'adj_matrix': adj_matrix,\n",
        "            'nodes': nodes,\n",
        "            'task_map': task_map,\n",
        "            'tasks_metadata': tasks_metadata\n",
        "        }\n",
        "\n",
        "        super(ModelMetadata, self).__init__(params=params, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGDTyXxs6rZy"
      },
      "source": [
        "#### DataSource Metadata\n",
        "\n",
        "<!---  \n",
        "$file=datasource_metadata.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uQGGds46uHR"
      },
      "outputs": [],
      "source": [
        "class DataSourceMetadata(Metadata):\n",
        "    def __init__(self, path, segment_size, segment_idx,\n",
        "                 num_workers, transforms, dataset, **kwargs):\n",
        "        \"\"\"\n",
        "        Not simply getting `**kwargs` as the strongly-typed args can be more\n",
        "        robustly handled in instantiation\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            'path': path,\n",
        "            'segment_size': segment_size,\n",
        "            'segment_idx': segment_idx,\n",
        "            'num_workers': num_workers,\n",
        "            'transforms': transforms,\n",
        "            'dataset': dataset\n",
        "        }\n",
        "\n",
        "        super(DataSourceMetadata, self).__init__(params=params, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objective Metadata\n",
        "\n",
        "<!---  \n",
        "$file=objective_metadata.py\n",
        "-->"
      ],
      "metadata": {
        "id": "2MjGwPVWEqQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectiveMetadata(Metadata):\n",
        "    def __init__(self, o_type, metric_key, polarity, score_weight,\n",
        "                 thresholds_enabled, min_threshold, target_threshold, **kwargs):\n",
        "        \"\"\"\n",
        "        Not simply getting `**kwargs` as the strongly-typed args can be more\n",
        "        robustly handled in instantiation\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            'type': o_type,\n",
        "            'metric_key': metric_key,\n",
        "            'polarity': polarity,\n",
        "            'score_weight': score_weight,\n",
        "            'thresholds_enabled': thresholds_enabled,\n",
        "            'min_threshold': min_threshold,\n",
        "            'target_threshold': target_threshold\n",
        "        }\n",
        "\n",
        "        super(ObjectiveMetadata, self).__init__(params=params, **kwargs)\n",
        "\n"
      ],
      "metadata": {
        "id": "QMLqLh-2Esc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezFi1nmY4QP7"
      },
      "source": [
        "#### Task Metadata\n",
        "\n",
        "<!---  \n",
        "$file=task_metadata.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE1FSFXr4SEL"
      },
      "outputs": [],
      "source": [
        "class TaskMetadata(Metadata):\n",
        "    def __init__(self, t_type, id, version, name, modality, objectives_metadata,\n",
        "                 search_space_metadata, datasource_metadata, **kwargs):\n",
        "        \"\"\"\n",
        "        Not simply getting `**kwargs` as the strongly-typed args can be more\n",
        "        robustly handled in instantiation\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            'type': t_type,\n",
        "            'id': id,\n",
        "            'version': version,\n",
        "            'name': name,\n",
        "            'modality': modality,\n",
        "            'objectives_metadata': objectives_metadata,\n",
        "            'search_space_metadata': search_space_metadata,\n",
        "            'datasource_metadata': datasource_metadata\n",
        "        }\n",
        "\n",
        "        super(TaskMetadata, self).__init__(params=params, **kwargs)\n",
        "\n",
        "\n",
        "class VisionTaskMetadata(TaskMetadata):\n",
        "    def __init__(self, t_type, id, version, name, modality, objectives_metadata,\n",
        "                 search_space_metadata, datasource_metadata, train_batch_size,\n",
        "                 val_batch_size, learning_rate, nas_epochs, candidate_epochs,\n",
        "                 in_shape, out_shape, classes, **kwargs):\n",
        "        \"\"\"\n",
        "        Not simply getting `**kwargs` as the strongly-typed args can be more\n",
        "        robustly handled in instantiation\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            't_type': t_type,\n",
        "            'id': id,\n",
        "            'version': version,\n",
        "            'name': name,\n",
        "            'modality': modality,\n",
        "            'objectives_metadata': objectives_metadata,\n",
        "            'search_space_metadata': search_space_metadata,\n",
        "            'datasource_metadata': datasource_metadata,\n",
        "            'train_batch_size': train_batch_size,\n",
        "            'val_batch_size': val_batch_size,\n",
        "            'learning_rate': learning_rate,\n",
        "            'nas_epochs': nas_epochs,\n",
        "            'candidate_epochs': candidate_epochs,\n",
        "            'in_shape': in_shape,\n",
        "            'out_shape': out_shape,\n",
        "            'classes': classes\n",
        "        }\n",
        "\n",
        "        super(VisionTaskMetadata, self).__init__(**params, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071MBvTyThrp"
      },
      "source": [
        "<!---  \n",
        "$file=reproducibility.py\n",
        "-->\n",
        "\n",
        "### Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pc7kBVlTl1w"
      },
      "outputs": [],
      "source": [
        "def set_reproducible(random_seed):\n",
        "    # torch\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # numpy\n",
        "    np.random.seed(random_seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QAc45AVCsCz"
      },
      "source": [
        "## Performance Metrics\n",
        "\n",
        "<!---  \n",
        "$module=performance_metrics\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu5jdYJcSRrS"
      },
      "source": [
        "### Evaluation Metrics\n",
        "\n",
        "<!---  \n",
        "$file=eval_metrics.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmJPBH69SUxo"
      },
      "outputs": [],
      "source": [
        "class EvaluationMetrics:\n",
        "    \"\"\"\n",
        "    Candidate evaluation metrics (1 record per epoch; batch-level metrics\n",
        "    could be saved in logs in an unstructured/plain-text format)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, task_metadata):\n",
        "\n",
        "        self.records = []\n",
        "        self.task_metadata = flatten_dict({'task': task_metadata.params})\n",
        "\n",
        "\n",
        "    def add_metrics(self, metrics, epoch, start_time):\n",
        "        \"\"\"\n",
        "        Record an epoch's metrics\n",
        "        \"\"\"\n",
        "\n",
        "        start_time_fmt = time.strftime('%Y/%m/%d, %H:%M:%S',\n",
        "                                       time.localtime(start_time))\n",
        "\n",
        "        rec = {\n",
        "            'epoch': epoch,\n",
        "            'start_time': start_time_fmt,\n",
        "            'duration': time.time() - start_time\n",
        "        }\n",
        "        rec = {**metrics, **rec}\n",
        "\n",
        "        self.records.append(rec)\n",
        "\n",
        "\n",
        "    def aggregate(self):\n",
        "        \"\"\"\n",
        "        Collates the iteratively-added metrics\n",
        "\n",
        "        Returns:\n",
        "            :obj:`dict`: the aggregated epochs' metrics\n",
        "        \"\"\"\n",
        "\n",
        "        return [{**self.task_metadata, **record} for record in self.records]\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert isinstance(idx, str), (\n",
        "            'Non-String indices are not supported for ModelMetrics objects'\n",
        "        )\n",
        "\n",
        "        # aggregate model data and records\n",
        "        agg_recs = self.aggregate()\n",
        "\n",
        "        vals = []\n",
        "        for record in agg_recs:\n",
        "            if idx not in record:\n",
        "                raise KeyError(f'metric key \"{idx}\" does not exist')\n",
        "            vals.append(record[idx])\n",
        "        return vals\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return str([{**self.task_metadata, **record} \\\n",
        "                    for record in self.records])\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AegDSNKQa9QO"
      },
      "source": [
        "### Model Metrics\n",
        "\n",
        "<!---  \n",
        "$file=model_metrics.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq_JvwB8bNR0"
      },
      "outputs": [],
      "source": [
        "class ModelMetrics:\n",
        "    \"\"\"\n",
        "    The metrics of an individual model (collection of all epochs' results)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_metadata=None):\n",
        "        self.records = []\n",
        "\n",
        "        if model_metadata is not None:\n",
        "            self.model_metadata = flatten_dict({'model': model_metadata.params})\n",
        "\n",
        "\n",
        "    def set_model_metadata(self, model_metadata):\n",
        "        \"\"\"\n",
        "        Initialize the static model data to be populated across the iteratively-\n",
        "        added training results\n",
        "        \"\"\"\n",
        "        self.model_metadata = flatten_dict({'model': model_metadata.params})\n",
        "\n",
        "\n",
        "    def add_eval_metrics(self, eval_metrics):\n",
        "        \"\"\"\n",
        "        Append training metric\n",
        "        \"\"\"\n",
        "\n",
        "        self.records.append(eval_metrics)\n",
        "\n",
        "\n",
        "    def aggregate(self, on_task_id=None, on_task_version=None, epoch=None):\n",
        "        \"\"\"\n",
        "        Collates the iteratively-added metrics\n",
        "\n",
        "        Returns:\n",
        "            :obj:`dict`: the aggregated epochs' metrics\n",
        "        \"\"\"\n",
        "        ret_recs = []\n",
        "        for record in self.records:\n",
        "            if on_task_id is not None and on_task_version is not None:\n",
        "                # aggregate on task ID and version\n",
        "                if record.task_metadata['task.id'] != on_task_id or \\\n",
        "                record.task_metadata['task.version'] != on_task_version:\n",
        "                    # skip\n",
        "                    continue\n",
        "            elif on_task_id is not None:\n",
        "                # aggregate on task ID\n",
        "                if record.task_metadata['task.id'] != on_task_id:\n",
        "                    # skip\n",
        "                    continue\n",
        "\n",
        "            agg_r = record.aggregate()\n",
        "\n",
        "            collated = [{**self.model_metadata, **sub_rec} \\\n",
        "                        for sub_rec in agg_r]\n",
        "\n",
        "            ret_recs.extend(collated)\n",
        "\n",
        "        if epoch:\n",
        "            if epoch >= len(ret_recs) or epoch < -len(ret_recs):\n",
        "                # out of range\n",
        "                return {}\n",
        "            return ret_recs[epoch]\n",
        "\n",
        "        return ret_recs\n",
        "\n",
        "\n",
        "    def save(self, filename, dir='./model_metrics/'):\n",
        "        \"\"\"\n",
        "        Saves model metrics (the accumulated metrics of all models can be\n",
        "        saved through :class:`~OptimizerMetrics`)\n",
        "        \"\"\"\n",
        "\n",
        "        dir_path = os.path.join(Config.BASE_PATH, dir)\n",
        "        full_path = os.path.join(dir_path, filename)\n",
        "\n",
        "        ensure_dir(dir_path, True)\n",
        "\n",
        "        pd.DataFrame(self.aggregate()).to_csv(full_path)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert isinstance(idx, int) or isinstance(idx, str), (\n",
        "            'ModelMetrics subscripts must be `int` or `str`'\n",
        "        )\n",
        "\n",
        "        if isinstance(idx, str):\n",
        "            # aggregate model data and records\n",
        "            agg_recs = self.aggregate()\n",
        "\n",
        "            vals = []\n",
        "            for record in agg_recs:\n",
        "                if idx not in record:\n",
        "                    raise KeyError(f'ModelMetric key \"{idx}\" does not exist')\n",
        "                vals.append(record[idx])\n",
        "            return vals\n",
        "\n",
        "        elif isinstance(idx, int):\n",
        "            return self.aggregate()[idx]\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.aggregate())\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsIzsuo1bBaO"
      },
      "source": [
        "### Optimizer Metrics\n",
        "\n",
        "<!---  \n",
        "$file=optimizer_metrics.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVMcm7HTCwPT"
      },
      "outputs": [],
      "source": [
        "\n",
        "class OptimizerMetrics:\n",
        "    \"\"\"\n",
        "    The performance metrics of all evaluated models (collection of\n",
        "    :class:`~ModelMetrics` objects)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        self.records = pd.DataFrame()\n",
        "\n",
        "\n",
        "    def add_results(self, model_metadata):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        self.records = pd.concat([self.records, pd.DataFrame(model_metadata)],\n",
        "                                 ignore_index=True)\n",
        "\n",
        "\n",
        "    def model_exists(self, model):\n",
        "        if 'model_hash' not in self.records:\n",
        "            return False\n",
        "\n",
        "        return hash(model) in self.records['model_hash'].values\n",
        "\n",
        "\n",
        "    def save(self, filename, dir='./nas_results/'):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        dir_path = os.path.join(Config.BASE_PATH, dir)\n",
        "        full_path = os.path.join(dir_path, filename)\n",
        "\n",
        "        ensure_dir(dir_path, True)\n",
        "\n",
        "        self.records.to_csv(full_path)\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.records)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System Metrics\n",
        "\n",
        "<!---  \n",
        "$file=sys_metrics.py\n",
        "-->"
      ],
      "metadata": {
        "id": "1UGDthT6EB08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SystemMetrics:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.records = []\n",
        "\n",
        "\n",
        "    def add_record(self, task_id, task_version, task_name,\n",
        "                   nas_epoch, sys_usage):\n",
        "        df_dict = {\n",
        "            'task_id': task_id,\n",
        "            'task_version': task_version,\n",
        "            'task_name': task_name,\n",
        "            'nas_idx': nas_epoch,\n",
        "            'time': time.strftime('%Y/%m/%d, %H:%M:%S', time.localtime())\n",
        "        }\n",
        "        df_dict = {**df_dict, **sys_usage}\n",
        "\n",
        "        self.records.append(df_dict)\n",
        "\n",
        "\n",
        "\n",
        "    def save(self, filename, dir='./sys_usage/'):\n",
        "        dir_path = os.path.join(Config.BASE_PATH, dir)\n",
        "        full_path = os.path.join(dir_path, filename)\n",
        "\n",
        "        ensure_dir(dir_path, True)\n",
        "\n",
        "        pd.DataFrame(self.records).to_csv(full_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "nylRGpWkEFhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSBFj6fbzUH8"
      },
      "source": [
        "## Scoring Functions\n",
        "\n",
        "<!---  \n",
        "$module=scoring\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Classification Scoring\n",
        "\n",
        "<!---  \n",
        "$file=img_classification_scoring.py\n",
        "-->"
      ],
      "metadata": {
        "id": "-Hg8G6cOyN6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjqRa3V1zVv7"
      },
      "outputs": [],
      "source": [
        "def default_img_classification_scoring(models, task):\n",
        "    \"\"\"\n",
        "    Default candidate scoring function for image classification tasks.\n",
        "    Since the scoring is relative, more than 1 model (typically best model\n",
        "    + currently evaluated model) should be provided.\n",
        "\n",
        "    The calculation involves normalizing each metric feature relative to\n",
        "    the same feature in other models (i.e. each column in the metrics\n",
        "    dataset).\n",
        "\n",
        "    Args:\n",
        "        models (:obj:`list`): list of :class:`~Network` objects to score\n",
        "        task (:class:`~BaseTask`): the task to score the given models based \\\n",
        "        upon. We use the task's ID and version to filter the model metrics, \\\n",
        "        and the objectives/score weights to proportionally aggregate all given \\\n",
        "        objectives into a single-objective (scalar) problem.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`list`: list of normalized scalar scores for each given model \\\n",
        "        (order is preserved and ensured to be the same as the passed list \\\n",
        "        of models).\n",
        "    \"\"\"\n",
        "\n",
        "    task_id = task.id\n",
        "    task_version = task.version\n",
        "    objective = task.objective\n",
        "    weights = objective.score_weights\n",
        "\n",
        "    # normalize weights (whilst preserving inversions)\n",
        "    total_weight = sum(abs(v) for v in weights.values())\n",
        "    normalized_weights = {k: v / total_weight for k, v in weights.items()}\n",
        "\n",
        "    # structure model metrics;\n",
        "    # filter for given task id and get last epoch's results\n",
        "    metrics = [model.metrics.aggregate(on_task_id=task_id,\n",
        "                                       on_task_version=task.version,\n",
        "                                       epoch=-1)\\\n",
        "               for model in models]\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        # filter metrics based on weights' key-set intersection\n",
        "        metrics[i] = { key: metric[key] \\\n",
        "                      for key in metric if key in normalized_weights }\n",
        "\n",
        "    # scale model metrics to conserve proportionality, then apply weights\n",
        "    for k in normalized_weights:\n",
        "        if not any([True for metric in metrics if k in metric]):\n",
        "            # weight key not found\n",
        "            continue\n",
        "\n",
        "        max_val = max([abs(metric[k]) for metric in metrics if k in metric])\n",
        "        for i, metric in enumerate(metrics):\n",
        "            if k not in metric:\n",
        "                # raise KeyError(f'Key {k} not found in a models\\' metrics')\n",
        "                # ^commented out, sometimes `metric` is `{}`\n",
        "                continue\n",
        "\n",
        "            if max_val == 0:\n",
        "                # avoid division by 0\n",
        "                Logger.debug('Scoring Function - Absolute Max Val is `0`', k)\n",
        "                metrics[i][k] = 0\n",
        "                continue\n",
        "\n",
        "            metrics[i][k] = metric[k] / max_val * normalized_weights[k]\n",
        "\n",
        "    scalar_scores = [sum(list(metric.values())) for metric in metrics]\n",
        "\n",
        "    return scalar_scores   # scalar scores' list; 1 for each model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWoyrVFaYv6k"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "<!---  \n",
        "$module=visualization\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=visualization.py\n",
        "-->"
      ],
      "metadata": {
        "id": "lu7P5HuTycJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4RGbraN3cih"
      },
      "outputs": [],
      "source": [
        "def visualize_network(graph, filename=None,\n",
        "                      dir='./plots/', show_plot=True):\n",
        "\n",
        "    assert len(graph.nodes()) > 0, (\n",
        "        'The network must be compiled prior to visualization'\n",
        "    )\n",
        "\n",
        "    if filename is None and not show_plot:\n",
        "        # not saving or showing the plot\n",
        "        return\n",
        "\n",
        "    # macros\n",
        "    node_radius = 0.04\n",
        "    box_height = 0.05\n",
        "    box_width = 0.225\n",
        "    text_pad = 0.01  # padding inside the box\n",
        "\n",
        "    # Step 1: assign hierarchical levels\n",
        "    def assign_levels(G):\n",
        "        levels = {}\n",
        "        for node in nx.topological_sort(G):\n",
        "            preds = list(G.predecessors(node))\n",
        "            level = 0 if not preds else max(levels[pred] for pred in preds) + 1\n",
        "            levels[node] = level\n",
        "        return levels\n",
        "\n",
        "    # Step 2: calculate node positions (both x & y)\n",
        "    def calculate_positions(graph, levels, x_spacing_factor, y_spacing_factor):\n",
        "        positions = {}\n",
        "        max_depth = max(levels.values())\n",
        "\n",
        "        level_heights = {}\n",
        "\n",
        "        for node, level in levels.items():\n",
        "            if level not in level_heights:\n",
        "                level_heights[level] = 0\n",
        "            level_heights[level] += 1\n",
        "\n",
        "        level_idx = {level: 0 for level in level_heights}\n",
        "\n",
        "        for node in graph.nodes():\n",
        "            level = levels[node]\n",
        "            x_pos = level / max_depth\n",
        "\n",
        "            y_pos = (level_idx[level] + 0.5) / level_heights[level]\n",
        "            level_idx[level] += 1\n",
        "            positions[node] = (x_pos * x_spacing_factor,\n",
        "                               y_pos * y_spacing_factor)\n",
        "\n",
        "        return positions\n",
        "\n",
        "\n",
        "    # Step 3: draw the network\n",
        "    def draw_neural_network(G, ax, legend_names=False):\n",
        "        levels = assign_levels(graph)\n",
        "        pos = calculate_positions(graph, levels, 1.25, 1.25)\n",
        "\n",
        "        nodes_per_level = {}\n",
        "        for node, level in levels.items():\n",
        "            if level not in nodes_per_level:\n",
        "                nodes_per_level[level] = 0\n",
        "            nodes_per_level[level] += 1\n",
        "\n",
        "        # plot limits calc\n",
        "        all_x = [x for x, y in pos.values()]\n",
        "        all_y = [y for x, y in pos.values()]\n",
        "\n",
        "        margin_x = 0.25  # margin outermost nodes for a bit of space\n",
        "        margin_y = 0.30\n",
        "\n",
        "        ax.set_xlim(min(all_x) - margin_x,\n",
        "                    max(all_x) + margin_x)\n",
        "        ax.set_ylim(min(all_y) - margin_y,\n",
        "                    max(all_y) + margin_y)\n",
        "\n",
        "        # draw edges (with a low z-order so they'd be covered by the nodes)\n",
        "        for edge in G.edges():\n",
        "            # x0, y0 = pos[edge[0]]\n",
        "            # x1, y1 = pos[edge[1]]\n",
        "\n",
        "            # ax.plot([x0, x1], [y0, y1], 'lightgrey', lw=1, zorder=1)\n",
        "\n",
        "            start, end = pos[edge[0]], pos[edge[1]]\n",
        "            level_diff = abs(levels[edge[0]] - levels[edge[1]])\n",
        "\n",
        "            # number of nodes at each level\n",
        "            diff_nodes_lvl = nodes_per_level[levels[edge[0]]] \\\n",
        "            != nodes_per_level[levels[edge[1]]]\n",
        "\n",
        "            if level_diff > 1 and not diff_nodes_lvl:\n",
        "                # curved edges for non-adjacent levels / same node counts\n",
        "                control = [(start[0] + end[0]) / 2,\n",
        "                           (start[1] + end[1]) / 2 + 0.1 * level_diff]\n",
        "                path = Path([start, control, end],\n",
        "                            [Path.MOVETO, Path.CURVE3, Path.CURVE3])\n",
        "                patch = PathPatch(path, facecolor='none', lw=1,\n",
        "                                  edgecolor='lightgrey', zorder=1)\n",
        "                ax.add_patch(patch)\n",
        "            else:\n",
        "                # straight edge path otherwise\n",
        "                ax.plot([start[0], end[0]], [start[1], end[1]],\n",
        "                        color='lightgrey', lw=1, zorder=1)\n",
        "\n",
        "\n",
        "        for node, (x, y) in pos.items():\n",
        "            # draw the node circle\n",
        "            circle = Circle(pos[node], radius=node_radius, edgecolor='grey',\n",
        "                            facecolor=node.op_color, lw=2, alpha=1.0, zorder=3)\n",
        "            ax.add_patch(circle)\n",
        "            # layer IDs in the middle of the node\n",
        "            plt.text(pos[node][0], pos[node][1], str(node.id),\n",
        "                     ha='center', va='center', color='white', fontsize=8)\n",
        "\n",
        "            if not legend_names:\n",
        "                # draw text box\n",
        "                rect = plt.Rectangle((x - box_width / 2,\n",
        "                                    y - node_radius - box_height - text_pad),\n",
        "                                    box_width, box_height,\n",
        "                                    linewidth=1, edgecolor='black',\n",
        "                                    facecolor='grey', alpha=0.8, zorder=4)\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(x, y - node_radius - box_height / 2 - text_pad,\n",
        "                        node.op_name, ha='center', va='center',\n",
        "                        color='white', fontsize=6, zorder=5)\n",
        "            else:\n",
        "                # draw legend (mapping IDs to layer names)\n",
        "                legend_handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                                             label=f'{node.id}: {node.op_name}',\n",
        "                                             markerfacecolor=node.op_color,\n",
        "                                             markersize=10,\n",
        "                                             alpha=0.75) \\\n",
        "                                  for node in graph.nodes()]\n",
        "                ax.legend(handles=legend_handles, bbox_to_anchor=(1.05, 1),\n",
        "                          loc='upper left')\n",
        "\n",
        "        version_text = f' -- v{graph.version}' if graph.version > 1 else ''\n",
        "        plt.text(x=0.025, y=0.975,\n",
        "                 s=f'MODEL ID ({graph.id}{version_text})',\n",
        "                 transform=plt.gca().transAxes, fontsize=5, color='grey',\n",
        "                 verticalalignment='top', style='italic')\n",
        "\n",
        "\n",
        "    # draw\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    # use legend_names by default as it is cleaner visually\n",
        "    draw_neural_network(graph, ax, legend_names=False)\n",
        "\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.set_aspect('equal')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if filename is not None:\n",
        "        dir_path = os.path.join(Config.BASE_PATH, dir)\n",
        "        full_path = os.path.join(dir_path, filename)\n",
        "\n",
        "        ensure_dir(dir_path, True)\n",
        "\n",
        "        plt.savefig(full_path)\n",
        "\n",
        "    if show_plot:\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$ignore-cell=True\n",
        "-->"
      ],
      "metadata": {
        "id": "hPA3Ubo87PGW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1sUD3AVYyAT"
      },
      "outputs": [],
      "source": [
        "\"\"\" DEPRECATED\n",
        "\"\"\"\n",
        "\n",
        "def __visualize_network(graph, dir='./plots/', filename=None, show_plot=True):\n",
        "\n",
        "    assert len(graph.nodes()) > 0, (\n",
        "        'The network must be compiled prior to visualization'\n",
        "    )\n",
        "\n",
        "    if filename is None and not show_plot:\n",
        "        # not saving or showing the plot\n",
        "        return\n",
        "\n",
        "    adj_matrix = graph.adj_matrix\n",
        "    num_nodes = adj_matrix.shape[0]\n",
        "\n",
        "    # --== start sub-methods\n",
        "    def nodes_pos():\n",
        "        pos = {}\n",
        "        max_depth = max(nx.get_node_attributes(graph, 'depth').values())\n",
        "        for depth in range(max_depth + 1):\n",
        "            nodes_at_depth = [node[0] for node in graph.nodes(data=True) \\\n",
        "                              if node[1]['depth'] == depth]\n",
        "            max_width = max([len(node.op_name) for node in nodes_at_depth])\n",
        "            for i, node in enumerate(nodes_at_depth):\n",
        "                y_pos = i - (len(nodes_at_depth) - 1) / 2\n",
        "                # symmetrical across the x-axis\n",
        "                pos[node] = (-(max_depth * 2 - depth) * 1.2, y_pos)\n",
        "\n",
        "        return pos\n",
        "\n",
        "    def draw_nodes(pos, ax, **kwargs):\n",
        "        path = None\n",
        "        for node in graph.nodes():\n",
        "            # label = '\\n'.join(textwrap.wrap(node.op_name, 15))\n",
        "            # node_x, node_y = pos[node]\n",
        "            # rect = FancyBboxPatch((node_x - 0.45, node_y - 0.075), 0.9, 0.15,\n",
        "            #                       boxstyle='round,pad=0.02',\n",
        "            #                       ec='black', fc=node.op_color, alpha=0.4)\n",
        "            # # rect = plt.Rectangle((node_x - 0.45, node_y - 0.075), 0.9, 0.15,\n",
        "            # #                      facecolor='#fff', edgecolor=node.op_color,\n",
        "            # #                      **kwargs)\n",
        "            # ax.add_patch(rect)\n",
        "            # ax.text(node_x, node_y, label, fontsize=8, ha='center',\n",
        "            #         va='center')\n",
        "            x, y = pos[node]\n",
        "            label = '\\n'.join(textwrap.wrap(node.op_name, 20))\n",
        "            text_obj = ax.text(x, y, label, fontsize=8,\n",
        "                               ha='center', va='center')\n",
        "\n",
        "            renderer = fig.canvas.get_renderer()\n",
        "            text_width = text_obj.get_window_extent(renderer).width\n",
        "\n",
        "            bbox_width = text_width / fig.dpi * 0.9\n",
        "            bbox_height = 0.1\n",
        "            text_obj.remove()\n",
        "            rect = FancyBboxPatch((x - bbox_width / 2, y - bbox_height / 2),\n",
        "                                  bbox_width, bbox_height,\n",
        "                                boxstyle='round,pad=0.02', ec='black',\n",
        "                                  fc=node.op_color, alpha=0.4)\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(x, y, label, fontsize=8, ha='center', va='center')\n",
        "            if path is None:\n",
        "                path = rect.get_path()\n",
        "\n",
        "        return path\n",
        "    # --== end sub-methods\n",
        "\n",
        "    pred = [list(graph.successors(node)) for node in graph.nodes()]\n",
        "\n",
        "    # # remove isolates\n",
        "    # for idx, node_conns in enumerate(input_conns):\n",
        "    #     if len(node_conns) == 0 and idx > 0:\n",
        "    #         graph.remove_node(idx)\n",
        "\n",
        "\n",
        "    # Logger.debug(graph.nodes(), '\\n', graph.adj_matrix, '\\n',\n",
        "    #              graph.compiled_ops)\n",
        "    # node_labels = {node: node for node in graph.nodes()}\n",
        "    # node_shapes = {node: 's' for node in graph.nodes()}\n",
        "\n",
        "    custom_nn_pos = nodes_pos()\n",
        "    # draw\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "    nx.draw_networkx_edges(graph, custom_nn_pos, ax=ax, node_shape='s',\n",
        "                           node_size=10, alpha=0.25)\n",
        "                            #, connectionstyle='arc3, rad=0.05')\n",
        "    draw_path = draw_nodes(custom_nn_pos, ax, linewidth=1.5, alpha=1.0)\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "    if filename is not None:\n",
        "        dir_path = os.path.join(Config.BASE_PATH, dir)\n",
        "        full_path = os.path.join(dir_path, filename)\n",
        "\n",
        "        ensure_dir(dir_path, True)\n",
        "\n",
        "        plt.savefig(full_path)\n",
        "\n",
        "    if show_plot:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # nx.draw(self, pos=custom_nn_pos, with_labels=False,\n",
        "    #         node_size=500, node_color='lightblue',\n",
        "    #         font_size=8, font_color='black', node_shape='s', alpha=0.9)\n",
        "\n",
        "    # nx.draw_networkx_labels(self, custom_nn_pos,\n",
        "    #                         labels=node_labels, font_size=8,\n",
        "    #                         font_color='black', verticalalignment='bottom',\n",
        "    #                         horizontalalignment='center')\n",
        "\n",
        "\n",
        "\n",
        "# def alt_visualize(G, node_labels=None):\n",
        "#     \"\"\"\n",
        "#     Draw a customized representation of a neural network with consistent\n",
        "#     spacing between nodes.\n",
        "\n",
        "#     Args:\n",
        "#     layers (list of int): A list containing the number of nodes in each layer.\n",
        "#     node_labels (dict): Optional. A dictionary with node index as keys and \\\n",
        "#                         labels as values.\n",
        "#                         If None, nodes are labeled with their indices.\n",
        "#     \"\"\"\n",
        "\n",
        "#     layers = list(G.nodes())\n",
        "#     total_nodes = len(layers)\n",
        "#     pos = {}\n",
        "#     layer_positions = [len(layers[:i]) for i in range(len(layers) + 1)]\n",
        "#     bbox_sizes = {}\n",
        "#     # Dummy figure to calculate text sizes\n",
        "#     fig, ax = plt.subplots()\n",
        "#     for i, layer in enumerate(layers):\n",
        "#         max_width = 0\n",
        "#         for j in range(layer):\n",
        "#             node = layer_positions[i] + j\n",
        "#             G.add_node(node)\n",
        "#             label = node.op_name\n",
        "#             text_size = ax.text(x, y, label, fontsize=8,\n",
        "#                                 ha='center', va='center')\n",
        "#             renderer = fig.canvas.get_renderer()\n",
        "#             text_width = text_obj.get_window_extent(renderer).width\n",
        "#             text_height = text_obj.get_window_extent(renderer).height\n",
        "\n",
        "#             bbox_width = text_width / fig.dpi * 0.9\n",
        "#             bbox_height = text_height / fig.dpi * 0.75\n",
        "#             bbox_sizes[node] = (bbox_width, bbox_height)\n",
        "#             max_width = max(max_width, bbox_width)\n",
        "#         for j in range(layer):\n",
        "#             node = layer_positions[i] + j\n",
        "#             pos[node] = (i * (max_width + 0.2), -j * \\\n",
        "#                          (bbox_sizes[node][1] + 0.2))\n",
        "#     plt.close(fig)\n",
        "\n",
        "#     # Create edges\n",
        "#     for i in range(len(layers) - 1):\n",
        "#         for j in range(layers[i]):\n",
        "#             for k in range(layers[i + 1]):\n",
        "#                 G.add_edge(layer_positions[i] + j, layer_positions[i + 1] + k)\n",
        "\n",
        "#     # Draw the network with spacing\n",
        "#     fig, ax = plt.subplots()\n",
        "#     nx.draw_networkx_edges(G, pos, ax=ax, arrows=True)\n",
        "#     for node in G.nodes:\n",
        "#         x, y = pos[node]\n",
        "#         label = str(node_labels[node]) \\\n",
        "#                 if node_labels and node in node_labels else str(node)\n",
        "#         bbox_width, bbox_height = bbox_sizes[node]\n",
        "#         bbox = FancyBboxPatch((x - bbox_width / 2, y - bbox_height / 2),\n",
        "#                               bbox_width, bbox_height,\n",
        "#                               boxstyle=\"round,pad=0.02\", ec=\"black\",\n",
        "#                               fc=node.op_color, alpha=0.7)\n",
        "#         ax.add_patch(bbox)\n",
        "#         ax.text(x, y, label, fontsize=8, ha='center', va='center')\n",
        "\n",
        "#     plt.title(\"Artificial Neural Network Architecture\")\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUy5GIyUYCSj"
      },
      "source": [
        "## Search Space\n",
        "\n",
        "<!---  \n",
        "$module=search_space\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY9FTAiLZMpa"
      },
      "source": [
        "### Base Operation\n",
        "\n",
        "<!---  \n",
        "$module=operations\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=base_operation.py\n",
        "-->"
      ],
      "metadata": {
        "id": "xQVnWPwO6Zjk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z5XoXuwZJEf"
      },
      "outputs": [],
      "source": [
        "class BaseOperation(nn.Module, abc.ABC):\n",
        "    \"\"\"\n",
        "    Abstract skeleton for operations on the edges of the \\\n",
        "    search space graph.\n",
        "\n",
        "    Supported operations are primitive operations and compound primitives \\\n",
        "    (i.e. a combination of primitives as an individual building block).\n",
        "\n",
        "    Hierarchical structures can be formed through recursive Network \\\n",
        "    objects, where the first stage is composed of `BaseOperation` objects.\n",
        "\n",
        "    Each :class:`~BaseOperation` must have an `id` attribute\n",
        "    \"\"\"\n",
        "\n",
        "    OPERATION_TYPE = None       # type of operation; spatial, reduction, etc.\n",
        "    HYPERPARAMS = {}\n",
        "\n",
        "    __ID_TRACKER = 0\n",
        "\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        # super().__init__()\n",
        "\n",
        "        # explicit superclass init to ensure expected MRO\n",
        "        nn.Module.__init__(self)\n",
        "        abc.ABC.__init__(self)\n",
        "\n",
        "        if 'id' in kwargs:\n",
        "            assert type(kwargs['id']) == int, 'Invalid operation ID provided'\n",
        "            self.id = kwargs['id']\n",
        "            # update auto-ID\n",
        "            BaseOperation.__ID_TRACKER = max(BaseOperation.__ID_TRACKER,\n",
        "                                             self.id)\n",
        "        else:\n",
        "            self.id = BaseOperation.__ID_TRACKER\n",
        "            BaseOperation.__ID_TRACKER += 1\n",
        "\n",
        "        self.in_shape = in_shape\n",
        "        self.out_shape = out_shape\n",
        "\n",
        "        # The block below is now deprecated;\n",
        "        # DFS is implemented to traverse the graph and find the nearest\n",
        "        # previous `out_channels`\n",
        "        # if kwargs and 'in_channels' in kwargs:\n",
        "        #     # pass `out_channels` for operations that do not require\n",
        "        #     # `in_channels`\n",
        "        #     # e.g. ReLU, BatchNorm, etc.\n",
        "        #     self.out_channels = kwargs['in_channels']\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, x, edge_data):\n",
        "        \"\"\"\n",
        "        The nn module forward operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`~torch.FloatTensor`): the input feature tensor\n",
        "            edge_data (dict): addtional kwargs to be stored on the edge\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n",
        "\n",
        "    @property\n",
        "    def op_name(self):\n",
        "        \"\"\"\n",
        "        Operation hash function. This method embeds both the operation type\n",
        "        and its hyperparameters into the name.\n",
        "\n",
        "        This property is used for the hashing and equality of operations,\n",
        "        should return a *unique* name.\n",
        "\n",
        "        Since operations could repeat with the same exact hyperaparameters in an\n",
        "        architecture, we use the index of the operation in the architecture to\n",
        "        make it distinct\n",
        "\n",
        "        Returns:\n",
        "            str: the operation's inferred name\n",
        "        \"\"\"\n",
        "        cls = self.__class__\n",
        "        id = self.id if hasattr(self, 'id') else -1\n",
        "        ret_name = f'(#{str(id)}) {cls.__name__}'\n",
        "\n",
        "        if hasattr(cls, 'HYPERPARAMS'):\n",
        "            for p, _ in cls.HYPERPARAMS.items():\n",
        "                if hasattr(self, p):\n",
        "                    hp_initials = ''.join([word[0].lower() \\\n",
        "                                           for word in p.split('_')])\n",
        "                    ret_name += f'-{hp_initials}({getattr(self, p)})'\n",
        "\n",
        "        return ret_name\n",
        "\n",
        "\n",
        "    @property\n",
        "    def op_color(self):\n",
        "        \"\"\"\n",
        "        Operation color getter\n",
        "\n",
        "        Returns:\n",
        "            str: the operation's visualization color\n",
        "        \"\"\"\n",
        "        if not hasattr(type(self), 'OPERATION_TYPE'):\n",
        "            return '#34495e'    # asphalt (grey-blue)\n",
        "\n",
        "        if type(self).OPERATION_TYPE == OperationType.SPATIAL_OP:\n",
        "            return '#2ecc71'    # emerald (light green)\n",
        "        elif type(self).OPERATION_TYPE == OperationType.REDUCTION_OP:\n",
        "            return '#3498db'    # peter river (sky blue)\n",
        "        elif type(self).OPERATION_TYPE == OperationType.NORMALIZATION_OP:\n",
        "            return '#e74c3c'    # crayola red (bright red)\n",
        "        elif type(self).OPERATION_TYPE == OperationType.ACTIVATION_OP:\n",
        "            return '#f1c40f'    # sunflower (yellow)\n",
        "        elif type(self).OPERATION_TYPE == OperationType.COMPOSITE_OP:\n",
        "            return '#9b59b6'    # amethyst (light purple)\n",
        "        elif type(self).OPERATION_TYPE == OperationType.ROLE_SPECIFIC_OP:\n",
        "            return '#2c3e50'    # midnight blue (grey-dark blue)\n",
        "        else:\n",
        "            return '#34495e'    # asphalt (grey-blue)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    # @lru_cache(maxsize=32)    # lru will not perform well given the random\n",
        "                                # nature of the sampler\n",
        "                                # we lazy-init/cache statically within the\n",
        "                                # subclass instead\n",
        "    def get_all_partials(cls):\n",
        "        \"\"\"\n",
        "        Gets all combinations of hyperparameters for a given \\\n",
        "        :class:`~BaseOperation` subclass using its respective static \\\n",
        "        `HYPERPARAMETERS` attribute\n",
        "\n",
        "        Returns:\n",
        "            list: a list of PartialWrapper partial functions of the \\\n",
        "            :class:`~BaseOperation`'s subclass (cls) pre-initialized with \\\n",
        "            all combinations of hyperaparameters\n",
        "        \"\"\"\n",
        "\n",
        "        if hasattr(cls, '__ALL_PARTIALS'):\n",
        "            # cached\n",
        "            return cls.__ALL_PARTIALS\n",
        "\n",
        "        ret_list = []\n",
        "\n",
        "        if hasattr(cls, 'HYPERPARAMS'):\n",
        "            params_dict = cls.HYPERPARAMS\n",
        "            param_keys = list(params_dict.keys())\n",
        "            combs = itertools.product(*params_dict.values())\n",
        "\n",
        "            for hyperparams in combs:\n",
        "                # generate wrappers for every hyperparam comb\n",
        "                hyperparam_dict = dict(zip(param_keys, hyperparams))\n",
        "                ret_list.append(PartialWrapper(cls, **hyperparam_dict))\n",
        "        else:\n",
        "            # hyperparameter-less operation\n",
        "            ret_list.append(PartialWrapper(cls))\n",
        "\n",
        "        cls.__ALL_PARTIALS = ret_list\n",
        "\n",
        "        return ret_list\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def deserialize(cls, args):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        return cls(**args)\n",
        "\n",
        "\n",
        "    def serialize(self):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        return json.dumps(self.metadata.params)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        hyperparams = {}\n",
        "        # operation-specific hyperparameters\n",
        "        if hasattr(type(self), 'HYPERPARAMS'):\n",
        "            params_dict = type(self).HYPERPARAMS\n",
        "            for p, _ in params_dict.items():\n",
        "                if hasattr(self, p):\n",
        "                    hyperparams[p] = getattr(self, p)\n",
        "\n",
        "        return OperationMetadata(op=type(self).__name__,\n",
        "                                 id=self.id,\n",
        "                                 in_shape=self.in_shape,\n",
        "                                 out_shape=self.out_shape,\n",
        "                                 **hyperparams)\n",
        "\n",
        "\n",
        "    def decompile(self):\n",
        "        \"\"\"\n",
        "        Decompiles the operation and returns a representative\n",
        "        :class:`PartialWrapper`.\n",
        "        \"\"\"\n",
        "        kwargs = {}\n",
        "        # operation-specific hyperparameters\n",
        "        if hasattr(type(self), 'HYPERPARAMS'):\n",
        "            params_dict = type(self).HYPERPARAMS\n",
        "            for p, _ in params_dict.items():\n",
        "                if hasattr(self, p):\n",
        "                    kwargs[p] = getattr(self, p)\n",
        "        kwargs['id'] = self.id\n",
        "        kwargs['in_shape'] = self.in_shape\n",
        "        kwargs['out_shape'] = self.out_shape\n",
        "\n",
        "        return PartialWrapper(type(self), **kwargs)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def reset_auto_id():\n",
        "        BaseOperation.__ID_TRACKER = 0\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_auto_id():\n",
        "         return BaseOperation.__ID_TRACKER\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        md = self.metadata\n",
        "        return f'{md.op}({str(md)})'\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "\n",
        "    def __hash__(self):\n",
        "        \"\"\"\n",
        "        Hashing the metadata of the operation as that is enforced to be\n",
        "        unique.\n",
        "        \"\"\"\n",
        "\n",
        "        return hash(str(self))\n",
        "\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, self.__class__):\n",
        "            return False\n",
        "\n",
        "        return self.metadata == other.metadata\n",
        "\n",
        "\n",
        "class OperationType(Enum):\n",
        "    SPATIAL_OP          = auto()     # Feature extraction and dimension\n",
        "                                     # preservation (conv, identity, etc.)\n",
        "    REDUCTION_OP        = auto()     # Dimension reduction (max_pool, avg_pool,\n",
        "                                     # strided conv, etc.)\n",
        "    NORMALIZATION_OP    = auto()     # Transformations and regularization\n",
        "                                     # (batch norm, group norm, etc.)\n",
        "    ACTIVATION_OP       = auto()     # Non-linear functions\n",
        "                                     # (ReLU, H-Swish, etc.)\n",
        "    COMPOSITE_OP        = auto()     # Grouped building blocks (ReLUConvBN,\n",
        "                                     # inception cell, hierarchical motif, etc.)\n",
        "    ROLE_SPECIFIC_OP    = auto()     # Rule-based operations that cannot be\n",
        "                                     # sampled (InputStem, OutputStem)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMdxXYAMe7JR"
      },
      "source": [
        "#### Searchable Operations\n",
        "\n",
        "<!---  \n",
        "$file=searchable_operation.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GJgZr4gWwWj"
      },
      "outputs": [],
      "source": [
        "class AbstractSearchableOperation(BaseOperation):\n",
        "    \"\"\"\n",
        "    Purely virtual class used to distinguish between the generic, sample-able\n",
        "    operations and role-specific operations (primarily input/output stems).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(AbstractSearchableOperation, self).__init__(in_shape,\n",
        "                                                          out_shape,\n",
        "                                                          **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=fe_operations.py\n",
        "-->"
      ],
      "metadata": {
        "id": "xDGu_0Wn8Hab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#=================================================================\n",
        "# FEATURE-EXTRACTION OPS\n",
        "#=================================================================\n",
        "\n",
        "\n",
        "class Conv2d(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.SPATIAL_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [8, 16, 32, 64, 128],\n",
        "        'kernel_size': [1, 3, 5, 7, 9, 11]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count,\n",
        "                 kernel_size, **kwargs):\n",
        "        super(Conv2d, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_count = filter_count\n",
        "        self.padding = (kernel_size - 1) // 2   # zero-padded to maintain input\n",
        "                                                # resolution\n",
        "        self.conv = nn.Conv2d(in_shape[1],\n",
        "                              filter_count,\n",
        "                              kernel_size,\n",
        "                              stride=1,\n",
        "                              padding=self.padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.conv(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                    out_shape=self.out_shape,\n",
        "                                    kernel_size=self.kernel_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(Conv2d.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=1,\n",
        "                                              padding='same',\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['stride'] = 1\n",
        "            partials[idx].keywords['dilation'] = 1\n",
        "            partials[idx].keywords['padding'] = 'same'\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class SepConv2d(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.SPATIAL_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [8, 16, 32, 64, 128],\n",
        "        'kernel_size': [1, 3, 5, 7, 9, 11]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count, kernel_size,\n",
        "                 **kwargs):\n",
        "        super(SepConv2d, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_count = filter_count\n",
        "        self.padding = (kernel_size - 1) // 2   # zero-padded to maintain input\n",
        "                                                # resolution\n",
        "        self.depthwise = nn.Conv2d(in_shape[1],\n",
        "                                   in_shape[1],\n",
        "                                   kernel_size,\n",
        "                                   stride=1,\n",
        "                                   padding=self.padding,\n",
        "                                   groups=in_shape[1])\n",
        "        self.pointwise = nn.Conv2d(in_shape[1], filter_count, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                    out_shape=self.out_shape,\n",
        "                                    kernel_size=self.kernel_size,\n",
        "                                    separable=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over\n",
        "        :func:`~BaseOperation.get_all_partials` for faster performance (the\n",
        "        cache uses minimal memory as these are uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(SepConv2d.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=1,\n",
        "                                              padding='same',\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['stride'] = 1\n",
        "            partials[idx].keywords['dilation'] = 1\n",
        "            partials[idx].keywords['padding'] = 'same'\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class DilatedConv2d(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.SPATIAL_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [8, 16, 32, 64, 128],\n",
        "        'kernel_size': [1, 3, 5, 7, 9, 11],\n",
        "        'dilation': [2, 3]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count, kernel_size,\n",
        "                 dilation, **kwargs):\n",
        "        super(DilatedConv2d, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_count = filter_count\n",
        "        self.padding = (kernel_size - 1) // 2  # zero-padded to maintain input\n",
        "                                               # resolution\n",
        "        self.dilation = dilation\n",
        "        self.conv = nn.Conv2d(in_shape[1],\n",
        "                              filter_count,\n",
        "                              kernel_size,\n",
        "                              stride=1,\n",
        "                              padding=self.padding,\n",
        "                              dilation=dilation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.conv(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                    out_shape=self.out_shape,\n",
        "                                    kernel_size=self.kernel_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(DilatedConv2d.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            dilation = p_op.keywords['dilation']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=1,\n",
        "                                              padding='same',\n",
        "                                              dilation=dilation)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['stride'] = 1\n",
        "            partials[idx].keywords['padding'] = 'same'\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class Identity(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.SPATIAL_OP\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(Identity, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(Identity.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n"
      ],
      "metadata": {
        "id": "fAS9O1CCAm4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=reduction_ops.py\n",
        "-->"
      ],
      "metadata": {
        "id": "JXXarW9yCBuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#=================================================================\n",
        "# REDUCTION OPS\n",
        "#=================================================================\n",
        "\n",
        "\n",
        "class MaxPool2d(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.REDUCTION_OP\n",
        "    HYPERPARAMS = {\n",
        "        'kernel_size': [2, 3],\n",
        "        'stride': [1, 2],\n",
        "        'padding': [0, 1, 2]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, kernel_size, stride, padding,\n",
        "                 **kwargs):\n",
        "        super(MaxPool2d, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.maxpool(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_pool_flops(out_shape=self.out_shape,\n",
        "                                    flops_per_element=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(MaxPool2d.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=in_shape[1],\n",
        "                                              stride=p_op.keywords['stride'],\n",
        "                                              padding=p_op.keywords['padding'],\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['dilation'] = 1\n",
        "            partials[idx].keywords['filter_count'] = in_shape[1]\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class AvgPool2d(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.REDUCTION_OP\n",
        "    HYPERPARAMS = {\n",
        "        'kernel_size': [2, 3],\n",
        "        'stride': [1, 2],\n",
        "        'padding': [0, 1, 2]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, kernel_size, stride, padding,\n",
        "                 **kwargs):\n",
        "        super(AvgPool2d, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.avgpool(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        # additions\n",
        "        fpe = (self.kernel_size**2 - 1) * self.in_shape[1] * \\\n",
        "        self.out_shape[2] * self.out_shape[3]\n",
        "\n",
        "        # divisions are accounted for below\n",
        "        return calculate_pool_flops(out_shape=self.out_shape,\n",
        "                                    flops_per_element=fpe)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(AvgPool2d.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=in_shape[1],\n",
        "                                              stride=p_op.keywords['stride'],\n",
        "                                              padding=p_op.keywords['padding'],\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['filter_count'] = in_shape[1]\n",
        "            partials[idx].keywords['dilation'] = 1\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class GlobalAvgPool2d(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.REDUCTION_OP\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(GlobalAvgPool2d, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return torch.mean(x, dim=(2, 3), keepdim=True)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        # we use the input shape below instead of the output shape since this\n",
        "        # is not a regular sliding window operation (padding-inference is not\n",
        "        # required). We add +1 to account for the single div. FLOP\n",
        "        return calculate_pool_flops(out_shape=self.in_shape,\n",
        "                                    flope_per_element=1) + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(GlobalAvgPool2d.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            out_shape = (*in_shape[:2], 1, 1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class TransformChannels(AbstractOperation):\n",
        "    \"\"\"\n",
        "    1x1 Conv used to transform the channel dimension\n",
        "    \"\"\"\n",
        "\n",
        "    OPERATION_TYPE = OperationType.REDUCTION_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [8, 16, 32, 64]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count, **kwargs):\n",
        "        super(TransformChannels, self).__init__(in_shape, out_shape,\n",
        "                                                **kwargs)\n",
        "\n",
        "        self.filter_count = filter_count\n",
        "        self.conv = nn.Conv2d(in_shape[1],\n",
        "                              filter_count,\n",
        "                              1,\n",
        "                              stride=1,\n",
        "                              padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        # this is a 1x1 convolution operation\n",
        "        return calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                    out_shape=self.out_shape,\n",
        "                                    kernel_size=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(TransformChannels.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=1,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=1,\n",
        "                                              padding=0,\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['kernel_size'] = 1\n",
        "            partials[idx].keywords['stride'] = 1\n",
        "            partials[idx].keywords['dilation'] = 1\n",
        "            partials[idx].keywords['padding'] = 0\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class ReduceResolution(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.REDUCTION_OP\n",
        "    HYPERPARAMS = {\n",
        "        'scale_factor': [2, 3]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, scale_factor, **kwargs):\n",
        "        super(ReduceResolution, self).__init__(in_shape, out_shape,\n",
        "                                               **kwargs)\n",
        "\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return nn.functional.interpolate(x, scale_factor=1 / self.scale_factor,\n",
        "                                         mode='bilinear', align_corners=False)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        # as per torchprofile's implementation for bilinear interpolation\n",
        "        # the flops_per_element is ~= 4\n",
        "        return calculate_pool_flops(out_shape=self.in_shape,\n",
        "                                    flops_per_element=4)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(ReduceResolution.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "\n",
        "            out_shape = (\n",
        "                in_shape[0],\n",
        "                in_shape[1],\n",
        "                in_shape[2] // p_op.keywords['scale_factor'],\n",
        "                in_shape[3] // p_op.keywords['scale_factor']\n",
        "            )\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class StridedConv2d(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.REDUCTION_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [8, 16, 32, 64, 128],\n",
        "        'kernel_size': [1, 3, 5, 7, 9],\n",
        "        'stride': [2, 3]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count, kernel_size,\n",
        "                 stride, **kwargs):\n",
        "        super(StridedConv2d, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_count = filter_count\n",
        "        self.stride = stride\n",
        "        self.conv = nn.Conv2d(in_shape[1],\n",
        "                              filter_count,\n",
        "                              kernel_size,\n",
        "                              stride=stride,\n",
        "                              padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.conv(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                    out_shape=self.out_shape,\n",
        "                                    kernel_size=self.kernel_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(StridedConv2d.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=p_op.keywords['stride'],\n",
        "                                              padding=0,\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['dilation'] = 1\n",
        "            partials[idx].keywords['padding'] = 0\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class StridedSepConv(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.REDUCTION_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [8, 16, 32, 64, 128],\n",
        "        'kernel_size': [1, 3, 5, 7, 9],\n",
        "        'stride': [2, 3]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count, kernel_size,\n",
        "                 stride, **kwargs):\n",
        "        super(StridedSepConv, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_count = filter_count\n",
        "        self.stride = stride\n",
        "        self.depthwise = nn.Conv2d(in_shape[1],\n",
        "                                   in_shape[1],\n",
        "                                   kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=0,\n",
        "                                   groups=in_shape[1])\n",
        "        self.pointwise = nn.Conv2d(in_shape[1], filter_count, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                    out_shape=self.out_shape,\n",
        "                                    kernel_size=self.kernel_size,\n",
        "                                    separable=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(StridedSepConv.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=p_op.keywords['stride'],\n",
        "                                              padding=0,\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['dilation'] = 1\n",
        "            partials[idx].keywords['padding'] = 0\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n"
      ],
      "metadata": {
        "id": "nR31rUMWCFwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=norm_ops.py\n",
        "-->"
      ],
      "metadata": {
        "id": "vNq8nrgVCLPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#=================================================================\n",
        "# NORMALIZATION OPS\n",
        "#=================================================================\n",
        "\n",
        "\n",
        "class BatchNormalization(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.NORMALIZATION_OP\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(BatchNormalization, self).__init__(in_shape,\n",
        "                                                 out_shape, **kwargs)\n",
        "\n",
        "        self.batch_norm = nn.BatchNorm2d(in_shape[1])  # channel-wise batch norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.batch_norm(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_norm_flops(in_shape=self.in_shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(BatchNormalization.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class LayerNormalization(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.NORMALIZATION_OP\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(LayerNormalization, self).__init__(in_shape,\n",
        "                                                 out_shape, **kwargs)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(in_shape[1:])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_norm_flops(in_shape=self.in_shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(LayerNormalization.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class GroupNormalization(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.NORMALIZATION_OP\n",
        "    HYPERPARAMS = {\n",
        "        'num_groups': [1, 4, 8, 16]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, num_groups, **kwargs):\n",
        "        super(GroupNormalization, self).__init__(in_shape, out_shape,\n",
        "                                                 **kwargs)\n",
        "\n",
        "        self.num_groups = num_groups\n",
        "        self.group_norm = nn.GroupNorm(num_groups, in_shape[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.group_norm(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_norm_flops(in_shape=self.in_shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(GroupNormalization.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        # validate operation (in_shape[1] must be divisible by num_groups)\n",
        "        partials = list(filter(lambda p: in_shape[1] % \\\n",
        "                               p.keywords['num_groups'] == 0, partials))\n",
        "\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "\n",
        "class InstanceNormalization(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.NORMALIZATION_OP\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(InstanceNormalization, self).__init__(in_shape,\n",
        "                                                    out_shape, **kwargs)\n",
        "\n",
        "        self.instance_norm = nn.InstanceNorm2d(in_shape[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.instance_norm(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_norm_flops(in_shape=self.in_shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(InstanceNormalization.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class Dropout(AbstractOperation):\n",
        "    OPERATION_TYPE = OperationType.NORMALIZATION_OP\n",
        "    HYPERPARAMS = {\n",
        "        'p': [0.25, 0.5]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, p, **kwargs):\n",
        "        super(Dropout, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.p = p\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        # if self.training:\n",
        "        #     mask = torch.bernoulli(torch.full(x.size(), 1 - self.p))\n",
        "        #     x = x * mask / (1 - self.p)\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        # dropout is typically negligible when calculating FLOPS (according to\n",
        "        # torchprofile implementation)\n",
        "        return 0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(Dropout.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n",
        "\n"
      ],
      "metadata": {
        "id": "c8O1n3-ECPCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=activation_ops.py\n",
        "-->"
      ],
      "metadata": {
        "id": "L2Jxg7sdCWCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#=================================================================\n",
        "# ACTIVATION OPS\n",
        "#=================================================================\n",
        "\n",
        "\n",
        "class ReLU(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.ACTIVATION_OP\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(ReLU, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return nn.functional.relu(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "        # activation functions are typically negligible when calculating FLOPS\n",
        "        # (according to torchprofile implementation)\n",
        "        return 0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(ReLU.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class Swish(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.ACTIVATION_OP\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(Swish, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "        # activation functions are typically negligible when calculating FLOPS\n",
        "        # (according to torchprofile implementation)\n",
        "        return 0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(Swish.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class HSwish(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.ACTIVATION_OP\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(HSwish, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return x * F.relu6(x + 3) / 6\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "        # activation functions are typically negligible when calculating FLOPS\n",
        "        # (according to torchprofile implementation)\n",
        "        return 0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(HSwish.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class LeakyReLU(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.ACTIVATION_OP\n",
        "    # inplace updates seem to be bugged\n",
        "    # (https://github.com/pytorch/pytorch/issues/104943)\n",
        "    # HYPERPARAMS = {\n",
        "    #     'learnable': [True, False]\n",
        "    # }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, learnable=False,\n",
        "                 negative_slope=0.01, **kwargs):\n",
        "        super(LeakyReLU, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.learnable = learnable\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n",
        "                                       #, inplace=learnable)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        return self.leaky_relu(x)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "        # activation functions are typically negligible when calculating FLOPS\n",
        "        # (according to torchprofile implementation)\n",
        "        return 0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(LeakyReLU.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = in_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "# class ParametricReLU(AbstractOperation):\n",
        "#     def __init__(self, num_parameters=1, **kwargs):\n",
        "#         super(ParametricReLU, self).__init__()\n",
        "#         self.prelu = nn.PReLU(num_parameters=num_parameters)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.prelu(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "3rHsOaaUCWZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=composite_ops.py\n",
        "-->"
      ],
      "metadata": {
        "id": "OAB69ieiCiDA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqK0YILotD0i"
      },
      "outputs": [],
      "source": [
        "\n",
        "#=================================================================\n",
        "# COMPOSITE OPS\n",
        "#=================================================================\n",
        "\n",
        "\n",
        "class ActivatedConv(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.COMPOSITE_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [8, 16, 32, 64, 128],\n",
        "        'kernel_size': [1, 3, 5, 7, 9],\n",
        "        'stride': [1, 2, 3],\n",
        "        'dilation': [1, 2, 3],\n",
        "        'activation': [ReLU, Swish, HSwish, LeakyReLU],\n",
        "        'separable': [True, False]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count,\n",
        "                 kernel_size, activation, stride, dilation, separable,\n",
        "                 **kwargs):\n",
        "        super(ActivatedConv, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_count = filter_count\n",
        "        self.padding = (kernel_size - 1) // 2\n",
        "        self.separable = separable\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "\n",
        "        if separable:\n",
        "            self.depthwise = nn.Conv2d(in_shape[1],\n",
        "                                       in_shape[1],\n",
        "                                       kernel_size,\n",
        "                                       padding=self.padding,\n",
        "                                       groups=in_shape[1])\n",
        "            self.pointwise = nn.Conv2d(in_shape[1], filter_count, 1)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(in_shape[1],\n",
        "                                  filter_count,\n",
        "                                  kernel_size,\n",
        "                                  stride=stride,\n",
        "                                  padding=self.padding,\n",
        "                                  dilation=dilation)\n",
        "\n",
        "        self.activation = activation(id=-1,\n",
        "                                     in_shape=out_shape, out_shape=out_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        if separable:\n",
        "            x = self.depthwise(x)\n",
        "            x = self.pointwise(x)\n",
        "        else:\n",
        "            x = self.conv()\n",
        "\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        return calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                    out_shape=self.out_shape,\n",
        "                                    kernel_size=self.kernel_size,\n",
        "                                    separable=self.separable)\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(ActivatedConv.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=1,\n",
        "                                              padding='same',\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['padding'] = 'same'\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class ConvBnReLUBlock(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.COMPOSITE_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [8, 16, 32, 64, 128],\n",
        "        'kernel_size': [1, 3, 5]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count,\n",
        "                 kernel_size, **kwargs):\n",
        "        super(ConvBnReLUBlock, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_count = filter_count\n",
        "        self.padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(in_shape[1],\n",
        "                              filter_count,\n",
        "                              kernel_size,\n",
        "                              stride=1,\n",
        "                              padding=self.padding)\n",
        "        self.bn = nn.BatchNorm2d(filter_count)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        c_flops = calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                       out_shape=self.out_shape,\n",
        "                                       kernel_size=self.kernel_size)\n",
        "        bn_flops = calculate_norm_flops(in_shape=self.in_shape)\n",
        "\n",
        "        return c_flops + bn_flops\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(ConvBnReLUBlock.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            k_size = p_op.keywords['kernel_size']\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=k_size,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=1,\n",
        "                                              padding='same',\n",
        "                                              dilation=1)\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "            partials[idx].keywords['stride'] = 1\n",
        "            partials[idx].keywords['dilation'] = 1\n",
        "            partials[idx].keywords['padding'] = 'same'\n",
        "\n",
        "        # validate operation\n",
        "        partials = list(filter(lambda p: kernel_based_validation(**p.keywords),\n",
        "                               partials))\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class ResidualBlock(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.COMPOSITE_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count': [ 16, 32, 64 ]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count,\n",
        "                 stride=1, **kwargs):\n",
        "        super(ResidualBlock, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.stride = stride\n",
        "        self.filter_count = filter_count\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_shape[1], filter_count, kernel_size=3,\n",
        "                               stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(filter_count)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(filter_count, filter_count, kernel_size=3,\n",
        "                               stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(filter_count)\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_shape[1], filter_count, kernel_size=1, stride=stride),\n",
        "            nn.BatchNorm2d(filter_count)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        c1_flops = calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                        out_shape=self.in_shape, # padding:same\n",
        "                                        kernel_size=3)\n",
        "        bn_flops = calculate_norm_flops(in_shape=self.in_shape)\n",
        "\n",
        "        c2_flops = calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                        out_shape=self.out_shape,\n",
        "                                        kernel_size=3)\n",
        "\n",
        "\n",
        "        ds_flops = calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                        out_shape=self.out_shape,\n",
        "                                        kernel_size=1)\n",
        "\n",
        "        return c1_flops + bn_flops * 3 + c2_flops + ds_flops\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(ResidualBlock.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "            fc = p_op.keywords['filter_count']\n",
        "            out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "                                              kernel_size=3,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=1,\n",
        "                                              padding=1,\n",
        "                                              dilation=1)\n",
        "            out_shape = kernel_based_outshape(in_shape=out_shape,\n",
        "                                              kernel_size=3,\n",
        "                                              filter_count=fc,\n",
        "                                              stride=1,\n",
        "                                              padding=1,\n",
        "                                              dilation=1)\n",
        "            out_shape = (\n",
        "                out_shape[0],\n",
        "                max(out_shape[1], in_shape[1]),\n",
        "                out_shape[2],\n",
        "                out_shape[3]\n",
        "            )       # all dimensions remain the same,\n",
        "                    # the larger of the 2 channels (residual block / identity)\n",
        "                    # is used\n",
        "\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "\n",
        "        return partials\n",
        "\n",
        "\n",
        "class InceptionBlock(AbstractOperation):\n",
        "\n",
        "    OPERATION_TYPE = OperationType.COMPOSITE_OP\n",
        "    HYPERPARAMS = {\n",
        "        'filter_count_1': [ 16, 32 ],\n",
        "        'filter_count_3': [ 16, 32, 64 ],\n",
        "        'filter_count_5': [ 32, 64 ],\n",
        "        'filter_count': [ 32, 64 ]\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, filter_count_1, filter_count_3,\n",
        "                 filter_count_5, filter_count, **kwargs):\n",
        "        super(InceptionBlock, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        self.filter_count = filter_count\n",
        "        self.filter_count_1 = filter_count_1\n",
        "        self.filter_count_3 = filter_count_3\n",
        "        self.filter_count_5 = filter_count_5\n",
        "\n",
        "        self.branch1 = nn.Conv2d(in_shape[1], filter_count_1, kernel_size=1)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_shape[1], filter_count_3, kernel_size=1),\n",
        "            nn.Conv2d(filter_count_3, filter_count_3, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_shape[1], filter_count_5, kernel_size=1),\n",
        "            nn.Conv2d(filter_count_5, filter_count_5, kernel_size=5, padding=2)\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_shape[1], filter_count, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        out1 = self.branch1(x)\n",
        "        out2 = self.branch2(x)\n",
        "        out3 = self.branch3(x)\n",
        "        out4 = self.branch4(x)\n",
        "        out = torch.cat((out1, out2, out3, out4), 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        c_flops = calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                       out_shape=self.in_shape,\n",
        "                                       kernel_size=1)\n",
        "        c_flops += calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                        out_shape=self.in_shape,\n",
        "                                        kernel_size=1)\n",
        "        c_flops += calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                        out_shape=self.in_shape,\n",
        "                                        kernel_size=3)\n",
        "        c_flops += calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                        out_shape=self.in_shape,\n",
        "                                        kernel_size=1)\n",
        "        c_flops += calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                        out_shape=self.in_shape,\n",
        "                                        kernel_size=5)\n",
        "        c_flops += calculate_conv_flops(in_shape=self.in_shape,\n",
        "                                        out_shape=self.out_shape,\n",
        "                                        kernel_size=1)\n",
        "        m_flops = calculate_pool_flops(out_shape=self.out_shape,\n",
        "                                       flops_per_element=1)\n",
        "\n",
        "        return c_flops + m_flops\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over :func:`~BaseOperation.get_all_partials`\n",
        "        for faster performance (the cache uses minimal memory as these are\n",
        "        uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a list of partial functions with initialized valid \\\n",
        "            hyperparameters\n",
        "        \"\"\"\n",
        "        # deepcopy the memoized partials list\n",
        "        partials = deepcopy(InceptionBlock.get_all_partials())\n",
        "\n",
        "        for idx, p_op in enumerate(partials):\n",
        "\n",
        "            countof = lambda kw: p_op.keywords[kw]\n",
        "            out_shape = (\n",
        "                in_shape[0],\n",
        "                sum([countof('filter_count_1'),  # channel-wise concat\n",
        "                    countof('filter_count_3'),\n",
        "                    countof('filter_count_5'),\n",
        "                    countof('filter_count')]),\n",
        "                in_shape[2],\n",
        "                in_shape[3]\n",
        "            )\n",
        "            partials[idx].keywords['in_shape'] = in_shape\n",
        "            partials[idx].keywords['out_shape'] = out_shape\n",
        "\n",
        "        return partials\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$ignore-cell=True\n",
        "-->"
      ],
      "metadata": {
        "id": "Rx9kZUFkCn8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Print the diversity achieved by the hyperparameters' combinations\n",
        "all_operations = AbstractOperation.__subclasses__()\n",
        "all_partials = [op.get_partials((64, 16, 128, 128)) for op in all_operations]\n",
        "all_partials = [op for ops in all_partials for op in ops]\n",
        "\n",
        "Logger.debug((\n",
        "    f'{len(all_operations)} primitives; diversified into '\n",
        "    f'{len(all_partials)} parameterized operations'\n",
        "), line=False, caller=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "p8IYppIWCqy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRq3UqZ55ZBO"
      },
      "source": [
        "#### Role-Specific Operations\n",
        "\n",
        "<!---  \n",
        "$file=abstract_role_specific_op.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK1JszmAT4NN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c0c631-3f41-4619-9b17-286fc135f475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[26/06 09:42:35:824] \u001b[1mDEBUG\u001b[0m:\u001b[0m 22 primitives; diversified into 1029 parameterized operations\n"
          ]
        }
      ],
      "source": [
        "class AbstractRoleSpecificOperation(BaseOperation):\n",
        "    \"\"\"\n",
        "    A virtual class encapsulating Role-Specific Operations (primarily input and\n",
        "    output stems).\n",
        "\n",
        "    The class is built to enhance readability, simply to distinguish between\n",
        "    role-specific and generic operations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        super(AbstractRoleSpecificOperation, self).__init__(in_shape,\n",
        "                                                            out_shape,\n",
        "                                                            **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=role_specific_operations.py\n",
        "-->"
      ],
      "metadata": {
        "id": "uBrggy_79sRG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkpRSlDn8AvS"
      },
      "outputs": [],
      "source": [
        "\n",
        "class InputStem(AbstractRoleSpecificOperation):\n",
        "    \"\"\"\n",
        "    Sequence of operations used to process all inputs\n",
        "    \"\"\"\n",
        "\n",
        "    OPERATION_TYPE = OperationType.ROLE_SPECIFIC_OP\n",
        "    FILTER_COUNT = 64\n",
        "    KERNEL_SIZE = 3\n",
        "    STRIDE = 1\n",
        "    PADDING = (KERNEL_SIZE - 1) // 2\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_sape (:class:`torch.Tensor`): the raw data input shape\n",
        "            **kwargs (dict): Additional keyword arguments provided as a \\\n",
        "            dictionary (used to unify intialization calls in case additional \\\n",
        "            args are provided)\n",
        "        \"\"\"\n",
        "        super(InputStem, self).__init__(in_shape, out_shape, **kwargs)\n",
        "\n",
        "        # self.kernel_size = InputStem.KERNEL_SIZE\n",
        "\n",
        "        # Stem layers\n",
        "        # self.conv = nn.Conv2d(in_shape[1],\n",
        "        #                       InputStem.FILTER_COUNT,\n",
        "        #                       kernel_size=self.kernel_size,\n",
        "        #                       stride=InputStem.STRIDE,\n",
        "        #                       padding=InputStem.PADDING)\n",
        "\n",
        "        # self.bn = nn.BatchNorm2d(InputStem.FILTER_COUNT)\n",
        "\n",
        "        # self.relu = nn.ReLU()\n",
        "        # self.maxpool = nn.MaxPool2d(kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        # out = self.conv(x)\n",
        "        # out = self.bn(out)\n",
        "        # out = self.relu(out)\n",
        "        # out = self.maxpool(out)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        # c_flops = calculate_conv_flops(in_shape=self.in_shape,\n",
        "        #                             out_shape=self.out_shape,\n",
        "        #                             kernel_size=self.kernel_size)\n",
        "\n",
        "        # bn_flops = calculate_norm_flops(in_shape=self.in_shape)\n",
        "\n",
        "        return 0 # bn_flops # + c_flops\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over\n",
        "        :func:`~BaseOperation.get_all_partials` for faster performance (the\n",
        "        cache uses minimal memory as these are uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a 1-element list of partial functions with initialized valid \\\n",
        "            hyperparameters (in the form of a list to conform with primitives)\n",
        "        \"\"\"\n",
        "\n",
        "        # out_shape = kernel_based_outshape(in_shape=in_shape,\n",
        "        #                                   kernel_size=InputStem.KERNEL_SIZE,\n",
        "        #                                   filter_count=InputStem.FILTER_COUNT,\n",
        "        #                                   stride=InputStem.STRIDE,\n",
        "        #                                   padding=InputStem.PADDING,\n",
        "        #                                   dilation=1)\n",
        "\n",
        "        return [PartialWrapper(InputStem,\n",
        "                               in_shape=in_shape,\n",
        "                               out_shape=in_shape)]\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return OperationMetadata(op=type(self).__name__,\n",
        "                                 id=self.id)\n",
        "\n",
        "\n",
        "class OutputStem(AbstractRoleSpecificOperation):\n",
        "    \"\"\"\n",
        "    Sequence of operations used as an output stem for image datasets\n",
        "    \"\"\"\n",
        "\n",
        "    OPERATION_TYPE = OperationType.ROLE_SPECIFIC_OP\n",
        "    INTERMEDIATE_LINEAR_SIZE = 128\n",
        "\n",
        "    def __init__(self, in_shape, out_shape, num_classes, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes (int): the number of output classes\n",
        "            **kwargs (dict): Additional keyword arguments provided as a \\\n",
        "            dictionary (used to unify intialization calls in case additional \\\n",
        "            args are provided)\n",
        "        \"\"\"\n",
        "        super(OutputStem, self).__init__(in_shape,\n",
        "                                         out_shape,\n",
        "                                         **kwargs)\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.fc = nn.Linear(in_shape[1], OutputStem.INTERMEDIATE_LINEAR_SIZE)\n",
        "        self.classifier = nn.Linear(OutputStem.INTERMEDIATE_LINEAR_SIZE,\n",
        "                                    num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the operation's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        out = torch.mean(x, dim=(2, 3), keepdim=True)   # Global Average Pooling\n",
        "        out_cropped = out.view(out.size(0), -1)\n",
        "        out1 = self.fc(out_cropped)\n",
        "        out2 = self.classifier(out1)\n",
        "        scout = F.log_softmax(out2, dim=-1)\n",
        "\n",
        "        return scout\n",
        "\n",
        "\n",
        "    def reshape(self, in_shape=None, num_classes=None):\n",
        "        \"\"\"\n",
        "        Adapt the output stem to new class count (class-incremental learning)\n",
        "        or reshapes the entire operation if the network completely changed.\n",
        "\n",
        "        *This operation preserves weights if applicable.*\n",
        "\n",
        "        Args:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        in_shape_changed = in_shape is not None and in_shape != self.in_shape\n",
        "        num_classes_changed = num_classes is not None and \\\n",
        "        num_classes != self.num_classes\n",
        "\n",
        "        if in_shape is not None and in_shape != self.in_shape:\n",
        "            # adjust the fully connected layer (if in_shape changed)\n",
        "            old_fc_weights = self.fc.weight\n",
        "            old_fc_bias = self.fc.bias\n",
        "            new_fc = nn.Linear(in_shape[1],\n",
        "                               OutputStem.INTERMEDIATE_LINEAR_SIZE)\n",
        "            min_fs = min(in_shape[1], self.in_shape[1])\n",
        "\n",
        "            with torch.no_grad():  # not tracked in the computation graph\n",
        "                new_fc.weight[:, :min_fs].copy_(old_fc_weights[:, :min_fs])\n",
        "                new_fc.bias[:min_fs].copy_(old_fc_bias[:min_fs])\n",
        "\n",
        "            self.fc = new_fc\n",
        "            self.in_shape = in_shape\n",
        "\n",
        "        if num_classes_changed:\n",
        "            # temp store input shape\n",
        "            def_nc = num_classes or self.num_classes\n",
        "            in_size = self.classifier.in_features\n",
        "\n",
        "            classifier = nn.Linear(in_size, def_nc)\n",
        "\n",
        "            # handle both increasing and decreasing number of classes\n",
        "            n_cls = min(self.classifier.out_features, def_nc)\n",
        "\n",
        "            # move portion of pretrained weights to new classifier\n",
        "            with torch.no_grad():  # not tracked in the computation graph\n",
        "                classifier.weight[:n_cls].copy_(self.classifier.weight[:n_cls])\n",
        "                classifier.bias[:n_cls].copy_(self.classifier.bias[:n_cls])\n",
        "\n",
        "            # update the attributes\n",
        "            self.classifier = classifier\n",
        "            self.num_classes = def_nc\n",
        "            self.out_shape = (self.in_shape[0], def_nc)\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        intermediate_shape = (0, OutputStem.INTERMEDIATE_LINEAR_SIZE)\n",
        "        l1_flops = calculate_linear_flops(in_shape=self.in_shape,\n",
        "                                          out_shape=intermediate_shape)\n",
        "        l2_flops = calculate_linear_flops(in_shape=intermediate_shape,\n",
        "                                          out_shape=(0, self.num_classes))\n",
        "        return l1_flops + l2_flops\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over\n",
        "        :func:`~BaseOperation.get_all_partials` for faster performance (the\n",
        "        cache uses minimal memory as these are uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shape (:class:`torch.Tensor`) the 4-dim shape of the input \\\n",
        "            feature map (NxCxHxW)\n",
        "        Returns:\n",
        "            list: a 1-element list of partial functions with initialized valid \\\n",
        "            hyperparameters (in the form of a list to conform with primitives)\n",
        "        \"\"\"\n",
        "\n",
        "        return [PartialWrapper(OutputStem,\n",
        "                        in_shape=in_shape,\n",
        "                        out_shape=(in_shape[0], num_classes),  # for uniformity,\n",
        "                                                               # discarded in\n",
        "                                                               # init\n",
        "                        num_classes=num_classes)]\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return OperationMetadata(op=type(self).__name__,\n",
        "                                 id=self.id)\n",
        "\n",
        "\n",
        "class AlignConcat1x1(nn.Module):\n",
        "    \"\"\"\n",
        "    [DEPRECATED] - opted for a functional approach instead since this op should\n",
        "    not have any trainable params.\n",
        "\n",
        "    ACC1x1 is a custom multiplexing module used to aggregate multiple inputs.\n",
        "    Aligns the inputs' resolutions by upsampling all inputs to the largest\n",
        "    resolution (aligning through downsampling results in a loss of information),\n",
        "    followed by channel-wise concatenation and a 1x1 convolution to\n",
        "    set the given target channel dimension\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_shapes, intermediary_shape, out_shape,\n",
        "                 prepend_to, concat_channels_size=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_shapes (list): list of 4D :class:`torch.Tensor` input shapes\n",
        "            prepend_to (:class:`functools.partial`): a partial \\\n",
        "            :class:`~BaseOperation` object that succeeds the \\\n",
        "            :class:`~AlignConcat1x1`.\n",
        "            intermediary_shape (:class:`torch.Tensor`): the output shape of \\\n",
        "            the ACC module (input shape to the `prepend_to` operation)\n",
        "            out_shape (:class:`torch.Tensor`): the operation's calculated \\\n",
        "            output shape (after the `prepend_to` operation)\n",
        "            concat_channels_size (optional, int): the size of the concatenated \\\n",
        "            inputs. Must be given to initialize the conv1x1 in case it channel-\\\n",
        "            shape adjustment is needed. (align -> concat -> conv1x1 transforms \\\n",
        "            channel dimension from `concat_channels_size` to \\\n",
        "            `prepend_to.in_channels` -> `prepend_to` operation). If this \\\n",
        "            argument is not provided, the conv1x1 operation is discarded and \\\n",
        "            `prepend_to.in_channels` = the concatenated channel dimension.\n",
        "            **kwargs (dict): Additional keyword arguments provided as a \\\n",
        "            dictionary (used to unify intialization calls in case additional \\\n",
        "            args are provided)\n",
        "        \"\"\"\n",
        "\n",
        "        super(AlignConcat1x1, self).__init__()\n",
        "\n",
        "        self.in_shapes = in_shapes\n",
        "        self.intermediary_shape = intermediary_shape\n",
        "        self.out_shape = out_shape\n",
        "\n",
        "        if isinstance(prepend_to, partial):\n",
        "            self.prepend_to = prepend_to()\n",
        "        else:\n",
        "            # pre-instantiated; serialized\n",
        "            self.prepend_to = prepend_to\n",
        "\n",
        "        self.id = self.prepend_to.id\n",
        "\n",
        "        self.upsample = PartialWrapper(nn.Upsample,\n",
        "                                mode='bilinear',\n",
        "                                align_corners=False)\n",
        "        if concat_channels_size:\n",
        "            self.conv = nn.Conv2d(concat_channels_size,\n",
        "                                  self.prepend_to.in_channels,\n",
        "                                  kernel_size=1)\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the operation\n",
        "\n",
        "        Args:\n",
        "            inputs (list): the operation's input data vector of\n",
        "            :class:`torch.Tensor` objects\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        # infer spatial dimensions\n",
        "        max_height = max([in_tensor.shape[2] for in_tensor in inputs])\n",
        "        max_width = max([in_tensor.shape[3] for in_tensor in inputs])\n",
        "\n",
        "        # if not hasattr(self, 'out_channels'):\n",
        "        #     self.out_channels = max([in_tensor[1] for in_tensor in inputs])\n",
        "\n",
        "        # align resolutions\n",
        "        aligned_inputs = [self.upsample(size=(max_height,\n",
        "                                              max_width))(input_tensor) \\\n",
        "                          for input_tensor in inputs]\n",
        "        self.inputs_count = len(inputs)\n",
        "        # stack inputs w.r.t. the channels\n",
        "        out = torch.cat(aligned_inputs, dim=1)\n",
        "\n",
        "        if hasattr(self, 'conv'):\n",
        "            out = self.conv(out)\n",
        "\n",
        "        out = self.prepend_to(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    @property\n",
        "    def op_name(self):\n",
        "        \"\"\"\n",
        "        Operation name getter\n",
        "\n",
        "        Returns:\n",
        "            str: the operations name\n",
        "        \"\"\"\n",
        "\n",
        "        return f'{self.prepend_to.op_name}\\n({type(self).__name__})'\n",
        "\n",
        "    @property\n",
        "    def op_color(self):\n",
        "        \"\"\"\n",
        "        Operation color getter\n",
        "\n",
        "        Returns:\n",
        "            str: the operation's visualization color\n",
        "        \"\"\"\n",
        "\n",
        "        return f'{self.prepend_to.op_color}'\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the FLOPs used for this operation\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the operation's FLOPs\n",
        "        \"\"\"\n",
        "\n",
        "        # upsampling/interpolation is estimated through the number of inputs\n",
        "        num_of_interpolation = 2\n",
        "        if hasattr(self, 'inputs_count'):\n",
        "            num_of_interpolation = self.inputs_count\n",
        "\n",
        "        # as per torchprofile's implementation for bilinear interpolation\n",
        "        # the flops_per_element is ~= 4\n",
        "        # upsampling flops:\n",
        "        ups_flops = calculate_pool_flops(out_shape=self.intermediary_shape,\n",
        "                                         flops_per_element=4)\n",
        "\n",
        "        return self.prepend_to.flops + ups_flops * num_of_interpolation\n",
        "\n",
        "    @staticmethod\n",
        "    def get_partials(in_shapes):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over\n",
        "        :func:`~BaseOperation.get_all_partials` for faster performance (the\n",
        "        cache uses minimal memory as these are uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shapes (list): list of :class:`torch.Tensor` objects, each \\\n",
        "            corressponding to a 4-dim shape of an input feature map (NxCxHxW)\n",
        "\n",
        "        Returns:\n",
        "            list: a 1-element list of partial functions with initialized valid \\\n",
        "            hyperparameters (in the form of a list to conform with primitives)\n",
        "        \"\"\"\n",
        "\n",
        "        intermediary_shape = (\n",
        "            max([o[0] for o in in_shapes]),\n",
        "            sum([o[1] for o in in_shapes]),\n",
        "            max([o[2] for o in in_shapes]),\n",
        "            max([o[3] for o in in_shapes])\n",
        "        )\n",
        "\n",
        "        return [PartialWrapper(AlignConcat1x1, in_shapes=in_shapes,\n",
        "                        intermediary_shape=intermediary_shape)]\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def deserialize(hyperparams):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        prepend_to = json.loads(hyperparams['prepend_to'])\n",
        "        prepend_cls = globals()[prepend_to['op']]\n",
        "        prepend_hps = prepend_to['hyperparams']\n",
        "\n",
        "        del hyperparams['prepend_to']\n",
        "\n",
        "        return AlignConcat1x1(prepend_to=prepend_cls(**prepend_hps),\n",
        "                              **hyperparams)\n",
        "\n",
        "\n",
        "\n",
        "    def serialize(self):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        return json.dumps(self.metadata.params)\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return OperationMetadata(op=type(self).__name__,\n",
        "                                 id=self.id,\n",
        "                                 in_shape=self.in_shape,\n",
        "                                 out_shape=self.out_shape)\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.metadata)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __hash__(self):\n",
        "        \"\"\"\n",
        "        Hashing the metadata of the operation as that is enforced to be\n",
        "        unique.\n",
        "        \"\"\"\n",
        "\n",
        "        return hash(str(self))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, self.__class__):\n",
        "            return False\n",
        "\n",
        "        return self.metadata == other.metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDc6g8wXvFHn"
      },
      "source": [
        "#### Functional Operations\n",
        "\n",
        "<!---  \n",
        "$file=functional_operations.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtzmuCl6vEhO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AlignConcat:\n",
        "    \"\"\"\n",
        "    AlignConcat is a custom multiplexing functional module used to aggregate\n",
        "    multiple inputs. Aligns the inputs' resolutions by upsampling all inputs to\n",
        "    the largest resolution (aligning through downsampling results in a loss of\n",
        "    information).\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def functional(inputs: List[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        Functional forward pass\n",
        "\n",
        "        Args:\n",
        "            inputs (list): the operation's input data vector of \\\n",
        "            :class:`torch.Tensor` objects\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        # infer spatial dimensions\n",
        "        max_height = max([in_tensor.shape[2] for in_tensor in inputs])\n",
        "        max_width = max([in_tensor.shape[3] for in_tensor in inputs])\n",
        "\n",
        "        # align resolutions (upsample deprecated, using interpolate instead)\n",
        "        # aligned_inputs = [F.upsample(input=input_tensor, size=(max_height,\n",
        "        #                                    max_width)) \\\n",
        "        #                   for input_tensor in inputs]\n",
        "\n",
        "        # align resolutions; spatial dimensions are altered, channel dimension\n",
        "        # is unaffected\n",
        "        # aligned_inputs = [F.interpolate(in_tensor,\n",
        "        #                                 size=(max_height, max_width),\n",
        "        #                                 mode='bilinear',\n",
        "        #                                 align_corners=False) \\\n",
        "        #                   for in_tensor in inputs]\n",
        "\n",
        "        aligned_inputs = []\n",
        "        for tensor in inputs:\n",
        "            if tensor.shape[2] != max_height or tensor.shape[3] != max_width:\n",
        "                upsampled_tensor = F.interpolate(tensor,\n",
        "                                                 size=(max_height, max_width),\n",
        "                                                 mode='bilinear',\n",
        "                                                 align_corners=False)\n",
        "                aligned_inputs.append(upsampled_tensor)\n",
        "            else:\n",
        "                aligned_inputs.append(tensor)\n",
        "\n",
        "\n",
        "        # stack inputs w.r.t. the channels\n",
        "        out = torch.cat(aligned_inputs, dim=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_shape(in_shapes):\n",
        "        \"\"\"\n",
        "        Gets partial functions with all valid hyperparameter combinations\n",
        "        given the input shape.\n",
        "\n",
        "        Uses lazy initialization over\n",
        "        :func:`~BaseOperation.get_all_partials` for faster performance\n",
        "        (the cache uses minimal memory as these are uninitialized partials)\n",
        "\n",
        "        Args:\n",
        "            in_shapes (list): list of :class:`torch.Tensor` objects, each \\\n",
        "            corressponding to a 4-dim shape of an input feature map (NxCxHxW)\n",
        "\n",
        "        Returns:\n",
        "            list: a 1-element list of partial functions with initialized valid \\\n",
        "            hyperparameters (in the form of a list to conform with primitives)\n",
        "        \"\"\"\n",
        "\n",
        "        intermediary_shape = (\n",
        "            max([o[0] for o in in_shapes]),\n",
        "            sum([o[1] for o in in_shapes]),\n",
        "            max([o[2] for o in in_shapes]),\n",
        "            max([o[3] for o in in_shapes])\n",
        "        )\n",
        "\n",
        "        return intermediary_shape\n",
        "\n",
        "\n",
        "class ShapeMatchTransform:\n",
        "    \"\"\"\n",
        "    This Transform functional module takes an aggregated input tensor and\n",
        "    transforms its shape to another module's expected shape.\n",
        "\n",
        "    It is primarily applied to upon network extensions as it is difficult to\n",
        "    constrain new paths with overlapping nodes to a specific shape.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def functional(input: torch.Tensor, shape):\n",
        "        \"\"\"\n",
        "        Functional forward pass\n",
        "\n",
        "        Args:\n",
        "            input (:class:`torch.Tensor`): the operation's input data\n",
        "            shape (:class:`np.ndarray`): the expected input shape\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the operation's output data\n",
        "        \"\"\"\n",
        "\n",
        "        # unpack\n",
        "        _, c, h, w = shape\n",
        "        x = input\n",
        "\n",
        "        # use adaptive_avg_pool2d for downsampling & interpolate for upsampling\n",
        "        if x.size(2) != h or x.size(3) != w:\n",
        "            x = F.interpolate(x, size=(h, w),\n",
        "                              mode='bilinear', align_corners=False)\n",
        "\n",
        "        # adapt channel dimension using a (somewhat) functional transform\n",
        "        if x.size(1) != c:\n",
        "            channel_transform = nn.Conv2d(in_channels=x.size(1),\n",
        "                                          out_channels=c,\n",
        "                                          kernel_size=(1, 1))\n",
        "\n",
        "            # move the channel_transform parameters to the same device as x\n",
        "            channel_transform.to(x.device)\n",
        "\n",
        "            x = channel_transform(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgpAmECE5hVW"
      },
      "source": [
        "### Model\n",
        "\n",
        "<!---  \n",
        "$module=model\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_VfUSOvYH5Z"
      },
      "source": [
        "#### Network\n",
        "\n",
        "<!---  \n",
        "$file=network.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWKFSRvPi4PY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Network(nn.Module, nx.DiGraph):\n",
        "    \"\"\"\n",
        "    The DAG representation of a neural architecture (or a motif/cell\n",
        "    subgraph).\n",
        "    \"\"\"\n",
        "\n",
        "    __SERIAL_ID = 0\n",
        "\n",
        "    def __init__(self, adj_matrix=None, operations=None, task_metadata=None):\n",
        "        # explicit superclass init to ensure expected MRO\n",
        "        nx.DiGraph.__init__(self)\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.id = Network.__SERIAL_ID\n",
        "        self.version = 0        # gets incremented with each graph update\n",
        "        Network.__SERIAL_ID += 1\n",
        "\n",
        "        self.task_map = {}      # task_id : model heuristics\n",
        "                                #           (adj_matrix, ops, predecessors)\n",
        "                                # we memoize preds to off-load the recursive\n",
        "                                # training complexity\n",
        "\n",
        "        self.tasks_metadata = []\n",
        "        self.metrics = ModelMetrics()\n",
        "\n",
        "        if adj_matrix and operations and task_metadata:\n",
        "            # compile on init\n",
        "            self.compile(adj_matrix, operations, task_metadata)\n",
        "\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    # GRAPH COMPILATION\n",
        "\n",
        "    def __update_graph(self, adj_matrix, ops):\n",
        "        \"\"\"\n",
        "        Initialize or alter the Network from the given graph (adjacency matrix\n",
        "        and operations).\n",
        "\n",
        "        This method 1) adds nodes from the given `ops` list if they do not\n",
        "        already exist in the supergraph (must match `op.id` to qualify).\n",
        "        2) Similarly, we add edges from the given `adj_matrix` if they do not\n",
        "        exist. 3) Remove isolates in case edge-removal resulted in layer\n",
        "        isolation. 4) We compute the predecessors for this particular subgraph\n",
        "        (the topology may only be altered in this function, thus the\n",
        "        predecessors cannot change otherwise).\n",
        "\n",
        "        Args:\n",
        "            adj_matrix (:class:`np.ndarray`): adjacency matrix-encoded DAG\n",
        "            ops (:obj:`list`): list of compiled ops corresponding the \\\n",
        "            adj_matrix topology\n",
        "        \"\"\"\n",
        "\n",
        "        # # add nodes to graph with additional node attributes (index/depth)\n",
        "        # depths = get_nodes_depths(adj_matrix)\n",
        "        # new_nodes_idx = len(self.operations) - len(added_nodes)\n",
        "        # nodes = [(op, {'index': idx, 'depth': depths[idx]}) \\\n",
        "        #          for idx, op in enumerate(self.operations) \\\n",
        "        #          if idx >= new_nodes_idx and op is not None]\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Filter new nodes to add to the graph\n",
        "        nodes_to_add = []\n",
        "\n",
        "        for node in ops:\n",
        "            node_exists = False\n",
        "            for existing_node in self.nodes():\n",
        "                if node.id == existing_node.id:\n",
        "                    # node already exists, do not add\n",
        "                    node_exists = True\n",
        "                    break\n",
        "\n",
        "            if not node_exists:\n",
        "                # new node, add it\n",
        "                nodes_to_add.append(node)\n",
        "\n",
        "        self.add_nodes_from([(node, {'signature': node.metadata.signature}) \\\n",
        "                             for node in nodes_to_add])\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Modify graph edges\n",
        "\n",
        "        subgraph_nodes = []\n",
        "        preds = defaultdict(list)\n",
        "        for i in range(len(adj_matrix)):\n",
        "            op_i = self.get_node_by_id(ops[i].id)\n",
        "            subgraph_nodes.append(op_i)\n",
        "            for j in range(len(adj_matrix)):\n",
        "                op_j = self.get_node_by_id(ops[j].id)\n",
        "\n",
        "                if adj_matrix[i][j] == 1:\n",
        "                    preds[op_j.id].append(op_i)\n",
        "                    # edge should exist (add if not exists)\n",
        "                    if not self.has_edge(op_i, op_j):\n",
        "                        self.add_edge(op_i, op_j)\n",
        "\n",
        "                else:\n",
        "                    # edge should not exist (remove if exists)\n",
        "                    if self.has_edge(op_i, op_j):\n",
        "                        self.remove_edge(op_i, op_j)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Check for isolates (0 degree nodes) resulting from edge removal,\n",
        "        # remove from graph accordingly\n",
        "        self.remove_nodes_from(list(nx.isolates(self)))\n",
        "\n",
        "        return (adj_matrix, subgraph_nodes, preds)\n",
        "\n",
        "    # $DEPRECATED\n",
        "    # def __out_channels_dfs(self, op_idx, input_conns):\n",
        "    #     \"\"\"\n",
        "    #     Recursively traverse the graph from `op_idx` to the root node, \\\n",
        "    #     populating the `out_channels` argument for the operations\n",
        "\n",
        "    #     Args:\n",
        "    #         op_idx (int): the operation's index to traverse back to the \\\n",
        "    #         input from\n",
        "    #         input_conns (list): 2D array of the operations' input connections\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     if 'out_channels' in self.operations[op_idx].keywords:\n",
        "    #         return self.operations[op_idx].keywords['out_channels']\n",
        "\n",
        "    #     tot_channels = 0\n",
        "    #     for in_conn in inputrecon_conns[op_idx]:\n",
        "    #         channels = self.__out_channels_dfs(in_conn, input_conns)\n",
        "    #         if 'out_channels' not in self.operations[in_conn].keywords:\n",
        "    #             self.operations[in_conn].keywords['out_channels'] = channels\n",
        "    #         tot_channels += channels\n",
        "\n",
        "    #     return tot_channels\n",
        "\n",
        "\n",
        "    # def __out_shape_dfs(self, op_idx):\n",
        "    #     \"\"\"\n",
        "    #     Recursively traverse the graph from `op_idx` to the root node, \\\n",
        "    #     calculating the output shape of each operation\n",
        "\n",
        "    #     Args:\n",
        "    #         op_idx (int): the operation's index to traverse back to the \\\n",
        "    #         input from\n",
        "    #     \"\"\"\n",
        "    #     assert self.output_shapes[0] is not None, (\n",
        "    #           'Base case missing for the out_shape DFS'\n",
        "    #     )\n",
        "    #     if self.output_shapes[op_idx] is not None:\n",
        "    #         # memoization\n",
        "    #         return self.output_shapes[op_idx]\n",
        "\n",
        "    #     if len(self.input_conns[op_idx]) == 1:\n",
        "    #         prev_out = self.__out_shape_dfs(self.input_conns[op_idx][0])\n",
        "    #         self.output_shapes[op_idx] = self.compiled_ops[op_idx]\\\n",
        "    #                                          .calculate_feature_map(prev_out)\n",
        "    #         return self.output_shapes[op_idx]\n",
        "    #     else:\n",
        "    #         max_in_shape = (0, 0)\n",
        "    #         # multi-input op\n",
        "    #         for in_conn in self.input_conns[op_idx]:\n",
        "    #             prev_out = self.__out_shape_dfs(in_conn)\n",
        "    #             max_in_shape = (max(max_in_shape[0], prev_out[0]),\n",
        "    #                                 max(max_in_shape[1], prev_out[1]))\n",
        "\n",
        "    #         self.output_shapes[op_idx] = max_in_shape\n",
        "    #         return self.output_shapes[op_idx]\n",
        "\n",
        "\n",
        "    def compile(self, adj_matrix, operations, task_metadata):\n",
        "        \"\"\"\n",
        "        Compile the given (sub-)graph (list of uninstantiated operations and\n",
        "        adjacency matrix) and add the edges of the :class:`nx.DiGraph`\n",
        "        accordingly.\n",
        "\n",
        "        Args:\n",
        "            adj_matrix (:class:`numpy.ndarray`): adjacency matrix-encoded DAG\n",
        "            operations (:obj:`list`): ordered list of (uninstantiated)\n",
        "            operations corresponding the edges of the adjacency matrix, each \\\n",
        "            in the form of a partial function (:class:`~PartialWrapper`)\n",
        "            task_metadata (:class:`~TaskMetadata`): the task data associated \\\n",
        "            with the given model; used to keep track of tasks and their\n",
        "            sub-graphs within extended models\n",
        "        \"\"\"\n",
        "\n",
        "        # list of compiled operations for this sub-graph\n",
        "        compiled_ops = []\n",
        "\n",
        "        for op in operations:\n",
        "            if isinstance(op, PartialWrapper):\n",
        "                # compile the operation temporarily to equate with existing\n",
        "                # operations in the super-graph.\n",
        "\n",
        "                # By design, if the op shares the same unique ID with an\n",
        "                # existing node, we are intended to reuse it.\n",
        "                temp_compiled = op()\n",
        "                op_is_precompiled = False\n",
        "\n",
        "                for precompiled_op in self.nodes():\n",
        "                    if precompiled_op.id == temp_compiled.id:\n",
        "                        if isinstance(precompiled_op, OutputStem):\n",
        "                            # reshape and preserve weights if applicable\n",
        "                            in_shape = temp_compiled.in_shape\n",
        "                            num_classes = temp_compiled.num_classes\n",
        "                            precompiled_op.reshape(in_shape=in_shape,\n",
        "                                                   num_classes=num_classes)\n",
        "\n",
        "                        # operation already compiled, do not recompile\n",
        "                        compiled_ops.append(precompiled_op)\n",
        "                        op_is_precompiled = True\n",
        "                        break\n",
        "                if not op_is_precompiled:\n",
        "                    compiled_ops.append(temp_compiled)\n",
        "            else:\n",
        "                # [DEPRECATED]\n",
        "                # ~~# isolate node, add `None` to preserve index order\n",
        "                # self.operations.append(None)~~\n",
        "\n",
        "                # [revised search space requirements]: `Network` does not accept\n",
        "                # isolate/`None` nodes. Only `PartialWrapper`s of\n",
        "                # `BaseOperation`s are accepted.\n",
        "\n",
        "                raise ValueError(\n",
        "                    f'An invalid operation of type `{type(op)}` was given. '\n",
        "                    'Only `PartialWrapper`s of `BaseOperation`s are '\n",
        "                    'accepted'\n",
        "                )\n",
        "\n",
        "        # if label_nodes:\n",
        "        #     label_map = {idx: f'{idx}:{op.op_name}' \\\n",
        "        #     for idx, op in enumerate(self.compiled_ops) if op is not None}\n",
        "        #     nx.relabel_nodes(self, label_map, copy=False)\n",
        "\n",
        "        # prev_subgraph = None\n",
        "        # if task_metadata.id in self.task_map:\n",
        "        #     prev_subgraph = self.get_subgraph(task_metadata.id)\n",
        "\n",
        "        subgraph = self.__update_graph(adj_matrix, compiled_ops)\n",
        "\n",
        "        self.task_map[task_metadata.id] = subgraph\n",
        "        self.tasks_metadata.append(task_metadata)\n",
        "\n",
        "        self.metrics.set_model_metadata(self.metadata)\n",
        "        self.version += 1\n",
        "\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    # CONTINUAL LEARNING METHODS\n",
        "\n",
        "    def adapt_output(self, task_id, new_class_count):\n",
        "        \"\"\"\n",
        "        Adapts the model for a new output space.\n",
        "        The `task_id` is used to identify the output layer to be modified.\n",
        "\n",
        "        Note: This method does not fine-tune or retrain the model, just merely\n",
        "        resizes the output layer for the given task's (sub-)graph *whilst*\n",
        "        masking or preserving the existing trained weights in said layer.\n",
        "\n",
        "        Args:\n",
        "            task_id (:obj:`int`): the task ID associated with the given model; \\\n",
        "            used to keep track of tasks and their sub-graphs within extended\n",
        "            models\n",
        "            new_class_count (:obj:`int`): the new number of output features \\\n",
        "            for the given `task_id`\n",
        "        \"\"\"\n",
        "\n",
        "        out_shape_pre = self.get_output_layer(task_id).out_shape\n",
        "        self.get_output_layer(task_id).reshape(num_classes=new_class_count)\n",
        "        out_shape_post = self.get_output_layer(task_id).out_shape\n",
        "\n",
        "        Logger.critical(f'Model ID ({self.id}) adapted its output ' + \\\n",
        "                    f'from {out_shape_pre} to {out_shape_post}...')\n",
        "\n",
        "    # [DEPRECATED]\n",
        "    # def extend_network(self, task_id, adj_matrix, operations):\n",
        "    #     \"\"\"\n",
        "    #     Compile the model given the list of uninstantiated operations and the\n",
        "    #     adjacency matrix. This function instantiates the operations and adds\n",
        "    #     them on the edges of the :class:`nx.DiGraph`\n",
        "    #     Args:\n",
        "    #         task_id (:obj:`int`): the task ID associated with the given model\\\n",
        "    #         ; used to keep track of tasks and their sub-graphs within extended\n",
        "    #         models\n",
        "    #         adj_matrix (:class:`numpy.ndarray`): adjacency matrix-encoded DAG\n",
        "    #         operations (:obj:`list`): ordered list of (uninstantiated)\n",
        "    #         operations corresponding the edges of the adjacency matrix, each \\\n",
        "    #         in the form of a partial function\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     if not hasattr(self, 'operations'):\n",
        "    #         # no existing graph, compile\n",
        "    #         return self.compile_network(adj_matrix, operations, task_id)\n",
        "\n",
        "    #     # extend model\n",
        "    #     self.metrics = ModelMetrics(self.metadata)\n",
        "\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    # TORCH METHODS\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the network graph.\\\n",
        "        This process is performed using DFS since the connections' order is \\\n",
        "        unknown a priori\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the network's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the network's output data\n",
        "        \"\"\"\n",
        "\n",
        "        assert hasattr(self, 'active_task') \\\n",
        "        and self.active_task is not None, (\n",
        "            'A task needs to be activated prior to model training'\n",
        "        )\n",
        "\n",
        "        return self.__recursive_forward(x)\n",
        "\n",
        "    def __recursive_forward(self, x):\n",
        "        \"\"\"\n",
        "        Recursively executes a forward pass for the active task's subgraph.\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): the network's input data\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.Tensor`: the network's output data\n",
        "        \"\"\"\n",
        "\n",
        "        # intermediate tensor memoization\n",
        "        outputs = {}\n",
        "\n",
        "        # get computed predecessors for the active task\n",
        "        _, _, all_preds = self.task_map[self.active_task]\n",
        "\n",
        "        def dfs_outputs(op):\n",
        "            # memoization check; already processed\n",
        "            if op.id in outputs:\n",
        "                return outputs[op.id]\n",
        "\n",
        "            preds = all_preds[op.id]\n",
        "\n",
        "            # root base case (input node / no predecessors)\n",
        "            if len(preds) == 0:\n",
        "                outputs[op.id] = op(x)\n",
        "                return outputs[op.id]\n",
        "\n",
        "            # single input operations\n",
        "            if len(preds) == 1:\n",
        "                outputs[op.id] = op(dfs_outputs(preds[0]))\n",
        "                return outputs[op.id]\n",
        "\n",
        "            # multi-input operations\n",
        "            input_tensors = []\n",
        "            for pred_op in preds:\n",
        "                if pred_op.id not in outputs:\n",
        "                    outputs[pred_op.id] = dfs_outputs(pred_op)\n",
        "\n",
        "                input_tensors.append(outputs[pred_op.id])\n",
        "\n",
        "            aggregate = AlignConcat.functional(input_tensors)\n",
        "\n",
        "            # TODO: I don't like this shape transform approach.\n",
        "            # This is currently only necessary for CL methods.\n",
        "            # Constrain the topology-extension properly to ensure\n",
        "            # shape-match and remove this. A big part of this framework\n",
        "            # is to get rid of paddings and up-/down-sampling to reduce\n",
        "            # overhead.\n",
        "            # I hate this.\n",
        "            aggregate = ShapeMatchTransform.functional(input=aggregate,\n",
        "                                                       shape=op.in_shape)\n",
        "            outputs[op.id] = op(aggregate)\n",
        "            return outputs[op.id]\n",
        "\n",
        "        # DFS starting at the output node\n",
        "        out = dfs_outputs(self.get_output_layer(task_id=self.active_task))\n",
        "\n",
        "        # clear intermediate feature maps\n",
        "        del outputs\n",
        "\n",
        "        return out\n",
        "\n",
        "    def parameters(self, recurse=True):\n",
        "        \"\"\"\n",
        "        Override :func:`nn.Module.parameters()` function.\n",
        "\n",
        "        PyTorch is normally expected to walk all members of this class\n",
        "        recursively, and accumulate parameters from:\n",
        "            1- anything manually registered with PyTorch's \\\n",
        "            :func:`register_parameter()`\n",
        "            2- any member sub-classing :class:`nn.Module` \\\n",
        "            3- any element in a `nn.ModuleList` (essentially same as rule 2)\n",
        "\n",
        "        Since we are maintaing the modules as the nodes of :class:`nx.DiGraph`,\n",
        "        we could either maintain a parallel list of modules to match the\n",
        "        graph's nodes, or override this method to yield the parameters in the\n",
        "        network dynamically. I thought the former is too cumbersome and\n",
        "        vulnerable to mistakes, so we dynamically yield the parameters from our\n",
        "        graph's nodes instead.\n",
        "\n",
        "        Args:\n",
        "            recurse (:obj:`bool`): whether we should recursively walk the \\\n",
        "            `nn.Module`\n",
        "\n",
        "        Returns:\n",
        "            :class:`nn.Parameter`: the yielded parameter (in the form of a \\\n",
        "            generator)\n",
        "        \"\"\"\n",
        "\n",
        "        for node in self.nodes():\n",
        "            if isinstance(node, nn.Module):\n",
        "                for param in node.parameters(recurse=recurse):\n",
        "                    yield param\n",
        "\n",
        "    def cuda(self):\n",
        "        \"\"\"\n",
        "        Override :func:`nn.Module.cuda()` function.\n",
        "\n",
        "        Similar to the :func:`nn.Module.parameters()` function, the lack of\n",
        "        module registration (into a class member :class:`nn.Sequential` or\n",
        "        :class:`nn.ModuleList`) raises an issue when setting `cuda` or `cpu`\n",
        "        settings.\n",
        "\n",
        "        We simply traverse our DAG and apply the setting manually.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for node in self.nodes():\n",
        "            if isinstance(node, nn.Module):\n",
        "                node.cuda()\n",
        "\n",
        "    def cpu(self):\n",
        "        \"\"\"\n",
        "        Override :func:`nn.Module.cpu()` function.\n",
        "\n",
        "        Similar to the :func:`nn.Module.parameters()` function, the lack of\n",
        "        module registration (into a class member :class:`nn.Sequential` or\n",
        "        :class:`nn.ModuleList`) raises an issue when setting `cuda` or `cpu`\n",
        "        settings.\n",
        "\n",
        "        We simply traverse our DAG and apply the setting manually.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for node in self.nodes():\n",
        "            if isinstance(node, nn.Module):\n",
        "                node.cpu()\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    # PROPERTIES\n",
        "\n",
        "    @property\n",
        "    def adj_matrix(self):\n",
        "        \"\"\"\n",
        "        Gets the network's adjacency matrix (including all sub-graphs).\n",
        "\n",
        "        Returns:\n",
        "            :class:`numpy.ndarray`: adjacency matrix representation of the \\\n",
        "            :class:`~Network`'s DAG\n",
        "        \"\"\"\n",
        "        if len(self.nodes) > 0:\n",
        "            return nx.adjacency_matrix(self).toarray()\n",
        "        else:\n",
        "            return np.ndarray([])\n",
        "\n",
        "    @property\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculates the total MFLOPs of all the operations in the graph.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`float`: the model's MFLOPs per forward pass\n",
        "        \"\"\"\n",
        "\n",
        "        return sum([op.flops for op in self.nodes()])\n",
        "\n",
        "    @property\n",
        "    def total_params(self):\n",
        "        \"\"\"\n",
        "        Calculates the total number of parameters in the model\n",
        "\n",
        "        Returns:\n",
        "            :obj:`int`: the total count of parameters in the model\n",
        "        \"\"\"\n",
        "\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    @property\n",
        "    def learnable_params(self):\n",
        "        \"\"\"\n",
        "        Calculates the total number of learnable parameters in the model\n",
        "\n",
        "        Returns:\n",
        "            :obj:`int`: the total count of learnable parameters in the model\n",
        "        \"\"\"\n",
        "\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    @property\n",
        "    def wl_hash(self):\n",
        "        \"\"\"\n",
        "        Get Weisfeiler-Lehman Hash representing the graph.\n",
        "        This is used to avoid duplicitous evaluations of isomorphic graphs,\n",
        "        as well as to structure the results.\n",
        "\n",
        "        According to NetworkX docs:\n",
        "\n",
        "            If no node or edge attributes are provided, the degree of each node\n",
        "            is used as its initial label. Otherwise, node and/or edge labels\n",
        "            are used to compute the hash.\n",
        "\n",
        "        We specify the degree of the nodes based on each operation's\n",
        "        structural (non-unique) signature. In other words, two\n",
        "        :class:`~BatchNormalization` operations should be considered identical\n",
        "        nodes (same `signature` value), but their hashes will be different due\n",
        "        to their unique `id` attributes.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`str`: the graph's WL hash\n",
        "        \"\"\"\n",
        "\n",
        "        return nx.weisfeiler_lehman_graph_hash(self, node_attr='signature')\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        \"\"\"\n",
        "        Encapsulates and returns the object's metadata for logging, hashing,\n",
        "        and equating purposes\n",
        "\n",
        "        Returns:\n",
        "            :class:`~ModelMetadata`: the object's metadata\n",
        "        \"\"\"\n",
        "        nodes = [str(op).strip() for op in self.nodes(data=True)]\n",
        "        adj_matrix = self.adj_matrix.tolist()\n",
        "\n",
        "        task_map = {}\n",
        "        for k, subgraph in self.task_map.items():\n",
        "            preds = {key: [o.id for o in ol] \\\n",
        "                     for key, ol in self.task_map[k][2].items()}\n",
        "            task_map[k] = (subgraph[0].tolist(),\n",
        "                           [o.id for o in subgraph[1]],\n",
        "                           preds)\n",
        "\n",
        "        return ModelMetadata(id=self.id,\n",
        "                             version=self.version,\n",
        "                             wl_hash=self.wl_hash,\n",
        "                             model_hash=hash(self),\n",
        "                             serialized_graph=self.serialize(),\n",
        "                             mflops=self.flops / 1e6,\n",
        "                             total_params=self.total_params,\n",
        "                             learnable_params=self.learnable_params,\n",
        "                             adj_matrix=adj_matrix,\n",
        "                             nodes=nodes,\n",
        "                             task_map=str(task_map),\n",
        "                             tasks_metadata=self.tasks_metadata)\n",
        "\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    # SERIALIZATION METHODS\n",
        "\n",
        "    @staticmethod\n",
        "    def deserialize(graph_str):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        graph_dict = json.loads(graph_str)\n",
        "\n",
        "        adj_matrix = np.array(graph_dict['adj_matrix'])\n",
        "        ops = []\n",
        "        for operation in graph_dict['ops']:\n",
        "            op = json.loads(operation)\n",
        "\n",
        "            # although the use of globals() is generally evil,\n",
        "            # there are no caveats in this particular use-case\n",
        "            # (especially since reproducibility is unlikely to be used in\n",
        "            # autonomous/deployed scenarios)\n",
        "\n",
        "            # nevertheless, TODO: explore other solutions for raw serialization\n",
        "            # without globals()\n",
        "            op_cls = globals()[op['op']]\n",
        "            if op_cls == Network:\n",
        "                # recursive deserialization to support hierarchical structures\n",
        "                ops.append(Network.deserialize(operation))\n",
        "            else:\n",
        "                # primitive op\n",
        "                ops.append(PartialWrapper(op_cls, **op['args']))\n",
        "\n",
        "        return PartialWrapper(Network, adj_matrix, ops)\n",
        "\n",
        "    def serialize(self):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        nodes = [str(op).strip() for op in self.nodes(data=True)]\n",
        "        adj_matrix = self.adj_matrix.tolist()\n",
        "\n",
        "        task_map = {}\n",
        "        for k, subgraph in self.task_map.items():\n",
        "            preds = {key: [o.id for o in ol] \\\n",
        "                     for key, ol in self.task_map[k][2].items()}\n",
        "            task_map[k] = (subgraph[0].tolist(),\n",
        "                           [o.id for o in subgraph[1]],\n",
        "                           preds)\n",
        "\n",
        "        params = {\n",
        "            'mflops': self.flops / 1e6,\n",
        "            'adj_matrix': adj_matrix,\n",
        "            'nodes': nodes,\n",
        "            'task_map':task_map,\n",
        "            'tasks_metadata': [md.params for md in self.tasks_metadata]\n",
        "        }\n",
        "\n",
        "        return json.dumps(params, default=lambda obj: obj.params \\\n",
        "                          if isinstance(obj, Metadata) else str(obj))\n",
        "\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    # UTILITIES & CUSTOM GETTERS\n",
        "\n",
        "    def activate_task(self, task_id):\n",
        "        \"\"\"\n",
        "        Set the active task for training\n",
        "\n",
        "        Args:\n",
        "            task_id (:obj:`Any`): the task's ID. This task must be compiled \\\n",
        "            prior to activation\n",
        "        \"\"\"\n",
        "        assert task_id in self.task_map, (\n",
        "            'Invalid task ID provided. Please ensure this model was compiled '\n",
        "            f'for task `{task_id}` prior to activating the task sub-graph'\n",
        "        )\n",
        "\n",
        "        self.active_task = task_id\n",
        "\n",
        "    # def get_adj_matrix(self):\n",
        "    #     \"\"\"[Deprecated], using nx built-in method instead\n",
        "    #     Returns the adjacency matrix representation of this graph \\\n",
        "    #     (:class:`nx.DiGraph`). This is not stored as a local variable as the \\\n",
        "    #     graph could be amended during compilation and so any changes to the \\\n",
        "    #     graph's edges need to be also be applied to the adjacency matrix. To \\\n",
        "    #     reduce the maintainability hassle, this is calculated on the fly.\n",
        "\n",
        "    #     TODO: This is clearly inefficient. Ideally we would have an \\\n",
        "    #     intermediate class to sync the edges with the adjacency matrix so we \\\n",
        "    #     wouldn't have to \"calculate\" the adjacency matrix every time from \\\n",
        "    #     the edges whilst being readable and maintainable\n",
        "\n",
        "    #     Returns:\n",
        "    #         :class:`np.ndarray`: the graph's adjacency matrix representation\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     # ordered list of nodes as they appear in the DAG\n",
        "    #     nodes = list(self.nodes())\n",
        "\n",
        "    #     mat = np.zeros((len(nodes), len(nodes)), dtype=int)\n",
        "\n",
        "    #     # extract connections from edges\n",
        "    #     for edge in self.edges():\n",
        "    #         src, dest = edge\n",
        "    #         src_idx = nodes.index(src)\n",
        "    #         dest_idx = nodes.index(dest)\n",
        "    #         mat[src_idx, dest_idx] = 1  # diagonal\n",
        "\n",
        "    #     return mat\n",
        "\n",
        "    def get_subgraph(self, task_id):\n",
        "        \"\"\"\n",
        "        Extracts the portion of the Network corresponding to the given task ID.\n",
        "\n",
        "        Args:\n",
        "            task_id (:obj:`int`): the target task ID\n",
        "\n",
        "        Returns:\n",
        "            :rtype: (:class:`np.ndarray`, :obj:`list`): the subgraph \\\n",
        "            pertaining to the given task, in the form of (`adjacency_matrix`, \\\n",
        "            `operations_list`)\n",
        "        \"\"\"\n",
        "        assert task_id in self.task_map, (\n",
        "            f'The provided task ID ({task_id}) does not have a corresponding '\n",
        "            'subgraph in this Network'\n",
        "        )\n",
        "\n",
        "        return self.task_map[task_id]\n",
        "\n",
        "    def get_torchscript(self, in_shape):\n",
        "        \"\"\"\n",
        "        Traces the model using a random input (conforming to the same shape as\n",
        "        `InputStem`) and returns a traced model object.\n",
        "\n",
        "        Warning: slightly and memory– and computionally-expensive. Not\n",
        "        recommended to use within the NAS loop\n",
        "\n",
        "        Returns:\n",
        "            :class:`torch.jit.RecursiveScriptModule`: the traced torchscript \\\n",
        "            model\n",
        "        \"\"\"\n",
        "\n",
        "        return torch.jit.trace(self, in_shape)\n",
        "                               # torch.rand(list(self.nodes()[0]).in_shape))\n",
        "\n",
        "    def get_node_by_id(self, node_id):\n",
        "        \"\"\"\n",
        "        Gets a node in the graph by its ID\n",
        "\n",
        "        Args:\n",
        "            node_id (:obj:`int`): the target node ID\n",
        "\n",
        "        Returns:\n",
        "            :class:`~BaseOperation`: the resulting node if it exists, \\\n",
        "            returns `None` otherwise\n",
        "        \"\"\"\n",
        "\n",
        "        for node in self.nodes():\n",
        "            if node.id == node_id:\n",
        "                return node\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_output_layer(self, task_id):\n",
        "        \"\"\"\n",
        "        Returns the output layer for a given task ID\n",
        "\n",
        "        Args:\n",
        "            task_id (:obj:`int`): the task ID associated with the given \\\n",
        "            model's sub-graph\n",
        "        Return:\n",
        "            :class:`nn.Module`: the output layer for the given task ID\n",
        "        \"\"\"\n",
        "\n",
        "        for op in self.task_map[task_id][1]:\n",
        "            if len(list(self.successors(op))) == 0:\n",
        "                # no successors = output layer\n",
        "                return op\n",
        "\n",
        "        raise ValueError(f'No output layer was found for task ({task_id})')\n",
        "\n",
        "    def clone(self):\n",
        "        \"\"\"\n",
        "        Returns a deep copy of this network.\n",
        "\n",
        "        Returns:\n",
        "            :class:`~Network`: the cloned network\n",
        "        \"\"\"\n",
        "        return deepcopy(self)\n",
        "\n",
        "    def visualize(self, dir='./plots/', filename=None, show_plot=False):\n",
        "        \"\"\"\n",
        "        Visualize the Network's graph using custom layout and positioning\n",
        "        \"\"\"\n",
        "        # default filename\n",
        "        fname = filename or f'model_{self.id}_v{self.version}.svg'\n",
        "\n",
        "        visualize_network(self, dir=dir, filename=fname, show_plot=show_plot)\n",
        "\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    # OPERATORS\n",
        "\n",
        "    def is_equal(self, other, equate_isomorph=False):\n",
        "        \"\"\"\n",
        "        An additional equality test operation with the option to account for\n",
        "        isomorphism\n",
        "\n",
        "        *Note* The :func:`__eq__` operator compares model metadata, which\n",
        "        includes the `wl_hash` (accounts for isomorphism) as well as the\n",
        "        trained tasks. i.e. Two graphs may be identical or isomorphic, but\n",
        "        one of which may have been fine-tuned or retrained, making them\n",
        "        distinct and unequal.\n",
        "\n",
        "        Args:\n",
        "            other (:class:`~Network`): the second operand in the equality test\n",
        "            equate_isomorph (:obj:`bool`): whether or not to account for \\\n",
        "            isomoprhism in the equality\n",
        "\n",
        "        Returns:\n",
        "            :obj:`bool`: result of the comparison test\n",
        "        \"\"\"\n",
        "        if not isinstance(other, Network):\n",
        "            return False\n",
        "\n",
        "        if equate_isomorph:\n",
        "            return hash(self) == hash(other)\n",
        "        else:\n",
        "            return str(self) == str(other)\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.metadata)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __hash__(self):\n",
        "        \"\"\"\n",
        "        Cannot hash the property :func:`~Network.metadata` as it creates a\n",
        "        circular dependency (weisfeiler_lehman hash also calls\n",
        "        :func:`~Network.__hash__`, as well as `total_parameters` + several torch\n",
        "        internal  methods). This preliminary hashing takes into account\n",
        "        attributes that ensure the network's uniqueness\n",
        "\n",
        "        Returns\n",
        "            :obj:`int`: Python's hash of the network's string representation\n",
        "        \"\"\"\n",
        "\n",
        "        return hash(str(self.serialize()))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, Network):\n",
        "            return False\n",
        "\n",
        "        return self.metadata == other.metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Sa3hdXF-Yi"
      },
      "source": [
        "### Continual Search Space Protocol\n",
        "\n",
        "<!---  \n",
        "$file=cl_search_space_protocol.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbZDVujNF-Ko"
      },
      "outputs": [],
      "source": [
        "class CLSearchSpaceProtocol(abc.ABC):\n",
        "    \"\"\"\n",
        "    Methods required to conform to Continual Learning protocol.\n",
        "\n",
        "    These abstract methods ensure that models can be dynamically extended and\n",
        "    augmented as needed.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def random_extend(self, adj_matrix, ops, initial_op_id=None,\n",
        "                      num_vertices=None, max_attempts=10):\n",
        "        \"\"\"\n",
        "        Topology Extension and Consolidation.\n",
        "\n",
        "        Similar to :func:`~random_sample()`, this method should sample a random\n",
        "        extention to a given graph (adjacency matrix + operations list).\n",
        "\n",
        "\n",
        "        Note: the vertices added could contain identity operations, which\n",
        "        would essentially mean a connection/edge is added rather than a node.\n",
        "\n",
        "        Additionally, new edges can probabilistically spawn as we randomly\n",
        "        generate a new $n \\times n$ adjacency matrix and apply logical OR to\n",
        "        mask the extension with the given subgraph.\n",
        "\n",
        "        The extended adjacency matrix guarantees the output layer to \\\n",
        "        be the last vertex (`adj_matrix[-1]`) of the DAG.\n",
        "\n",
        "        Args:\n",
        "            adj_matrix (:class:`numpy.ndarray`): the graph's adjacency matrix\n",
        "            ops (:obj:`list`): the graph's nodes list\n",
        "            initial_op_id (:obj:`int`, optional): the starting operation ID \\\n",
        "            for this extension (recommended value is the \\\n",
        "            `max(o.id for o in supergraph) + 1`; if `None` is given, you may \\\n",
        "            use :func:`~BaseOperation.get_auto_id()`)\n",
        "            num_vertices (:obj:`int`): the number of nodes to be extended\n",
        "            max_attempts (:obj:`int`): maximum attempts to find a valid \\\n",
        "            architecture. If this is exceeded, a \\\n",
        "            `InvalidArchitectureException` with code `3` is raised (caught \\\n",
        "            and handled on the NAS level)\n",
        "\n",
        "        Returns:\n",
        "            :obj:`tuple`: a tuple comprising of an adjacency matrix \\\n",
        "            (:class:`np.ndarray`) and a list of :class:`~PartialWrapper`-\\\n",
        "            encapsulated :class:`~BaseOperation` objects (i.e. the layers in \\\n",
        "            the network)\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNucO-w-YN-u"
      },
      "source": [
        "### Base Search Space\n",
        "\n",
        "<!---  \n",
        "$file=base_search_space.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP7ACb--PhvR"
      },
      "outputs": [],
      "source": [
        "class BaseSearchSpace(abc.ABC):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_vertices,\n",
        "                 operations,\n",
        "                 encoding,\n",
        "                 embed_operations_in_metadata=False):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        assert num_vertices > 2, (\n",
        "            'The given number of vertices is not sufficient'\n",
        "        )\n",
        "\n",
        "        self.num_vertices = num_vertices\n",
        "        self.encoding = encoding\n",
        "        self.embed_operations_in_metadata = embed_operations_in_metadata\n",
        "\n",
        "        self._operation_weights_hooks = []\n",
        "\n",
        "        self.operations = set()\n",
        "\n",
        "        for op in operations:\n",
        "            if issubclass(op, BaseOperation):\n",
        "                # unpack class into partials\n",
        "                self.operations.update(op.get_all_partials())\n",
        "            elif isinstance(op, PartialWrapper):\n",
        "                # already a partial\n",
        "                self.operations.add(op)\n",
        "            else:\n",
        "                raise ValueError(f'Invalid operation type {type(op)}. ' +\n",
        "                                 'Allowed types are `BaseOperation` ' +\n",
        "                                 'subclasses or `PartialWrapper` ' +\n",
        "                                 'encapsulating an `BaseOperation`')\n",
        "\n",
        "        # default op sampling hook (uniform)\n",
        "        self.register_sampling_hook(hook_id=0,\n",
        "                                    hook_func=uniform_operation_weights)\n",
        "\n",
        "        ## [deprecated] - Normalization occurs during the operation assignment\n",
        "        ## as the chosen partials will likely be a subset of all partials\n",
        "\n",
        "        # if len(self.operation_weights) != len(self.operations):\n",
        "        #     raise ValueError('The provided operation weights are not of ' +\n",
        "        #                      'the same length as the provided operations')\n",
        "\n",
        "        # if not math.isclose(sum(self.operation_weights.values()), 1.0):\n",
        "        #     # float comparison, sum of probs is not 1.0\n",
        "        #     total = sum(self.operation_weights.values())\n",
        "\n",
        "        #     # edge case where sum is 0; avoid division by 0\n",
        "        #     if total != 0:\n",
        "        #         # normalize weights\n",
        "        #         self.operation_weights = {key: value / total \\\n",
        "        #                                   for key, value \\\n",
        "        #                                   in self.operation_weights.items()}\n",
        "        #     else:\n",
        "        #         # will default to uniform probability\n",
        "        #         self.operation_weights = None\n",
        "\n",
        "\n",
        "    def register_sampling_hook(self, hook_id, hook_func):\n",
        "        \"\"\"\n",
        "        Registers a new operation-sampling weight calculation hook. Each hook\n",
        "        function feeds the next (as a stack; in the order they were registered).\n",
        "\n",
        "        Hook functions take the following arguments:\n",
        "\n",
        "            operation_set (:obj:`set`): the given filtered set of operations' \\\n",
        "            partials to calculate weights for\n",
        "            weights (:obj:`dict`): a dict of weights corresponding to \\\n",
        "            `operation_set`; calculated from previous hooks. The values in \\\n",
        "            this dict are not guaranteed to sum to `1.0`.\n",
        "            topological_order (:obj:`int`): node's depth in the graph. Could \\\n",
        "            be used to add weight for certain operations in deeper sections \\\n",
        "            of the architecture\n",
        "            max_depth (:obj:`int`): max depth of the given graph\n",
        "            prev_ops (:obj:`list`): list of the current node's direct \\\n",
        "            predecessor(s)\n",
        "            in_shape (:obj:`tuple`): the expected input shape to this node\n",
        "\n",
        "        Args:\n",
        "            hook_id (:obj:`hashable`): a hashable ID for the hook (can be used \\\n",
        "            to :func:`~remove_op_sampling_hook`)\n",
        "            hook_func (:obj:`callable`): the hook function\n",
        "        \"\"\"\n",
        "        assert not any(h[0] == hook_id \\\n",
        "                       for h in self._operation_weights_hooks), (\n",
        "                           'The given hook ID is already in use'\n",
        "                           )\n",
        "\n",
        "        self._operation_weights_hooks.append((hook_id, hook_func))\n",
        "\n",
        "    def remove_sampling_hook(self, hook_id):\n",
        "        \"\"\"\n",
        "        Removes a previously registered operation-sampling hook function.\n",
        "\n",
        "        Args:\n",
        "            hook_id (:obj:`hashable`): a hashable ID for the hook to be removed\n",
        "        \"\"\"\n",
        "        for i, (h_id, hook_func) in enumerate(self._operation_weights_hooks):\n",
        "            if h_id == hook_id:\n",
        "                self._operation_weights_hooks.pop(i)\n",
        "                return\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        ops_metadata = None\n",
        "        if self.embed_operations_in_metadata:\n",
        "            # do not embed op_metadata unless explicitly needed as it bloats\n",
        "            # the results\n",
        "            ops_metadata = [OperationMetadata.init_from_partial(op) \\\n",
        "                            for op in self.operations]\n",
        "\n",
        "        return SearchSpaceMetadata(type=type(self).__name__,\n",
        "                                   num_vertices=self.num_vertices,\n",
        "                                   encoding=self.encoding,\n",
        "                                   operations_metadata=ops_metadata)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.metadata)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.metadata)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, self.__class__):\n",
        "            return False\n",
        "\n",
        "        # if self.num_vertices != other.num_vertices:\n",
        "        #     return False\n",
        "\n",
        "        # if self.encoding != other.encoding:\n",
        "        #     return False\n",
        "\n",
        "        # if len(self.operations) != len(other.operations):\n",
        "        #     return False\n",
        "\n",
        "        # if self.operations != other.operations:\n",
        "        #     # set comparison\n",
        "        #     return False\n",
        "\n",
        "        # return True\n",
        "\n",
        "        # chances of collision are minimal\n",
        "        return self.metadata == other.metadata\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    #   Abstract Methods (implement to conform to this base class)\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def random_sample(self, in_shape, out_shape, max_attempts=10):\n",
        "        \"\"\"\n",
        "        Randomly sample an architecture (a DAG + list of uncompiled operations),\n",
        "        ensuring that:\n",
        "            1. The graph's adjacency matrix is an upper-triangular matrix (\\\n",
        "            directed and acyclic; i.e. represents a DAG)\n",
        "            2. The sampled DAG is a rooted-tree \\\n",
        "            (see :func:`~dag_is_rooted_tree()`)\n",
        "            3. Adjacency matrix has no isolates (0-column/rows)\n",
        "            4. Has a valid respective list of operations/nodes\n",
        "\n",
        "        Args:\n",
        "            in_shape (:obj:`tuple`): the 4-dimensional input shape (BxCxHxW) \\\n",
        "            expected at the input node of the graph\n",
        "            out_shape (:obj:`tuple`): the 2-dimensional output shape (BxN) \\\n",
        "            expected out of the output node of the graph\n",
        "            max_attempts (:obj:`int`, optional): maximum number of attempts to \\\n",
        "            sample a valid architecture, after which the function raises \\\n",
        "            `InvalidArchitectureException` with `code=3`. One scenario this \\\n",
        "            would occur is if the majority of architectures in the search \\\n",
        "            space have been evaluated, or the given input/output shapes do not \\\n",
        "            fit with the given operations. This is a stochastic sampling \\\n",
        "            process and so this parameter is used to add tolerance to missed \\\n",
        "            generations. Exceptions raised are caught and handled on the NAS \\\n",
        "            level\n",
        "            initial_op_id (:obj:`int`, optional): the starting operation ID \\\n",
        "            for this extension (recommended value is the \\\n",
        "            `max(o.id for o in supergraph) + 1`; if `None` is given, you may \\\n",
        "            use :func:`~BaseOperation.get_auto_id()`)\n",
        "\n",
        "        Returns:\n",
        "            :obj:`tuple`: a tuple comprising of an adjacency matrix \\\n",
        "            (:class:`np.ndarray`) and a list of :class:`~PartialWrapper`-\\\n",
        "            encapsulated :class:`~BaseOperation` objects (i.e. the layers in \\\n",
        "            the network).\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n",
        "\n",
        "\n",
        "class InvalidArchitectureException(Exception):\n",
        "    def __init__(self, code, msg):\n",
        "        super().__init__(msg)\n",
        "\n",
        "        self.code = code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSenVbxocVbB"
      },
      "source": [
        "#### Macro Search Space\n",
        "\n",
        "<!---  \n",
        "$file=macro_search_space.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3P0F6QYnseU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MacroSearchSpace(BaseSearchSpace, CLSearchSpaceProtocol):\n",
        "    \"\"\"\n",
        "    Macro Search Space definition.\n",
        "\n",
        "    Often called \"Layer-Wise\",  \"Chain-Structured\", or \"Global\" Search Space\n",
        "    in the literature. This Search Space is expansive (therefore\n",
        "    computationally expensive to optimize) as it contains all\n",
        "    `num_vertices` x `num_vertices` DAG-based architectures (i.e. not just\n",
        "    sequential/chain architectures, but also multi-branch topologies).\n",
        "\n",
        "    Single-branch encoding is supported optionally.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_vertices, operations, encoding):\n",
        "        super(MacroSearchSpace, self).__init__(num_vertices, operations,\n",
        "                                               encoding)\n",
        "\n",
        "    # [DEPRECATED]\n",
        "    # def __enforce_architecture_connectivity(self, dag, num_vertices):\n",
        "    #     # check if a path exists from node 0 to node `num_vertices - 1`\n",
        "    #     if not nx.has_path(dag, source=0, target=num_vertices - 1):\n",
        "\n",
        "    #         max_mid_nodes = 1  # maximum intermediate nodes from input to\n",
        "    #                            # output\n",
        "\n",
        "    #         for i in range(max_mid_nodes):\n",
        "    #             mid_node = num_vertices + i\n",
        "    #             dag.add_node(mid_node)\n",
        "    #             dag.add_edge(0, mid_node)\n",
        "    #             dag.add_edge(mid_node, num_vertices - 1)\n",
        "\n",
        "    #     return dag\n",
        "\n",
        "    def __generate_topology(self, num_vertices, base_topology=None):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            num_vertices (:obj:`int`): the number of vertices in the generated \\\n",
        "            topology\n",
        "            base_topology (:class:`numpy.ndarray`, optional): an optional \\\n",
        "            base adjacency matrix to be extended. If `None` is given, sample \\\n",
        "            from scratch\n",
        "        \"\"\"\n",
        "\n",
        "        # SINGLE BRANCH TOPOLOGY\n",
        "        if self.encoding == 'single-branch':\n",
        "            adj_matrix = np.zeros((num_vertices,\n",
        "                                   num_vertices), dtype=int)\n",
        "\n",
        "            for i in range(num_vertices - 1):\n",
        "                # sequential path from node 0 to node num_vertices-1\n",
        "                adj_matrix[i, i + 1] = 1\n",
        "\n",
        "            if base_topology is not None:\n",
        "                # mask topology excluding the output layer\n",
        "                # (applying element-wise OR to allow the creation of new edges)\n",
        "                # Note: only new nodes will be single branched\n",
        "                base_n = len(base_topology)\n",
        "                mask = np.logical_or(adj_matrix[:base_n,:base_n], base_topology)\n",
        "                adj_matrix[:base_n-1,:base_n-1] = mask[:-1,:-1]\n",
        "                adj_matrix[-1:,-1:] = mask[-1:,-1:] # output layer\n",
        "\n",
        "            return adj_matrix\n",
        "\n",
        "        # MULTI-BRANCH TOPOLOGY\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Step 1: sample random topology\n",
        "        adj_matrix = np.random.choice([0, 1],\n",
        "                                      # non-uniform prob, add: p=[0.7, 0.3])\n",
        "                                      size=(num_vertices,\n",
        "                                            num_vertices))\n",
        "\n",
        "        if base_topology is not None:\n",
        "            # mask topology excluding the output layer\n",
        "            # (applying element-wise OR to allow the creation of new edges)\n",
        "            base_n = len(base_topology)\n",
        "            mask = np.logical_or(adj_matrix[:base_n,:base_n], base_topology)\n",
        "            adj_matrix[:base_n-1,:base_n-1] = mask[:-1,:-1]\n",
        "            adj_matrix[-1:,-1:] = mask[-1:,-1:] # output layer\n",
        "\n",
        "        # ensure no self-connections / prevent cycles\n",
        "        adj_matrix = np.triu(adj_matrix, 1)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Step 2: establish intermediary connections (if applicable)\n",
        "        pred, succ = predecessor_successor_lists(adj_matrix)\n",
        "\n",
        "        in_idx = 0\n",
        "        out_idx = len(adj_matrix) - 1\n",
        "\n",
        "        # ensure all operations have at least 1 input if they have an output\n",
        "        for op_idx, conns in enumerate(pred):\n",
        "            if op_idx == in_idx or op_idx == out_idx:\n",
        "                # skip input/output stems\n",
        "                continue\n",
        "\n",
        "            elif len(conns) == 0 and len(succ[op_idx]) > 0:\n",
        "                # has outputs but no inputs; connect to the input stem\n",
        "                pred[op_idx].append(in_idx)\n",
        "                succ[in_idx].append(op_idx)\n",
        "                adj_matrix[in_idx][op_idx] = adj_matrix[op_idx][in_idx] = 1\n",
        "\n",
        "            elif len(conns) > 0 and len(succ[op_idx]) == 0:\n",
        "                # has inputs but no outputs; connect to output stem\n",
        "                pred[out_idx].append(op_idx)\n",
        "                succ[op_idx].append(out_idx)\n",
        "                adj_matrix[out_idx][op_idx] = adj_matrix[op_idx][out_idx] = 1\n",
        "\n",
        "        # upper triangular matrix (again, post intermediary connections)\n",
        "        adj_matrix = np.triu(adj_matrix, 1)\n",
        "\n",
        "        if dag_is_rooted_tree(adj_matrix):\n",
        "            # valid multi-branch neural network topology\n",
        "            return adj_matrix\n",
        "\n",
        "        raise InvalidArchitectureException(code=1,\n",
        "                                           msg=(\n",
        "                                               'Could not find a valid '\n",
        "                                               'topology'\n",
        "                                          ))\n",
        "\n",
        "\n",
        "    def __assign_operations(self, adj_matrix,\n",
        "                            in_stem_shape, out_stem_shape,\n",
        "                            default_ops=None, initial_op_id=0):\n",
        "        \"\"\"\n",
        "        Takes an architecture as an adjacency matrix + input/output shapes,\n",
        "        and assigns valid operations for all nodes.\n",
        "\n",
        "        This method uses DFS to propagate feature shapes across the graph and\n",
        "        find valid operations/hyperparameters (e.g. too many reduction ops\n",
        "        could yield a (0x0) output, etc.). Additionally, concatenation is also\n",
        "        applied where applicable.\n",
        "\n",
        "        If a `default_ops` parameter is given, the function will extend the\n",
        "        missing nodes.\n",
        "        \"\"\"\n",
        "\n",
        "        num_vertices = len(adj_matrix)\n",
        "        pred, succ = predecessor_successor_lists(adj_matrix)\n",
        "\n",
        "        # set defaults\n",
        "        ops = [None for _ in range(num_vertices)]\n",
        "        id_tracker = initial_op_id\n",
        "\n",
        "        if default_ops is not None:\n",
        "            # augmenting network\n",
        "            ops[:len(default_ops)-1] = default_ops[:-1]\n",
        "        else:\n",
        "            # initial sample\n",
        "            ops[0] = InputStem.get_partials(in_stem_shape)[0]\n",
        "            ops[0].keywords['id'] = id_tracker\n",
        "            id_tracker += 1\n",
        "\n",
        "        # compute topological order\n",
        "        topological_orders = get_topological_orders(pred=pred)\n",
        "\n",
        "        # find output node idx\n",
        "        out_idx = num_vertices - 1\n",
        "        for idx, s_list in enumerate(succ):\n",
        "            if len(s_list) == 0:\n",
        "                # output node found; 0 successors\n",
        "                op_idx = idx\n",
        "                break\n",
        "\n",
        "        def dfs_partials(op_idx):\n",
        "            \"\"\"Traverse the generated DAG using DFS to infer the operations'\n",
        "            valid hyperparameters\n",
        "            \"\"\"\n",
        "\n",
        "            nonlocal id_tracker\n",
        "\n",
        "            if ops[op_idx] is not None:\n",
        "                # memoized shape\n",
        "                return ops[op_idx].keywords['out_shape']\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Compute the input shape to the operation at op_idx.\n",
        "            # Recursively calculated by propagating back to the input layer\n",
        "\n",
        "            if len(pred[op_idx]) == 1:\n",
        "                # single input to op_idx\n",
        "                in_shape = dfs_partials(pred[op_idx][0])\n",
        "            else:\n",
        "                # multi inputs to op_idx\n",
        "                all_inputs = [dfs_partials(conn) for conn in pred[op_idx]]\n",
        "                in_shape = AlignConcat.compute_shape(all_inputs)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Filter given list of operations for validity based on the in_shape\n",
        "            # from the previous step\n",
        "\n",
        "            if op_idx == out_idx:\n",
        "                # output layer; no need to filter or randomly assign operations\n",
        "                ops[op_idx] = OutputStem.get_partials(in_shape,\n",
        "                                                      out_stem_shape[1])[0]\n",
        "            else:\n",
        "                # filter operations for validity\n",
        "                valid_partials = []\n",
        "\n",
        "                for op in self.operations:\n",
        "                    valid_partials.extend(op.func.get_partials(in_shape))\n",
        "\n",
        "                if len(valid_partials) == 0:\n",
        "                    # will be caught internally on the search space level,\n",
        "                    # this is the cleanest approach to exit the recursive\n",
        "                    # control flow\n",
        "                    exc_msg = 'Could not find a valid operation for one of the'\n",
        "                    exc_msg += ' graph\\'s vertices.'\n",
        "                    raise InvalidArchitectureException(code=2,\n",
        "                                                       msg=exc_msg)\n",
        "\n",
        "                # get weights for valid operations' subset\n",
        "                # uniform weight of 1.0 as a default\n",
        "                weights_dict = {op: 1.0 for op in valid_partials}\n",
        "\n",
        "                all_sampled = [(op, depth) \\\n",
        "                               for op, depth in zip(ops, topological_orders) \\\n",
        "                               if op is not None]\n",
        "\n",
        "                for hook_id, weight_hook in self._operation_weights_hooks:\n",
        "                    weights_dict = weight_hook(weights_dict,\n",
        "                                               topological_orders[op_idx],\n",
        "                                               max(topological_orders),\n",
        "                                               [ops[i] for i in pred[op_idx]],\n",
        "                                               all_sampled,\n",
        "                                               in_shape)\n",
        "\n",
        "                    # normalize between hook calls to ensure uniformity\n",
        "                    weights_sum = sum(weights_dict.values())\n",
        "                    weights_dict = {key: value / weights_sum \\\n",
        "                                    for key, value in weights_dict.items()}\n",
        "\n",
        "\n",
        "                # [DEPRECATED]\n",
        "                # if self._operation_weights_hook is not None and False:\n",
        "                #     weights_dict = self.\\\n",
        "                #     _operation_weights_hook(valid_partials)\n",
        "                # else:\n",
        "                #     # defaults to uniform weights across op. groups\n",
        "                #     weights_dict = self._uniform_operation_weights(\n",
        "                #         valid_partials\n",
        "                #     )\n",
        "\n",
        "                # # the operation-weights' calculation does not guarantee order\n",
        "                # ordered_w = []\n",
        "                # for partial in valid_partials:\n",
        "                #     ordered_w.append(weights_dict[partial])\n",
        "\n",
        "                # # normalize weights\n",
        "                # partial_probs = [w / sum(ordered_w) for w in ordered_w]\n",
        "\n",
        "                chosen_partial = np.random.choice(list(weights_dict.keys()),\n",
        "                                                  p=list(weights_dict.values()))\n",
        "\n",
        "                ops[op_idx] = chosen_partial\n",
        "\n",
        "            if 'id' not in ops[op_idx].keywords:\n",
        "                ops[op_idx].keywords['id'] = id_tracker\n",
        "                id_tracker += 1\n",
        "\n",
        "            return ops[op_idx].keywords['out_shape']\n",
        "\n",
        "        dfs_partials(out_idx)  # populate ops list\n",
        "\n",
        "        return ops\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Conforming to Base Search Space\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def random_sample(self,\n",
        "                      in_shape, out_shape,\n",
        "                      max_attempts=10, initial_op_id=0):\n",
        "        \"\"\"\n",
        "        Randomly sample an architecture (a DAG + list of uncompiled operations),\n",
        "        ensuring that:\n",
        "            1. The graph's adjacency matrix is an upper-triangular matrix (\\\n",
        "            directed and acyclic; i.e. represents a DAG)\n",
        "            2. The sampled DAG is a rooted-tree \\\n",
        "            (see :func:`~dag_is_rooted_tree()`)\n",
        "            3. Adjacency matrix has no isolates (0-column/rows)\n",
        "            4. Has a valid respective list of operations/nodes\n",
        "\n",
        "        Args:\n",
        "            in_shape (:obj:`tuple`): the 4-dimensional input shape (BxCxHxW) \\\n",
        "            expected at the input node of the graph\n",
        "            out_shape (:obj:`tuple`): the 2-dimensional output shape (BxN) \\\n",
        "            expected out of the output node of the graph\n",
        "            max_attempts (:obj:`int`, optional): maximum number of attempts to \\\n",
        "            sample a valid architecture, after which the function raises \\\n",
        "            `InvalidArchitectureException` with `code=3`. One scenario this \\\n",
        "            would occur is if the majority of architectures in the search \\\n",
        "            space have been evaluated, or the given input/output shapes do not \\\n",
        "            fit with the given operations. This is a stochastic sampling \\\n",
        "            process and so this parameter is used to add tolerance to missed \\\n",
        "            generations. Exceptions raised are caught and handled on the NAS \\\n",
        "            level\n",
        "            initial_op_id (:obj:`int`, optional): the starting operation ID \\\n",
        "            for this extension (recommended value is the \\\n",
        "            `max(o.id for o in supergraph) + 1`; if `None` is given, you may \\\n",
        "            use :func:`~BaseOperation.get_auto_id()`)\n",
        "\n",
        "        Returns:\n",
        "            :obj:`tuple`: a tuple comprising of an adjacency matrix \\\n",
        "            (:class:`np.ndarray`) and a list of :class:`~PartialWrapper`-\\\n",
        "            encapsulated :class:`~BaseOperation` objects (i.e. the layers in \\\n",
        "            the network)\n",
        "        \"\"\"\n",
        "        for _ in range(max_attempts):\n",
        "            # maximum attempts to find a valid architecture, otherwise raises\n",
        "            # exception\n",
        "\n",
        "            try:\n",
        "                # --------------------------------------------------------------\n",
        "                # Steps 1 & 2: sample a valid architecture topology\n",
        "                adj_matrix = self.__generate_topology(self.num_vertices)\n",
        "\n",
        "                # --------------------------------------------------------------\n",
        "                # Step 3: trim degree zero nodes (if any exist)\n",
        "                adj_matrix = trim_isolate_nodes(adj_matrix)\n",
        "\n",
        "                # --------------------------------------------------------------\n",
        "                # Step 4: sample random valid operations for the given topology\n",
        "                ops = self.__assign_operations(adj_matrix,\n",
        "                                               in_shape, out_shape,\n",
        "                                               initial_op_id=initial_op_id)\n",
        "\n",
        "\n",
        "            except InvalidArchitectureException as e:\n",
        "                if e.code == 1:\n",
        "                    # failed to establish a valid topology\n",
        "                    continue\n",
        "                elif e.code == 2:\n",
        "                    # could not find a valid operation for one or more nodes\n",
        "                    continue\n",
        "                else:\n",
        "                    raise e\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "\n",
        "            # valid architecture found\n",
        "            return adj_matrix, ops\n",
        "\n",
        "        exc_msg = f'Could not find a valid architecture in {max_attempts} '\n",
        "        exc_msg += 'attempts!'\n",
        "\n",
        "        raise InvalidArchitectureException(code=3,\n",
        "                                           msg=exc_msg)\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Conforming to the Contiual Search Space Protocol\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def random_extend(self, adj_matrix, ops, initial_op_id=None,\n",
        "                      num_vertices=None, max_attempts=10):\n",
        "        \"\"\"\n",
        "        Topology Extension and Consolidation module.\n",
        "\n",
        "        Similar to :func:`~random_sample()`, this method samples a random\n",
        "        extention to a given graph (adjacency matrix + operations).\n",
        "\n",
        "\n",
        "        Note: the vertices added could contain identity operations, which\n",
        "        would essentially mean a connection/edge is added rather than a node.\n",
        "\n",
        "        Additionally, new edges can probabilistically spawn as we randomly\n",
        "        generate a new $n \\times n$ adjacency matrix and apply logical OR to\n",
        "        mask the extension with the given subgraph.\n",
        "\n",
        "        The extended adjacency matrix guarantees the output layer to \\\n",
        "        be the last vertex (`adj_matrix[-1]`) of the DAG.\n",
        "\n",
        "        Args:\n",
        "            adj_matrix (:class:`numpy.ndarray`): the graph's adjacency matrix\n",
        "            ops (:obj:`list`): the graph's nodes list\n",
        "            initial_op_id (:obj:`int`, optional): the starting operation ID \\\n",
        "            for this extension (recommended value is the \\\n",
        "            `max(o.id for o in supergraph) + 1`; if `None` is given, you may \\\n",
        "            use :func:`~BaseOperation.get_auto_id()`)\n",
        "            num_vertices (:obj:`int`): the number of nodes to be extended\n",
        "            max_attempts (:obj:`int`): maximum attempts to find a valid \\\n",
        "            architecture. If this is exceeded, a \\\n",
        "            `InvalidArchitectureException` with code `3` is raised (caught \\\n",
        "            and handled on the NAS level)\n",
        "\n",
        "        Returns:\n",
        "            :obj:`tuple`: a tuple comprising of an adjacency matrix \\\n",
        "            (:class:`np.ndarray`) and a list of :class:`~PartialWrapper`-\\\n",
        "            encapsulated :class:`~BaseOperation` objects (i.e. the layers in \\\n",
        "            the network)\n",
        "        \"\"\"\n",
        "\n",
        "        curr_n = len(adj_matrix)\n",
        "        # default to a quarter of the initial `num_vertices` of the search space\n",
        "        # with a minimum value of 1 vertex extension\n",
        "        n_extend = num_vertices if num_vertices else max(self.num_vertices // 4,\n",
        "                                                         1)\n",
        "        extended_shape = curr_n + n_extend\n",
        "\n",
        "        for _ in range(max_attempts):\n",
        "            # maximum attempts to find a valid architecture, otherwise raises\n",
        "            # exception\n",
        "\n",
        "            try:\n",
        "                # --------------------------------------------------------------\n",
        "                # Steps 1 & 2: extend adjacency matrix by `n_extend`\n",
        "                ext_adj = self.__generate_topology(extended_shape,\n",
        "                                                   base_topology=adj_matrix)\n",
        "\n",
        "                # --------------------------------------------------------------\n",
        "                # Step 3: trim degree zero nodes (if any exist)\n",
        "                ext_adj = trim_isolate_nodes(ext_adj)\n",
        "\n",
        "                # --------------------------------------------------------------\n",
        "                # Step 4: sample valid operations for the given topology\n",
        "                out_shape = ops[-1].keywords['out_shape']\n",
        "                initial_id = initial_op_id or BaseOperation.get_auto_id()\n",
        "                ext_ops = self.__assign_operations(ext_adj,\n",
        "                                                   in_stem_shape=None,\n",
        "                                                   out_stem_shape=out_shape,\n",
        "                                                   default_ops=ops,\n",
        "                                                   initial_op_id=initial_id)\n",
        "                # replace the output layer's ID (the criteria used for\n",
        "                # consolidation on the `Network` level)\n",
        "                ext_ops[-1].keywords['id'] = ops[-1].keywords['id']\n",
        "\n",
        "            except InvalidArchitectureException as e:\n",
        "                if e.code == 1:\n",
        "                    # failed to establish a valid topology\n",
        "                    continue\n",
        "                elif e.code == 2:\n",
        "                    # could not find a valid operation for one or more nodes\n",
        "                    continue\n",
        "                else:\n",
        "                    raise e\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "\n",
        "            # valid architecture found\n",
        "            return ext_adj, ext_ops\n",
        "\n",
        "        exc_msg = f'Could not find a valid architecture in {max_attempts} '\n",
        "        exc_msg += 'attempts!'\n",
        "\n",
        "        raise InvalidArchitectureException(code=3,\n",
        "                                           msg=exc_msg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6QVLAjtERnT"
      },
      "source": [
        "## Search Algorithm\n",
        "\n",
        "<!---  \n",
        "$module=optimizer\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWer-MfFD1FL"
      },
      "source": [
        "### Continual Optimization Protocol\n",
        "\n",
        "<!---  \n",
        "$file=cl_optimizer_protocol.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKSF0o5tEBXi"
      },
      "outputs": [],
      "source": [
        "class CLOptimizerProtocol(abc.ABC):\n",
        "    \"\"\"\n",
        "    Methods required to conform to Continual Learning protocol.\n",
        "\n",
        "    These abstract methods ensure that the optimizer can handle all three types\n",
        "    of incremental learning (domain-/class-/task-incremental).\n",
        "    \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Checks if a given task has already been encountered through the model\n",
        "        evaluation metrics.\n",
        "\n",
        "        This method is responsible for:\n",
        "            1- adapting the relevant output layer in a model if the number of\n",
        "            classes is altered\n",
        "            2- detecting if fine-tuning is required\n",
        "\n",
        "        Returns:\n",
        "            :obj:`tuple`: a tuple containing the current best performing \\\n",
        "            model(s) and a boolean indicating whether fine-tuning is required. \\\n",
        "            It is guaranteed that if fine-tuning is required \\\n",
        "            (i.e. `ret_tuple[1] == True`) is `True`, then the best performing \\\n",
        "            model(s) will exist (i.e. `ret_tuple[0]` will not be `None`).\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def augment(self, base_model):\n",
        "        \"\"\"\n",
        "        Identifies the type of incremental learning and augments the model\n",
        "        accordingly. This method is similar to :func:`~BaseOptimizer.sample()`\n",
        "        with the exception that it takes a existing `base_model` and either:\n",
        "\n",
        "            1- Extends the model; increases the complexity of a task's \\\n",
        "            sub-graph within the model (by adding vertices). This is to \\\n",
        "            overcome growing complexities in Domain-/Class-Incremental \\\n",
        "            problems, or\n",
        "            2- Samples a new architecture for a Task-Incremental scenario and \\\n",
        "            consolidates it with the existing `base_model` (should incentivize \\\n",
        "            operation overlapping).\n",
        "\n",
        "        This is used when a model's performance drops due to a shift in the\n",
        "        underlying distribution of the dataset. In most scenarios, fine-tuning\n",
        "        the model overcomes the shift, however, when the dataset's complexity\n",
        "        increases, this method might be called to adapt the respective portion\n",
        "        of the model accordingly.\n",
        "\n",
        "        Args:\n",
        "            base_model (:class:`~Network`): the base model comprising of \\\n",
        "            multiple sub-graphs for all encountered tasks.\n",
        "        Returns:\n",
        "            :obj:`list`: a list of :class:`~Network` objects, containing the \\\n",
        "            augmented model(s)\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLNgZan5EZ_B"
      },
      "source": [
        "### Base Optimizer\n",
        "\n",
        "<!---  \n",
        "$file=base_optimizer.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8X0exZ7EUDU"
      },
      "outputs": [],
      "source": [
        "class BaseOptimizer(abc.ABC):\n",
        "\n",
        "    def __init__(self, invalid_arch_tolerance, cache_top_n_candidates):\n",
        "        super().__init__()\n",
        "\n",
        "        self._current_task = None\n",
        "        self._best_candidate = None\n",
        "        self._enable_cl = False\n",
        "\n",
        "        self._cache_top_n_candidates = cache_top_n_candidates\n",
        "\n",
        "        self._invalid_arch_tolerance = invalid_arch_tolerance\n",
        "\n",
        "        self.metrics = OptimizerMetrics()\n",
        "\n",
        "\n",
        "    def _candidate_scoring(self, models, tasks):\n",
        "        \"\"\"\n",
        "        Calculate scalar scores for all encountered tasks and gets 1 mean score\n",
        "        per model (weighted average of all tasks based on\n",
        "        :func:`~BaseTask.importance_weight`)\n",
        "\n",
        "        Scoring is a relative function that requires more than 1 model\n",
        "\n",
        "        Returns:\n",
        "            :obj:`list`: list of scalar scores per model across given tasks. \\\n",
        "            The returned scores' list preserves the order of the passed `models`\n",
        "        \"\"\"\n",
        "\n",
        "        scores_per_model = [0.0 for _ in models]    # init with 0.0 scores\n",
        "        for task in tasks:\n",
        "            # each task is scored individually as:\n",
        "            # a) tasks could have different scoring functions,\n",
        "            # b) the accuracy obtained on - for instance - ImageNet cannot be\n",
        "            # compared with that of CIFAR10 (number of classes + complexity) and\n",
        "            # c) to support multi-modality in the future\n",
        "\n",
        "            t_scores = task.scoring_func(models, task)\n",
        "            # [1 scalar score per model]\n",
        "            for m_idx, score in enumerate(t_scores):\n",
        "                scores_per_model[m_idx] += score\n",
        "\n",
        "        # return score mean (across tasks)\n",
        "\n",
        "        # TODO: rethink the logic here. Do we really need to divide by a\n",
        "        # constant value when the scores are relative to one another?\n",
        "        return [score / len(tasks) for score in scores_per_model]\n",
        "\n",
        "\n",
        "    def candidate_selection(self, models, tasks):\n",
        "        \"\"\"\n",
        "        Default candidate selection across all optimizers (we keep track of the\n",
        "        best candidate in all cases; storing additional models for population-\n",
        "        based optimizers can be done on the sub-class level along with their\n",
        "        memory-management).\n",
        "\n",
        "        This method uses the internal scoring functions within each given task\n",
        "        and the training metrics stored on the model level. These scalar scores\n",
        "        are then used to rank the highest performing model from the given set\n",
        "        of models and caching it to `self._best_candidate`.\n",
        "\n",
        "        Args:\n",
        "            models (:obj:`list`): list of :class:`~Network` objects; all \\\n",
        "            selectable models (newly evaluated model + cached best-performing \\\n",
        "            model in most cases)\n",
        "            tasks (:obj:`list`): list of all encountered tasks\n",
        "        \"\"\"\n",
        "        assert models and len(models) > 0, 'No models were passed to selection'\n",
        "\n",
        "        if self._best_candidate is None:\n",
        "            # intial candidate(s)\n",
        "            if len(models) == 1:\n",
        "                self._best_candidate = models[0]\n",
        "                return\n",
        "            # else; calculate relative scores for all given models\n",
        "\n",
        "        # relative scoring between given model(s) and the cached best candidate\n",
        "        all_candidates = [self._best_candidate]\n",
        "        all_candidates.extend(models)\n",
        "\n",
        "        scores = self._candidate_scoring(all_candidates, tasks)\n",
        "        # sort scores in descending order; highest score at [0]\n",
        "        # (this is also used for gc later)\n",
        "        # `scores` preserves ordering; i.e. scores[0] -> models[0]'s score\n",
        "        sorted_scores = sorted(scores, reverse=True)\n",
        "\n",
        "        candidate_idx = scores.index(sorted_scores[0])\n",
        "\n",
        "        if candidate_idx != 0:\n",
        "            # new best candidate was selected\n",
        "            old_id = self._best_candidate.id\n",
        "            old_v = self._best_candidate.version\n",
        "            new_id = all_candidates[candidate_idx].id\n",
        "            new_v = all_candidates[candidate_idx].version\n",
        "\n",
        "            Logger.critical((\n",
        "                f'A new top candidate has been selected '\n",
        "                f'(Model ({old_id} v{old_v}) -> Model ({new_id} v{new_v})'\n",
        "            ))\n",
        "\n",
        "        self._best_candidate = all_candidates[candidate_idx]\n",
        "\n",
        "        # housekeeping; delete models outside the caching range\n",
        "        for s_idx, score in enumerate(sorted_scores):\n",
        "            if s_idx > self._cache_top_n_candidates - 1:\n",
        "                # get candidate's index from preserved scores list\n",
        "                del all_candidates[scores.index(score)]\n",
        "                # gc triggered once at the end of each NAS cycle for efficiency\n",
        "\n",
        "\n",
        "    def add_results(self, model_metrics):\n",
        "        \"\"\"\n",
        "        Append aggregated model metrics\n",
        "        \"\"\"\n",
        "\n",
        "        # commit new results to optimizer records\n",
        "        self.metrics.add_results(model_metrics)\n",
        "\n",
        "\n",
        "    def assign_task(self, task):\n",
        "        \"\"\"\n",
        "        Sets the current task for the optimizer\n",
        "\n",
        "        Args:\n",
        "            task (:class:`~BaseTask`): the task set to be activated\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(task, BaseTask), (\n",
        "            'Invalid task provided. '\n",
        "            f'expected type {type(BaseTask)}, received '\n",
        "            f'{type(task)}'\n",
        "        )\n",
        "\n",
        "        assert hasattr(task, 'search_space') and \\\n",
        "        isinstance(task.search_space, BaseSearchSpace), (\n",
        "            'Invalid search space provided. '\n",
        "            f'expected type {type(BaseSearchSpace)}, received '\n",
        "            f'{type(task.search_space)}'\n",
        "        )\n",
        "\n",
        "        self._current_task = task\n",
        "\n",
        "\n",
        "    @property\n",
        "    def top_metrics(self):\n",
        "        return self._best_candidate.metrics\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    #   Abstract Methods (implement to conform to this base class)\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def sample(self):\n",
        "        \"\"\"\n",
        "        Samples a new architecture from the assigned task's search space (\n",
        "        accessible through `optimizer._current_task.search_space`)\n",
        "\n",
        "        Returns:\n",
        "            :obj:`list`: a list of :class:`~Network` (contains $> 1$ model for \\\n",
        "            population-based optimizers, a list of 1 element otherwise ). An \\\n",
        "            empty list or `None` may be returned if the optimizer failed to \\\n",
        "            sample an architecture.\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0MVMHRDG0bM"
      },
      "source": [
        "\n",
        "#### Random Search\n",
        "\n",
        "<!---  \n",
        "$file=random_search.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx17505cJo5r"
      },
      "outputs": [],
      "source": [
        "class RandomSearch(BaseOptimizer, CLOptimizerProtocol):\n",
        "\n",
        "    def __init__(self, invalid_arch_tolerance=10, cache_top_n_candidates=1):\n",
        "        super().__init__(invalid_arch_tolerance, cache_top_n_candidates)\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Conforming to Base Optimizer\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"\n",
        "        Samples a new architecture from the assigned task's search space.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`list`: a list of :class:`~Network` objects, containing the \\\n",
        "            sampled model\n",
        "        \"\"\"\n",
        "        assert self._current_task is not None, (\n",
        "            'A task must be set prior to sampling'\n",
        "        )\n",
        "\n",
        "        task = self._current_task\n",
        "        in_shape, out_shape = task.shapes\n",
        "\n",
        "        for tolerance_idx in range(self._invalid_arch_tolerance):\n",
        "            # two types of invalid architectures: previously evaluated,\n",
        "            # or dysfunctional network (couldn't find a valid layer for\n",
        "            # one or more nodes, etc.)\n",
        "            try:\n",
        "                # sample a random architecture from the task's search space\n",
        "                adj_matrix, ops = task.search_space.random_sample(\n",
        "                    in_shape=in_shape,\n",
        "                    out_shape=out_shape,\n",
        "                    max_attempts=self._invalid_arch_tolerance - tolerance_idx\n",
        "                )\n",
        "\n",
        "                model = Network()\n",
        "                model.compile(adj_matrix, ops, task.metadata)\n",
        "\n",
        "                if self.metrics.model_exists(model):\n",
        "                    # model isomorphism has been evaluated\n",
        "                    del model\n",
        "                    continue\n",
        "\n",
        "                # return as a list to conform with population-based optimizers\n",
        "                return [model]\n",
        "\n",
        "            except InvalidArchitectureException as e:\n",
        "                if e.code == 3:\n",
        "                    # failed to find a valid architecture\n",
        "                    return None\n",
        "                raise e\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Conforming to Continual Optimizer Protocol\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Checks if a given task has already been encountered through the model\n",
        "        evaluation metrics.\n",
        "\n",
        "        This method is responsible for:\n",
        "            1- adapting the relevant output layer in a model if the number of\n",
        "            classes is altered\n",
        "            2- detecting if fine-tuning is required\n",
        "\n",
        "        Returns:\n",
        "            :obj:`tuple`: a tuple containing the current best performing \\\n",
        "            model(s) and a boolean indicating whether fine-tuning is required. \\\n",
        "            It is guaranteed that if fine-tuning is required \\\n",
        "            (i.e. `ret_tuple[1] == True`) is `True`, then the best performing \\\n",
        "            model(s) will exist (i.e. `ret_tuple[0]` will not be `None`).\n",
        "        \"\"\"\n",
        "\n",
        "        assert self._current_task is not None, (\n",
        "            'A task must be set prior to `fit()`. Use '\n",
        "            '`optimizer.assign_task()` to set a task.'\n",
        "        )\n",
        "\n",
        "        task = self._current_task\n",
        "        require_fine_tuning = False\n",
        "\n",
        "        if self._best_candidate is not None:\n",
        "            # Perform class/domain adaptation check\n",
        "            for m_idx, m in enumerate(self._best_candidate.tasks_metadata):\n",
        "                if m.id == task.metadata.id:\n",
        "                    # class adaptation check\n",
        "                    if hasattr(task.metadata, 'classes') and \\\n",
        "                    not set(task.metadata.classes).issubset(set(m.classes)):\n",
        "                        # class-incremental learning\n",
        "                        self._best_candidate.adapt_output(task.metadata.id,\n",
        "                                                          len(task.classes))\n",
        "\n",
        "                        # update task's metadata according to the new increment\n",
        "                        # This assumes that\n",
        "                        # `SegmentableImageFolder.accumulate_classes` is `True`\n",
        "                        new_t = task.metadata\n",
        "                        self._best_candidate.tasks_metadata[m_idx] = new_t\n",
        "\n",
        "                        require_fine_tuning = True\n",
        "\n",
        "                    # domain adaptation check\n",
        "                    # if task.boundary.distance_from(m.boundary) > threshold:\n",
        "                    #     require_fine_tuning = True\n",
        "                    break\n",
        "\n",
        "            # _best_candidate could be `None` on the first optimization itr.\n",
        "            # if require_fine_tuning is `True`, the model is guaranteed to exist\n",
        "            return self._best_candidate, require_fine_tuning\n",
        "\n",
        "        return None, False\n",
        "\n",
        "\n",
        "    def augment(self, base_model):\n",
        "        \"\"\"\n",
        "        Identifies the type of incremental learning and augments the model\n",
        "        accordingly. This method is similar to :func:`~BaseOptimizer.sample()`\n",
        "        with the exception that it takes a existing `base_model` and either:\n",
        "\n",
        "            1- Extends the model; increases the complexity of a task's \\\n",
        "            sub-graph within the model (by adding vertices). This is to \\\n",
        "            overcome growing complexities in Domain-/Class-Incremental \\\n",
        "            problems, or\n",
        "            2- Samples a new architecture for a Task-Incremental scenario and \\\n",
        "            consolidates it with the existing `base_model` (incentivizes \\\n",
        "            operation overlapping).\n",
        "\n",
        "        This is used when a model's performance drops due to a shift in the\n",
        "        underlying distribution of the dataset. In most scenarios, fine-tuning\n",
        "        the model overcomes the shift, however, when the dataset's complexity\n",
        "        increases, this method might be called to adapt the respective portion\n",
        "        of the model accordingly.\n",
        "\n",
        "        Args:\n",
        "            base_model (:class:`~Network`): the base model comprising of \\\n",
        "            multiple sub-graphs for all given tasks.\n",
        "        Returns:\n",
        "            :obj:`list`: a list of :class:`~Network` objects, containing the \\\n",
        "            augmented model\n",
        "        \"\"\"\n",
        "        assert self._current_task is not None, (\n",
        "            'A task must be set prior to sampling'\n",
        "        )\n",
        "\n",
        "        assert base_model is None or isinstance(base_model, Network), (\n",
        "            'The provided model must be of type `Network`'\n",
        "        )\n",
        "\n",
        "        task = self._current_task\n",
        "        in_shape, out_shape = task.shapes\n",
        "        initial_id = max([o.id for o in base_model.nodes()]) + 1\n",
        "\n",
        "        for tolerance_idx in range(self._invalid_arch_tolerance):\n",
        "            # two types of invalid architectures: previously evaluated,\n",
        "            # or non-functional DNN\n",
        "            try:\n",
        "                adj_matrix = None\n",
        "                ops = None\n",
        "\n",
        "                if any([True for t in base_model.tasks_metadata \\\n",
        "                        if t.id == task.id]):\n",
        "                    # task was previously encountered; DIL or CIL -> extend\n",
        "\n",
        "                    # extend existing subgraph to increase its complexity\n",
        "                    Logger.critical((\n",
        "                        f'Extending subgraph for task ({task.id} '\n",
        "                        f'v.{task.version})...'\n",
        "                    ))\n",
        "\n",
        "                    subgraph = base_model.get_subgraph(task.id)\n",
        "                    decomp_ops = [op.decompile() for op in subgraph[1]]\n",
        "\n",
        "                    adj_matrix,\\\n",
        "                    ops = task.search_space.random_extend(subgraph[0],\n",
        "                                                          decomp_ops,\n",
        "                                                          initial_op_id=\\\n",
        "                                                          initial_id)\n",
        "\n",
        "                else:\n",
        "                    # new task; TIL\n",
        "\n",
        "                    # enable overlapping incentivization\n",
        "                    depths = get_topological_orders(base_model.adj_matrix)\n",
        "                    zipped_ops = zip(base_model.nodes(), depths)\n",
        "                    decomp_ops = [(node.decompile(), depth) \\\n",
        "                                  for node, depth in zipped_ops]\n",
        "\n",
        "                    hook_id = '__op_overlap_hook'\n",
        "                    hk = incentivize_operation_overlap_hook_factory(decomp_ops)\n",
        "                    task.search_space.register_sampling_hook(hook_id,\n",
        "                                                             hk)\n",
        "\n",
        "                    # sample new architceture that has incentivized\n",
        "                    # intersection with `base_model`\n",
        "                    adj_matrix, ops = task.search_space.random_sample(\n",
        "                        in_shape=in_shape,\n",
        "                        out_shape=out_shape,\n",
        "                        max_attempts=self._invalid_arch_tolerance - \\\n",
        "                        tolerance_idx,\n",
        "                        initial_op_id=initial_id)\n",
        "\n",
        "                    task.search_space.remove_sampling_hook(hook_id)\n",
        "\n",
        "                # clone model in case we need to revert back to older model\n",
        "                model = base_model.clone()\n",
        "                model.compile(adj_matrix, ops, task.metadata)\n",
        "\n",
        "                if self.metrics.model_exists(model):\n",
        "                    # model isomorphism has been evaluated\n",
        "                    del model\n",
        "                    continue\n",
        "\n",
        "                # return as a list to conform with population-based optimizers\n",
        "                return [model]\n",
        "\n",
        "            except InvalidArchitectureException as e:\n",
        "                if e.code == 3:\n",
        "                    # failed to find a valid architecture\n",
        "                    return None\n",
        "                raise e\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIMMmv23KKNg"
      },
      "source": [
        "#### Genetic Algorithms\n",
        "\n",
        "<!---  \n",
        "$file=genetic_algorithms.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2XIM0LQKNyO"
      },
      "outputs": [],
      "source": [
        "# class GeneticAlgorithms(BaseOptimizer, CLOptimizerProtocol):\n",
        "\n",
        "#     def __init__(self, population_size,\n",
        "#                  invalid_arch_tolerance=10, cache_top_n_candidates=1):\n",
        "\n",
        "#         super().__init__(invalid_arch_tolerance, cache_top_n_candidates)\n",
        "\n",
        "#         self.__population = None\n",
        "#         self.population_size = population_size\n",
        "\n",
        "\n",
        "#     def __initialize_population(self):\n",
        "#         \"\"\"\n",
        "#         Initializes the population with random models.\n",
        "\n",
        "#         Returns:\n",
        "#             List[:class:`~Network`]: A list of randomly initialized models.\n",
        "#         \"\"\"\n",
        "#         new_population = []\n",
        "#         task = self._current_task\n",
        "#         in_shape, out_shape = task.shapes\n",
        "\n",
        "#         for _ in range(self.population_size):\n",
        "#             adj_matrix, ops = task.search_space.random_sample(\n",
        "#                 in_shape=in_shape, out_shape=out_shape)\n",
        "\n",
        "#             model = Network()\n",
        "#             model.compile(adj_matrix, ops, task.metadata)\n",
        "#             new_population.append(model)\n",
        "\n",
        "#         return new_population\n",
        "\n",
        "\n",
        "#     def __mutate_population(self):\n",
        "#         \"\"\"\n",
        "#         Mutates the existing population to create a new generation.\n",
        "\n",
        "#         Returns:\n",
        "#             List[:class:`~Network`]: A list of mutated models.\n",
        "#         \"\"\"\n",
        "#         mutated_population = []\n",
        "#         task = self._current_task\n",
        "\n",
        "#         for parent_model in self.__population:\n",
        "#             adj_matrix, ops = parent_model.get_architecture()\n",
        "\n",
        "#             try:\n",
        "#                 new_adj_matrix, new_ops = task.search_space.mutate(adj_matrix, ops)\n",
        "#                 mutated_model = Network()\n",
        "#                 mutated_model.compile(new_adj_matrix, new_ops, task.metadata)\n",
        "#                 if not self.metrics.model_exists(mutated_model):\n",
        "#                     mutated_population.append(mutated_model)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error during mutation or compilation: {e}\")\n",
        "\n",
        "#         return mutated_population\n",
        "\n",
        "\n",
        "#     # --------------------------------------------------------------------------\n",
        "#     # Conforming to Base Optimizer\n",
        "#     # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#     def sample(self):\n",
        "#         \"\"\"\n",
        "#         Samples or mutates models based on the current state of the population.\n",
        "#         If this is the first generation, it initializes the population randomly.\n",
        "#         Otherwise, it mutates the existing population to create a new generation.\n",
        "\n",
        "#         Returns:\n",
        "#             List[:class:`~Network`]: The current generation of models.\n",
        "#         \"\"\"\n",
        "\n",
        "#         if self.__population is None:\n",
        "#             # Initialize the first generation with random models\n",
        "#             self.__population = self.__initialize_population()\n",
        "#         else:\n",
        "#             # Mutate the existing population to create a new generation\n",
        "#             self.__population = self.__mutate_population()\n",
        "\n",
        "#         return self.__population\n",
        "\n",
        "\n",
        "#     # --------------------------------------------------------------------------\n",
        "#     # Conforming to Continual Optimizer Protocol\n",
        "#     # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#     def fit(self):\n",
        "#         \"\"\"\n",
        "#         Checks if a given task has already been encountered through the model\n",
        "#         evaluation metrics.\n",
        "\n",
        "#         This method is responsible for:\n",
        "#             1- adapting the relevant output layer in a model if the number of\n",
        "#             classes is altered\n",
        "#             2- detecting if fine-tuning is required\n",
        "\n",
        "#         Returns:\n",
        "#             :obj:`tuple`: a tuple containing the current best performing \\\n",
        "#             model(s) and a boolean indicating whether fine-tuning is required. \\\n",
        "#             It is guaranteed that if fine-tuning is required \\\n",
        "#             (i.e. `ret_tuple[1] == True`) is `True`, then the best performing \\\n",
        "#             model(s) will exist (i.e. `ret_tuple[0]` will not be `None`).\n",
        "#         \"\"\"\n",
        "\n",
        "#         assert self._current_task is not None, (\n",
        "#             'A task must be set prior to `fit()`. Use '\n",
        "#             '`optimizer.assign_task()` to set a task.'\n",
        "#         )\n",
        "\n",
        "#         task = self._current_task\n",
        "#         require_fine_tuning = False\n",
        "\n",
        "#         if self._best_candidate is not None:\n",
        "#             # Perform class/domain adaptation check\n",
        "#             for m_idx, m in enumerate(self._best_candidate.tasks_metadata):\n",
        "#                 if m.id == task.metadata.id:\n",
        "#                     # class adaptation check\n",
        "#                     if hasattr(task.metadata, 'classes') and \\\n",
        "#                     not set(task.metadata.classes).issubset(set(m.classes)):\n",
        "#                         # class-incremental learning\n",
        "#                         self._best_candidate.adapt_output(task.metadata.id,\n",
        "#                                                           len(task.classes))\n",
        "\n",
        "#                         # update task's metadata according to the new increment\n",
        "#                         # This assumes that\n",
        "#                         # `SegmentableImageFolder.accumulate_classes` is `True`\n",
        "#                         new_t = task.metadata\n",
        "#                         self._best_candidate.tasks_metadata[m_idx] = new_t\n",
        "\n",
        "#                         require_fine_tuning = True\n",
        "\n",
        "#                     # domain adaptation check\n",
        "#                     # if task.boundary.distance_from(m.boundary) > threshold:\n",
        "#                     #     require_fine_tuning = True\n",
        "#                     break\n",
        "\n",
        "#             # _best_candidate could be `None` on the first optimization itr.\n",
        "#             # if require_fine_tuning is `True`, the model is guaranteed to exist\n",
        "#             return self._best_candidate, require_fine_tuning\n",
        "\n",
        "#         return None, False\n",
        "\n",
        "\n",
        "#     def augment(self, base_model):\n",
        "#         \"\"\"\n",
        "#         \"\"\"\n",
        "\n",
        "#         pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reinforcement Learning\n",
        "\n",
        "<!---  \n",
        "$file=reinforcement_learning.py\n",
        "-->"
      ],
      "metadata": {
        "id": "VFDCHhOUKO9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class ReinforcementLearning(BaseOptimizer, CLOptimizerProtocol):\n",
        "\n",
        "#     def __init__(self, population_size,\n",
        "#                  invalid_arch_tolerance=10, cache_top_n_candidates=1):\n",
        "\n",
        "#         super().__init__(invalid_arch_tolerance, cache_top_n_candidates)\n",
        "\n",
        "#         self.__population = None\n",
        "#         self.population_size = population_size\n",
        "\n",
        "\n",
        "#     def __initialize_population(self):\n",
        "#         \"\"\"\n",
        "#         Initializes the population with random models.\n",
        "\n",
        "#         Returns:\n",
        "#             List[:class:`~Network`]: A list of randomly initialized models.\n",
        "#         \"\"\"\n",
        "#         new_population = []\n",
        "#         task = self._current_task\n",
        "#         in_shape, out_shape = task.shapes\n",
        "\n",
        "#         for _ in range(self.population_size):\n",
        "#             adj_matrix, ops = task.search_space.random_sample(\n",
        "#                 in_shape=in_shape, out_shape=out_shape)\n",
        "\n",
        "#             model = Network()\n",
        "#             model.compile(adj_matrix, ops, task.metadata)\n",
        "#             new_population.append(model)\n",
        "\n",
        "#         return new_population\n",
        "\n",
        "\n",
        "#     def __mutate_population(self):\n",
        "#         \"\"\"\n",
        "#         Mutates the existing population to create a new generation.\n",
        "\n",
        "#         Returns:\n",
        "#             List[:class:`~Network`]: A list of mutated models.\n",
        "#         \"\"\"\n",
        "#         mutated_population = []\n",
        "#         task = self._current_task\n",
        "\n",
        "#         for parent_model in self.__population:\n",
        "#             adj_matrix, ops = parent_model.get_architecture()\n",
        "\n",
        "#             try:\n",
        "#                 new_adj_matrix, new_ops = task.search_space.mutate(adj_matrix, ops)\n",
        "#                 mutated_model = Network()\n",
        "#                 mutated_model.compile(new_adj_matrix, new_ops, task.metadata)\n",
        "#                 if not self.metrics.model_exists(mutated_model):\n",
        "#                     mutated_population.append(mutated_model)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error during mutation or compilation: {e}\")\n",
        "\n",
        "#         return mutated_population\n",
        "\n",
        "\n",
        "#     # --------------------------------------------------------------------------\n",
        "#     # Conforming to Base Optimizer\n",
        "#     # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#     def sample(self):\n",
        "#         \"\"\"\n",
        "#         Samples or mutates models based on the current state of the population.\n",
        "#         If this is the first generation, it initializes the population randomly.\n",
        "#         Otherwise, it mutates the existing population to create a new generation.\n",
        "\n",
        "#         Returns:\n",
        "#             List[:class:`~Network`]: The current generation of models.\n",
        "#         \"\"\"\n",
        "\n",
        "#         if self.__population is None:\n",
        "#             # Initialize the first generation with random models\n",
        "#             self.__population = self.__initialize_population()\n",
        "#         else:\n",
        "#             # Mutate the existing population to create a new generation\n",
        "#             self.__population = self.__mutate_population()\n",
        "\n",
        "#         return self.__population\n",
        "\n",
        "\n",
        "#     # --------------------------------------------------------------------------\n",
        "#     # Conforming to Continual Optimizer Protocol\n",
        "#     # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#     def fit(self):\n",
        "#         \"\"\"\n",
        "#         Checks if a given task has already been encountered through the model\n",
        "#         evaluation metrics.\n",
        "\n",
        "#         This method is responsible for:\n",
        "#             1- adapting the relevant output layer in a model if the number of\n",
        "#             classes is altered\n",
        "#             2- detecting if fine-tuning is required\n",
        "\n",
        "#         Returns:\n",
        "#             :obj:`tuple`: a tuple containing the current best performing \\\n",
        "#             model(s) and a boolean indicating whether fine-tuning is required. \\\n",
        "#             It is guaranteed that if fine-tuning is required \\\n",
        "#             (i.e. `ret_tuple[1] == True`) is `True`, then the best performing \\\n",
        "#             model(s) will exist (i.e. `ret_tuple[0]` will not be `None`).\n",
        "#         \"\"\"\n",
        "\n",
        "#         assert self._current_task is not None, (\n",
        "#             'A task must be set prior to `fit()`. Use '\n",
        "#             '`optimizer.assign_task()` to set a task.'\n",
        "#         )\n",
        "\n",
        "#         task = self._current_task\n",
        "#         require_fine_tuning = False\n",
        "\n",
        "#         if self._best_candidate is not None:\n",
        "#             # Perform class/domain adaptation check\n",
        "#             for m_idx, m in enumerate(self._best_candidate.tasks_metadata):\n",
        "#                 if m.id == task.metadata.id:\n",
        "#                     # class adaptation check\n",
        "#                     if hasattr(task.metadata, 'classes') and \\\n",
        "#                     not set(task.metadata.classes).issubset(set(m.classes)):\n",
        "#                         # class-incremental learning\n",
        "#                         self._best_candidate.adapt_output(task.metadata.id,\n",
        "#                                                           len(task.classes))\n",
        "\n",
        "#                         # update task's metadata according to the new increment\n",
        "#                         # This assumes that\n",
        "#                         # `SegmentableImageFolder.accumulate_classes` is `True`\n",
        "#                         new_t = task.metadata\n",
        "#                         self._best_candidate.tasks_metadata[m_idx] = new_t\n",
        "\n",
        "#                         require_fine_tuning = True\n",
        "\n",
        "#                     # domain adaptation check\n",
        "#                     # if task.boundary.distance_from(m.boundary) > threshold:\n",
        "#                     #     require_fine_tuning = True\n",
        "#                     break\n",
        "\n",
        "#             # _best_candidate could be `None` on the first optimization itr.\n",
        "#             # if require_fine_tuning is `True`, the model is guaranteed to exist\n",
        "#             return self._best_candidate, require_fine_tuning\n",
        "\n",
        "#         return None, False\n",
        "\n",
        "\n",
        "#     def augment(self, base_model):\n",
        "#         \"\"\"\n",
        "#         \"\"\"\n",
        "\n",
        "#         pass\n",
        "\n"
      ],
      "metadata": {
        "id": "AjzgD5vEKU94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE60LvpQKlcr"
      },
      "source": [
        "## Evaluation Strategy\n",
        "\n",
        "<!---  \n",
        "$module=evaluation_strategy\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnZi5G2QmcPA"
      },
      "source": [
        "### Base XAI Interpreter\n",
        "\n",
        "<!---  \n",
        "$module=xai_interpreters\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=base_xai_interpreter.py\n",
        "-->"
      ],
      "metadata": {
        "id": "x9Z7Ra2ZKox5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N28FW6c3oKTq"
      },
      "outputs": [],
      "source": [
        "class BaseXAIInterpreter(abc.ABC):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def set_caching(self, false_pred_count, true_pred_count):\n",
        "\n",
        "        self.false_preds = false_pred_count\n",
        "        self.true_preds = true_pred_count\n",
        "\n",
        "        self.cached_false, self.cached_true = [], []\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def register_hooks(self, model):\n",
        "        \"\"\"\n",
        "        Register the layers' backward hooks.\n",
        "\n",
        "        Do not store gradients or memory-intensive attributes\n",
        "        during this phase, as the hooks are applied prior to training\n",
        "        (i.e. could result in storing all training gradients)\n",
        "\n",
        "        For a single pass/batch analysis, register the hooks during the\n",
        "        interpretation phase.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def interpret(self, model):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def cache_predictions(self, inputs, preds):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        assert hasattr(self, 'false_preds') and hasattr(self, 'true_preds'), (\n",
        "            '`set_caching()` must be called prior to caching predictions!'\n",
        "        )\n",
        "\n",
        "        false_count = self.false_preds\n",
        "        true_count = self.true_preds\n",
        "\n",
        "        # cache for interpretation\n",
        "        if false_count > 0 and len(self.cached_false) < false_count:\n",
        "            remaining = false_count - len(self.cached_false)\n",
        "            if remaining < 1:\n",
        "                return\n",
        "            # filter indices\n",
        "            all_false_preds = np.where(preds.cpu().numpy() == False)[0]\n",
        "            valid_indices = all_false_preds[:min(remaining,\n",
        "                                                 len(all_false_preds) - 1)]\n",
        "            self.cached_false.extend(inputs[valid_indices].cpu())\n",
        "\n",
        "        if true_count > 0 and len(self.cached_true) < true_count:\n",
        "            remaining = true_count - len(self.cached_true)\n",
        "            if remaining < 1:\n",
        "                return\n",
        "            # filter indices\n",
        "            all_true_preds = np.where(preds.cpu().numpy() == True)[0]\n",
        "            valid_indices = all_true_preds[:min(remaining,\n",
        "                                                len(all_true_preds) - 1)]\n",
        "            self.cached_true.extend(inputs[valid_indices].cpu())\n",
        "\n",
        "\n",
        "    def get_cached_predictions(self, shuffle=True):\n",
        "        # stack cached predictions along the first dimension\n",
        "        preds = torch.stack(self.cached_false + self.cached_true, dim=0)\n",
        "\n",
        "        if shuffle:\n",
        "            preds = preds[torch.randperm(preds.size(0))]\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def reset(self):\n",
        "        self.cached_false, self.cached_true = [], []\n",
        "\n",
        "\n",
        "    def normalize_scores(self, scores):\n",
        "        \"\"\"\n",
        "        Normalizing the scores to prevent exploding gradients/scores\n",
        "        Regularize by 1e-10 to prevent division by 0 or exploding values.\n",
        "\n",
        "        Modifies the scores dict in-place.\n",
        "\n",
        "        Args:\n",
        "            scores (:obj:`dict`): the scores dict, wherein the keys are module \\\n",
        "            IDs, and the values are the scores\n",
        "        \"\"\"\n",
        "\n",
        "        values = list(scores.values())\n",
        "\n",
        "        min_score = min(values)\n",
        "        max_score = max(values)\n",
        "\n",
        "        for key in scores:\n",
        "            scores[key] = (scores[key] - min_score) / \\\n",
        "                          (max_score - min_score + 1e-10)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        cls = self.__class__\n",
        "        assert hasattr(cls, 'NAME'), (\n",
        "            'XAI interpreters must have a static `NAME` attribute'\n",
        "        )\n",
        "\n",
        "        return cls.NAME\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huD3qA8xrEXI"
      },
      "source": [
        "#### Deep Taylor Decomposition\n",
        "\n",
        "<!---  \n",
        "$file=dtd.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDW-5EgnrL4q"
      },
      "outputs": [],
      "source": [
        "class DeepTaylorDecomposition(BaseXAIInterpreter):\n",
        "\n",
        "    NAME = 'DTD'\n",
        "\n",
        "    def __init__(self, false_pred_count, true_pred_count):\n",
        "        super().__init__()\n",
        "\n",
        "        self.set_caching(false_pred_count, true_pred_count)\n",
        "\n",
        "        self.gradients = {}\n",
        "        self.hooks = []\n",
        "\n",
        "\n",
        "    def register_hooks(self, model):\n",
        "        # hooks registered in the interpret() method for custom passes\n",
        "        # running DTD within the training process is too expensive\n",
        "        pass\n",
        "\n",
        "\n",
        "    def interpret(self, model, x):\n",
        "        model.eval()\n",
        "\n",
        "        relevance_scores = {}\n",
        "        intermediate_outputs = {}\n",
        "        back_hooks = []\n",
        "        forward_hooks = []\n",
        "\n",
        "        # layer hooks\n",
        "        def _backward_hook(module, grad_input, grad_output):\n",
        "            relevance_scores[module.id] = grad_output[0]\n",
        "\n",
        "        def _forward_hook(module, args, output):\n",
        "            intermediate_outputs[module.id] = output[0]\n",
        "\n",
        "        for op in model.nodes():\n",
        "            if op is None:\n",
        "                # omitted isolates\n",
        "                continue\n",
        "\n",
        "            hook_op = op\n",
        "\n",
        "            if isinstance(op, AlignConcat1x1):\n",
        "                # apply the hook to the prepended operation rather than the\n",
        "                # multiplexer\n",
        "                hook_op = op.prepend_to\n",
        "\n",
        "            if isinstance(hook_op, InputStem) \\\n",
        "            or isinstance(hook_op, OutputStem):\n",
        "                # Input/Output Stem are ignored, the relevance of the\n",
        "\n",
        "                # OutputStem is inherently 0 using DTD since we're calculating\n",
        "                # the gradients w.r.t the output, which throws off the\n",
        "                # normalization\n",
        "                continue\n",
        "\n",
        "            back_hook = hook_op.register_full_backward_hook(_backward_hook,\n",
        "                                                            prepend=False)\n",
        "            back_hooks.append(back_hook)\n",
        "            forward_hook = hook_op.register_forward_hook(_forward_hook,\n",
        "                                                         prepend=False)\n",
        "            forward_hooks.append(forward_hook)\n",
        "\n",
        "        # forward pass the passed data\n",
        "        logits = model(x)\n",
        "        pred_class = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # init gradient of predicted class score w.r.t. the input\n",
        "        out_grads = torch.zeros_like(logits)\n",
        "        out_grads[0, pred_class] = 1.0\n",
        "\n",
        "        # backward pass (triggers previously registered hooks)\n",
        "        x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        logits.backward(out_grads)\n",
        "\n",
        "        # calculate the DTD for each layer\n",
        "        for layer_id, grad in relevance_scores.items():\n",
        "            scores = grad * intermediate_outputs[layer_id]\n",
        "            # convert the scores tensor into a scalar score value\n",
        "            scalar = torch.mean(grad * intermediate_outputs[layer_id]).item()\n",
        "                     # scores.sum(dim=tuple(range(scores.dim())), keepdim=True)\n",
        "            relevance_scores[layer_id] = scalar\n",
        "\n",
        "        # normalize scores [omitted; no need, the scores are relative]\n",
        "        # self.normalize_scores(relevance_scores)\n",
        "\n",
        "        # cleanup\n",
        "        for hook in back_hooks + forward_hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        del intermediate_outputs\n",
        "        del back_hooks\n",
        "        del forward_hooks\n",
        "\n",
        "        return relevance_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQDm9IKhrdyZ"
      },
      "source": [
        "#### Layer-wise Relevance Propagation\n",
        "\n",
        "<!---  \n",
        "$file=lrp.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVo-3Ko0rggN"
      },
      "outputs": [],
      "source": [
        "class LRP(BaseXAIInterpreter):\n",
        "\n",
        "    NAME = 'LRP'\n",
        "\n",
        "    def __init__(self, false_pred_count, true_pred_count):\n",
        "        super().__init__()\n",
        "\n",
        "        self.set_caching(false_pred_count, true_pred_count)\n",
        "\n",
        "        self.gradients = {}\n",
        "        self.hooks = []\n",
        "\n",
        "\n",
        "    def register_hooks(self, model):\n",
        "        # hooks registered in the interpret() method for custom passes\n",
        "        # running DTD within the training process is too expensive\n",
        "        pass\n",
        "\n",
        "\n",
        "    def interpret(self, model, x):\n",
        "        model.eval()\n",
        "\n",
        "        relevance_scores = {}\n",
        "        intermediate_outputs = {}\n",
        "        back_hooks = []\n",
        "        forward_hooks = []\n",
        "\n",
        "        # layer hooks\n",
        "        def _backward_hook(module, grad_input, grad_output):\n",
        "            relevance_scores[module.id] = grad_output[0]\n",
        "\n",
        "        def _forward_hook(module, args, output):\n",
        "            intermediate_outputs[module.id] = output[0]\n",
        "\n",
        "        for op in model.nodes():\n",
        "            if op is None:\n",
        "                # omitted isolates\n",
        "                continue\n",
        "\n",
        "            hook_op = op\n",
        "\n",
        "            if isinstance(op, AlignConcat1x1):\n",
        "                # apply the hook to the prepended operation rather than the\n",
        "                # multiplexer\n",
        "                hook_op = op.prepend_to\n",
        "\n",
        "            if isinstance(hook_op, InputStem) \\\n",
        "            or isinstance(hook_op, OutputStem):\n",
        "                # Input/Output Stem are ignored, the relevance of the\n",
        "\n",
        "                # OutputStem is inherently 0 using DTD since we're calculating\n",
        "                # the gradients w.r.t the output, which throws off the\n",
        "                # normalization\n",
        "                continue\n",
        "\n",
        "            back_hook = hook_op.register_full_backward_hook(_backward_hook,\n",
        "                                                            prepend=False)\n",
        "            back_hooks.append(back_hook)\n",
        "            forward_hook = hook_op.register_forward_hook(_forward_hook,\n",
        "                                                         prepend=False)\n",
        "            forward_hooks.append(forward_hook)\n",
        "\n",
        "        # forward pass the passed data\n",
        "        logits = model(x)\n",
        "        pred_class = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # init gradient of predicted class score w.r.t. the input\n",
        "        out_grads = torch.zeros_like(logits)\n",
        "        out_grads[0, pred_class] = 1.0\n",
        "\n",
        "        # backward pass (triggers previously registered hooks)\n",
        "        x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        logits.backward(out_grads)\n",
        "\n",
        "        # calculate the DTD for each layer\n",
        "        for layer_id, grad in relevance_scores.items():\n",
        "            scores = grad * intermediate_outputs[layer_id]\n",
        "            # convert the scores tensor into a scalar score value\n",
        "            scalar = torch.mean(grad * intermediate_outputs[layer_id]).item()\n",
        "                     # scores.sum(dim=tuple(range(scores.dim())), keepdim=True)\n",
        "            relevance_scores[layer_id] = scalar\n",
        "\n",
        "        # normalize scores [omitted; no need, the scores are relative]\n",
        "        # self.normalize_scores(relevance_scores)\n",
        "\n",
        "        # cleanup\n",
        "        for hook in back_hooks + forward_hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        del intermediate_outputs\n",
        "        del back_hooks\n",
        "        del forward_hooks\n",
        "\n",
        "        return relevance_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjDS-iLNrk5-"
      },
      "source": [
        "#### Fisher Information\n",
        "\n",
        "<!---  \n",
        "$file=fi.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdEFMUxCrnCp"
      },
      "outputs": [],
      "source": [
        "class FisherInformation(BaseXAIInterpreter):\n",
        "\n",
        "    NAME = 'FI'\n",
        "\n",
        "\n",
        "    def __init__(self, false_pred_count, true_pred_count):\n",
        "        super().__init__()\n",
        "\n",
        "        self.set_caching(false_pred_count, true_pred_count)\n",
        "\n",
        "        self.gradients = {}\n",
        "        self.hooks = []\n",
        "\n",
        "\n",
        "    def register_hooks(self, model):\n",
        "        # hooks registered in the interpret() method for custom passes\n",
        "        # running DTD within the training process is too expensive\n",
        "        pass\n",
        "\n",
        "\n",
        "    def interpret(self, model, x):\n",
        "        model.eval()\n",
        "\n",
        "        relevance_scores = {}\n",
        "        intermediate_outputs = {}\n",
        "        back_hooks = []\n",
        "        forward_hooks = []\n",
        "\n",
        "        # layer hooks\n",
        "        def _backward_hook(module, grad_input, grad_output):\n",
        "            relevance_scores[module.id] = grad_output[0]\n",
        "\n",
        "        def _forward_hook(module, args, output):\n",
        "            intermediate_outputs[module.id] = output[0]\n",
        "\n",
        "        for op in model.nodes():\n",
        "            if op is None:\n",
        "                # omitted isolates\n",
        "                continue\n",
        "\n",
        "            hook_op = op\n",
        "\n",
        "            if isinstance(op, AlignConcat1x1):\n",
        "                # apply the hook to the prepended operation rather than the\n",
        "                # multiplexer\n",
        "                hook_op = op.prepend_to\n",
        "\n",
        "            if isinstance(hook_op, InputStem) \\\n",
        "            or isinstance(hook_op, OutputStem):\n",
        "                # Input/Output Stem are ignored, the relevance of the\n",
        "\n",
        "                # OutputStem is inherently 0 using DTD since we're calculating\n",
        "                # the gradients w.r.t the output, which throws off the\n",
        "                # normalization\n",
        "                continue\n",
        "\n",
        "            back_hook = hook_op.register_full_backward_hook(_backward_hook,\n",
        "                                                            prepend=False)\n",
        "            back_hooks.append(back_hook)\n",
        "            forward_hook = hook_op.register_forward_hook(_forward_hook,\n",
        "                                                         prepend=False)\n",
        "            forward_hooks.append(forward_hook)\n",
        "\n",
        "        # forward pass the passed data\n",
        "        logits = model(x)\n",
        "        pred_class = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # init gradient of predicted class score w.r.t. the input\n",
        "        out_grads = torch.zeros_like(logits)\n",
        "        out_grads[0, pred_class] = 1.0\n",
        "\n",
        "        # backward pass (triggers previously registered hooks)\n",
        "        x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        logits.backward(out_grads)\n",
        "\n",
        "        # calculate the DTD for each layer\n",
        "        for layer_id, grad in relevance_scores.items():\n",
        "            scores = grad * intermediate_outputs[layer_id]\n",
        "            # convert the scores tensor into a scalar score value\n",
        "            scalar = torch.mean(grad * intermediate_outputs[layer_id]).item()\n",
        "                     # scores.sum(dim=tuple(range(scores.dim())), keepdim=True)\n",
        "            relevance_scores[layer_id] = scalar\n",
        "\n",
        "        # normalize scores [omitted; no need, the scores are relative]\n",
        "        # self.normalize_scores(relevance_scores)\n",
        "\n",
        "        # cleanup\n",
        "        for hook in back_hooks + forward_hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        del intermediate_outputs\n",
        "        del back_hooks\n",
        "        del forward_hooks\n",
        "\n",
        "        return relevance_scores\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSI64362ymS"
      },
      "source": [
        "#### Weight Analysis\n",
        "\n",
        "<!---  \n",
        "$file=wa.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ6rNMOp24hs"
      },
      "outputs": [],
      "source": [
        "class WeightAnalysis(BaseXAIInterpreter):\n",
        "\n",
        "    NAME = 'WA'\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def register_hooks(self, model):\n",
        "        pass\n",
        "\n",
        "    def interpret(self):\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Space\n",
        "\n",
        "<!---  \n",
        "$module=embedding_space\n",
        "-->"
      ],
      "metadata": {
        "id": "fgFEneJD2TGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=embedding_space.py\n",
        "-->"
      ],
      "metadata": {
        "id": "5iiniH7VK9gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_space(dataset):\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=10000,\n",
        "                                         shuffle=False)\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = data_iter.next()\n",
        "    images = images.numpy()\n",
        "\n",
        "    input_shape = images.shape[1:]\n",
        "    input_size = np.prod(input_shape)\n",
        "\n",
        "    output_classes = len(set(labels.numpy()))\n",
        "\n",
        "    pixel_mean = np.mean(images)\n",
        "    pixel_std = np.std(images)\n",
        "    pixel_skewness = skew(images.flatten())\n",
        "    pixel_kurtosis = kurtosis(images.flatten())\n",
        "\n",
        "    color_channels = images.shape[1] if len(images.shape) > 3 else 1\n",
        "\n",
        "    pixel_entropy = entropy(np.histogram(images.flatten(), bins=256,\n",
        "                                         density=True)[0])\n",
        "\n",
        "    edges = np.mean([cv2.Canny(image.squeeze(), 100, 200).mean() \\\n",
        "                     for image in images])\n",
        "\n",
        "    hog_features = np.mean([hog(image.squeeze(), pixels_per_cell=(8, 8),\n",
        "                                cells_per_block=(2, 2),\n",
        "                                feature_vector=True).mean() \\\n",
        "                            for image in images])\n",
        "\n",
        "    avg_objects = np.mean([cv2.connectedComponents(cv2.threshold\\\n",
        "     (image.squeeze(), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1])[0] \\\n",
        "                           for image in images])\n",
        "\n",
        "    return [input_size, output_classes, pixel_mean, pixel_std, pixel_skewness,\n",
        "            pixel_kurtosis, color_channels, pixel_entropy, edges,\n",
        "            hog_features, avg_objects]\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                           transform=transform)\n",
        "mnist = datasets.MNIST(root='./data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "permuted_mnist = datasets.MNIST(root='./data', train=True, download=True,\n",
        "                                transform=transform)\n",
        "cifar100 = datasets.CIFAR100(root='./data', train=True, download=True,\n",
        "                             transform=transform)\n",
        "\n",
        "np.random.seed(0)\n",
        "perm = np.random.permutation(28*28)\n",
        "permuted_mnist.data = torch.from_numpy(permuted_mnist.data.numpy()\\\n",
        "                                       .reshape(-1, 28*28)[:, perm]\\\n",
        "                                       .reshape(-1, 28, 28))\n",
        "\n",
        "stats_cifar10 = calculate_statistics(cifar10)\n",
        "stats_mnist = calculate_statistics(mnist)\n",
        "stats_permuted_mnist = calculate_statistics(permuted_mnist)\n",
        "stats_cifar100 = calculate_statistics(cifar100)\n",
        "\n",
        "stats = {\n",
        "    \"CIFAR10\": stats_cifar10,\n",
        "    \"MNIST\": stats_mnist,\n",
        "    \"Permuted MNIST\": stats_permuted_mnist,\n",
        "    \"CIFAR100\": stats_cifar100\n",
        "}\n",
        "\n",
        "distances = {}\n",
        "datasets = list(stats.keys())\n",
        "for i in range(len(datasets)):\n",
        "    for j in range(i+1, len(datasets)):\n",
        "        dist = euclidean(stats[datasets[i]], stats[datasets[j]])\n",
        "        distances[f\"{datasets[i]} vs {datasets[j]}\"] = dist\n",
        "\n",
        "distances_df = pd.DataFrame.from_dict(distances, orient='index',\n",
        "                                      columns=['Distance'])\n"
      ],
      "metadata": {
        "id": "siRxi-iQ2Ynb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2D3eySOgBKd"
      },
      "source": [
        "<!---  \n",
        "$module=callbacks\n",
        "-->\n",
        "\n",
        "### Evaluation Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZcAruIRGVy"
      },
      "source": [
        "#### Adaptive Cutoff Threshold\n",
        "\n",
        "<!---  \n",
        "$file=act.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-ibiVqQRJmK"
      },
      "outputs": [],
      "source": [
        "class AdaptiveCutoffThreshold:\n",
        "    def __init__(self, patience=2, beta=0.01):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.beta = beta\n",
        "\n",
        "\n",
        "    def __call__(self, metrics, num_classes, *args, **kwargs):\n",
        "        if 'val_avg_acc' not in metrics:\n",
        "            return\n",
        "\n",
        "        score = metrics['val_avg_acc']\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta(num_classes):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "\n",
        "    def delta(self, num_classes):\n",
        "        return self.beta / np.log(num_classes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1-nJkrFifP"
      },
      "source": [
        "### Continual Evaluation Protocol\n",
        "\n",
        "<!---  \n",
        "$file=cl_evaluation_protocol.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKOIXagwFiL8"
      },
      "outputs": [],
      "source": [
        "class CLEvaluatorProtocol(abc.ABC):\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def evaluate(self, model, task):\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def fine_tune(self, model, task,\n",
        "                  output_layer_only=False, dir='./model_metrics/'):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kCTWwvGKrOj"
      },
      "source": [
        "### Base Evaluator\n",
        "\n",
        "<!---  \n",
        "$file=base_evaluator.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNo5T5vFKt0S"
      },
      "outputs": [],
      "source": [
        "class BaseEvaluator(abc.ABC):\n",
        "\n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 save_training_logs,\n",
        "                 verbose,\n",
        "                 xai_interpreter=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.save_training_logs = save_training_logs\n",
        "        self.verbose = verbose\n",
        "        self.xai = xai_interpreter\n",
        "        self.running_metrics = defaultdict(dict)\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    #   Abstract Methods (implement to conform to this base class)\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def optimize(self, model, task):\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdEQDRK5Kyhp"
      },
      "source": [
        "#### Image Classification Evaluator\n",
        "\n",
        "<!---  \n",
        "$file=image_cassification_evaluator.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BadyPrJHK7F3"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationEvaluator(BaseEvaluator, CLEvaluatorProtocol):\n",
        "\n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 save_training_logs,\n",
        "                 verbose,\n",
        "                 xai_interpreter=None):\n",
        "        super().__init__(device=device,\n",
        "                         save_training_logs=save_training_logs,\n",
        "                         verbose=verbose,\n",
        "                         xai_interpreter=xai_interpreter)\n",
        "\n",
        "\n",
        "    def _calculate_batch_metrics(self, split, loss,\n",
        "                                 outputs, labels, n_batches,\n",
        "                                 top_k=[]):\n",
        "        \"\"\"\n",
        "        TODO: add hook for batch/epoch metrics calculation to allow additional\n",
        "        metrics' calculations per task\n",
        "        \"\"\"\n",
        "\n",
        "        if split not in self.running_metrics:\n",
        "            # init running metrics\n",
        "            self.running_metrics[split]['running_loss'] = 0.0\n",
        "            self.running_metrics[split]['correct'] = 0\n",
        "            self.running_metrics[split]['samples'] = 0\n",
        "            self.running_metrics[split]['avg_acc'] = 0\n",
        "            self.running_metrics[split]['avg_loss'] = 0\n",
        "            self.running_metrics[split]['acc_conv_rates'] = []\n",
        "            self.running_metrics[split]['loss_conv_rates'] = []\n",
        "            if len(top_k) > 0:\n",
        "                # `avg_acc` == `top_1_acc`\n",
        "                self.running_metrics[split]['top_k'] = {\n",
        "                    k: (0.0, 0.0) for k in top_k\n",
        "                }\n",
        "\n",
        "        metrics = self.running_metrics[split]\n",
        "        metrics['running_loss'] += loss.item()\n",
        "\n",
        "        _, y_pred = torch.max(outputs, 1)\n",
        "        prev_acc = metrics['avg_acc']\n",
        "        prev_loss = metrics['avg_loss']\n",
        "\n",
        "        metrics['correct'] += (y_pred == labels).sum().item()\n",
        "        metrics['samples'] += labels.size(0)\n",
        "\n",
        "        metrics['avg_loss'] = metrics['running_loss'] / n_batches\n",
        "        metrics['avg_acc'] = metrics['correct'] / metrics['samples']\n",
        "\n",
        "        metrics['acc_conv_rates'].append(metrics['avg_acc'] - prev_acc)\n",
        "        metrics['loss_conv_rates'].append(metrics['avg_loss'] - prev_loss)\n",
        "\n",
        "        self.running_metrics[split] = metrics\n",
        "\n",
        "        if len(top_k) > 0:\n",
        "            for k in top_k:\n",
        "                # calculate top_k accuracy\n",
        "                _, pred = outputs.topk(k, 1, True, True)\n",
        "                crct, tot = metrics['top_k'][k]\n",
        "                crct += torch.eq(pred, labels.view(-1, 1)).sum().item()\n",
        "                tot += labels.size(0)\n",
        "                self.running_metrics[split]['top_k'][k] = (crct, tot)\n",
        "\n",
        "        return self.running_metrics[split]\n",
        "\n",
        "\n",
        "    def _calculate_epoch_metrics(self, reset_running_metrics):\n",
        "        res_dict = deepcopy(self.running_metrics)\n",
        "        for split in self.running_metrics:\n",
        "            top_k = {}\n",
        "            if 'top_k' in self.running_metrics[split]:\n",
        "                temp_top_k = self.running_metrics[split]['top_k']\n",
        "                for k in temp_top_k:\n",
        "                    top_k[k] = temp_top_k[k][0] / temp_top_k[k][1]\n",
        "\n",
        "                res_dict[split]['top_k'] = top_k\n",
        "\n",
        "            # acc/loss second derivatives (our custom convergence rates)\n",
        "            res_dict[split]['acc_conv_rate'] = self.__convergence_derivative(\n",
        "                res_dict[split]['acc_conv_rates'])\n",
        "            res_dict[split]['loss_conv_rate'] = self.__convergence_derivative(\n",
        "                res_dict[split]['loss_conv_rates'])\n",
        "\n",
        "            # delete recorded rates (too many unnecessary values; were only\n",
        "            # used to calculated the convergence derivative)\n",
        "            del res_dict[split]['acc_conv_rates']\n",
        "            del res_dict[split]['loss_conv_rates']\n",
        "\n",
        "        if reset_running_metrics:\n",
        "            self.running_metrics = defaultdict(dict)\n",
        "\n",
        "        return res_dict\n",
        "\n",
        "\n",
        "    def __convergence_derivative(self, convergence_rates):\n",
        "        sec_deriv = []      # the difference of means of adjacent pairs\n",
        "        for i in range(1, len(convergence_rates) - 1):\n",
        "            # (i-1 and i+1 to get the central difference for numerical\n",
        "            # differentiation)\n",
        "            rate_change = (convergence_rates[i+1] - convergence_rates[i-1]) / 2\n",
        "            sec_deriv.append(rate_change)\n",
        "\n",
        "        # we take the mean of the second derivative values to get a scalar value\n",
        "        if len(sec_deriv) > 0:\n",
        "            # this inequality will only fail if we have < 4 batches\n",
        "            # (3 batches => 2 approx. numerical 1st derivates => cannot get\n",
        "            # central difference for 2nd deriv.)\n",
        "            return sum(sec_deriv) / len(sec_deriv)\n",
        "\n",
        "        return 0\n",
        "\n",
        "\n",
        "    def _train(self, model, epoch, loader, optimizer, criterion):\n",
        "        model.train()\n",
        "\n",
        "        # training loop\n",
        "        n_batches = len(loader)\n",
        "        for batch_idx, (inputs, labels) in enumerate(loader):\n",
        "\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = None\n",
        "            try:\n",
        "                # forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # calculate/update metrics\n",
        "                batch_res = self._calculate_batch_metrics(split='train',\n",
        "                                                          loss=loss,\n",
        "                                                          outputs=outputs,\n",
        "                                                          labels=labels,\n",
        "                                                          n_batches=n_batches,\n",
        "                                                          top_k=[1, 5])\n",
        "\n",
        "                if self.verbose:\n",
        "                    TrainingLogger.log_training(epoch,\n",
        "                                                batch_idx,\n",
        "                                                batch_res['avg_loss'],\n",
        "                                                batch_res['avg_acc'])\n",
        "\n",
        "            except Exception as e:\n",
        "                Logger.debug(model.adj_matrix)\n",
        "                Logger.debug(str(model))\n",
        "                model.visualize()\n",
        "                raise e\n",
        "\n",
        "\n",
        "    def _validate(self, model, epoch, loader, criterion):\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # validation loop\n",
        "            n_batches = len(loader)\n",
        "            for batch_idx, (inputs, labels) in enumerate(loader):\n",
        "                # set tensor device\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # calculate loss\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # calculate/update metrics\n",
        "                batch_res = self._calculate_batch_metrics(split='val',\n",
        "                                                          loss=loss,\n",
        "                                                          outputs=outputs,\n",
        "                                                          labels=labels,\n",
        "                                                          n_batches=n_batches,\n",
        "                                                          top_k=[1, 5])\n",
        "\n",
        "                # pass samples to the interpreter for caching (if applicable)\n",
        "                if self.xai is not None:\n",
        "                    _, y_pred = torch.max(outputs, 1)\n",
        "                    self.xai.cache_predictions(inputs, (y_pred == labels))\n",
        "\n",
        "                if self.verbose:\n",
        "                    TrainingLogger.log_validation(epoch,\n",
        "                                                  batch_idx,\n",
        "                                                  batch_res['avg_loss'],\n",
        "                                                  batch_res['avg_acc'])\n",
        "\n",
        "\n",
        "    def optimize(self, model, task, fine_tune=False, dir='./model_metrics/'):\n",
        "        \"\"\"\n",
        "        Candidate optimization; this is the \"evaluation\" phase in a traditional\n",
        "        NAS cycle.\n",
        "\n",
        "        Args:\n",
        "            model (:class:`~Network`): the candidate model to be optimized (\\\n",
        "            in-place)\n",
        "            task (:class:`~BaseTask`): any :class:`~BaseTask` sub-\\\n",
        "            class, defining the optimization task/objectives\n",
        "            fine_tune (:obj:`bool`): defines whether the optimization \\\n",
        "            should fine-tune the model or fully optimize it.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.device.type != 'cpu':\n",
        "            # move model to GPU\n",
        "            model.cuda()\n",
        "\n",
        "        model.activate_task(task.id)\n",
        "\n",
        "        # loss function\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # candidate optimzer\n",
        "        # optimizer = optim.SGD(model.parameters(),\n",
        "        #                       lr=task.lr,\n",
        "        #                       momentum=0.9)\n",
        "        lr = task.learning_rate if not fine_tune else task.finetune_lr\n",
        "        optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n",
        "                                      model.parameters()),\n",
        "                               lr=lr)\n",
        "\n",
        "        # init loaders\n",
        "        train_loader, val_loader = task.loaders()\n",
        "\n",
        "        # training metrics\n",
        "        eval_metrics = EvaluationMetrics(task.metadata)\n",
        "\n",
        "        if self.verbose:\n",
        "            TrainingLogger.reset(task.id, task.version, task.name,\n",
        "                                 task.candidate_epochs,\n",
        "                                 len(train_loader), len(val_loader),\n",
        "                                 log_to_file=self.save_training_logs)\n",
        "\n",
        "        for epoch in range(task.candidate_epochs):\n",
        "            start_time = time.time()\n",
        "            early_stop = False\n",
        "\n",
        "            self._train(model, epoch, train_loader, optimizer, self.criterion)\n",
        "            self._validate(model, epoch, val_loader, self.criterion)\n",
        "\n",
        "            metrics = self._calculate_epoch_metrics(reset_running_metrics=True)\n",
        "            flat_metrics = flatten_dict(metrics, sep='_')\n",
        "\n",
        "            eval_metrics.add_metrics(metrics=flat_metrics,\n",
        "                                     epoch=epoch,\n",
        "                                     start_time=start_time)\n",
        "\n",
        "            if task.objective.target_threshold_met(eval_metrics):\n",
        "                break\n",
        "\n",
        "            for cb in task.callbacks:\n",
        "                cb(flat_metrics, task.num_classes, model)\n",
        "                if cb.early_stop:\n",
        "                    early_stop = True\n",
        "\n",
        "            if early_stop:\n",
        "                Logger.critical(f'Early stopping candidate...')\n",
        "                break\n",
        "\n",
        "        # TODO: move the XAI calculation and recording to a hook for flexibility\n",
        "        # init XAI data\n",
        "        xai_name, xai_scores = 'None', []\n",
        "        if self.xai is not None:\n",
        "            xai_scores = self.xai.interpret(model,\n",
        "                                            self.xai.get_cached_predictions()\\\n",
        "                                            .to(self.device))\n",
        "            xai_name = self.xai.name\n",
        "\n",
        "        Logger.progress(f'XAI: {xai_name} -- Scores {xai_scores}')\n",
        "\n",
        "        # commit candidate data\n",
        "        TrainingLogger.commit_file(filename=str(hash(model)) + '.txt')\n",
        "\n",
        "        model.metrics.add_eval_metrics(eval_metrics)\n",
        "        model.metrics.save(filename=str(hash(model)) + '.csv',\n",
        "                           dir=dir)\n",
        "\n",
        "\n",
        "    def evaluate(self, model, task, insert_metrics=True, epochs=1,\n",
        "                 dir='./model_metrics/'):\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        Args:\n",
        "            insert_metrics (:obj:`bool`): whether or not to insert \\\n",
        "            the evaluation metrics into the :obj:`~model.metrics`\n",
        "            epochs (:obj:`int`): number of validation epochs. This defaults \\\n",
        "            to `1` as the network is not being optimized, hence the \\\n",
        "            validation metrics are going to be static. Argument is added in \\\n",
        "            case a special case needs multiple epochs\n",
        "        \"\"\"\n",
        "\n",
        "        if self.device.type != 'cpu':\n",
        "            # move model to GPU\n",
        "            model.cuda()\n",
        "\n",
        "        model.activate_task(task.id)\n",
        "\n",
        "        # loss function\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # init validation loader\n",
        "        _, val_loader = task.loaders()\n",
        "\n",
        "        # reset logger\n",
        "        if self.verbose:\n",
        "            TrainingLogger.reset(task.id, task.version, task.name,\n",
        "                                 epochs,\n",
        "                                 0, len(val_loader),\n",
        "                                 log_to_file=self.save_training_logs)\n",
        "\n",
        "        # training metrics\n",
        "        eval_metrics = EvaluationMetrics(task.metadata)\n",
        "\n",
        "        Logger.info(\n",
        "            f'Evaluating model ({str(model.wl_hash)}) '\n",
        "            f'for task {task.id} v.{task.version} | {task.name}...'\n",
        "        )\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "\n",
        "            self._validate(model, epoch, val_loader, self.criterion)\n",
        "\n",
        "            metrics = self._calculate_epoch_metrics(reset_running_metrics=True)\n",
        "            flat_metrics = flatten_dict(metrics, sep='_')\n",
        "\n",
        "            eval_metrics.add_metrics(metrics=flat_metrics,\n",
        "                                     epoch=epoch,\n",
        "                                     start_time=start_time)\n",
        "\n",
        "        # commit candidate data if applicable\n",
        "        if insert_metrics:\n",
        "            model.metrics.add_eval_metrics(eval_metrics)\n",
        "            model.metrics.save(filename=str(hash(model)) + '.csv',\n",
        "                               dir=dir)\n",
        "\n",
        "        return eval_metrics\n",
        "\n",
        "\n",
        "    def fine_tune(self, model, task, output_layer_only=False,\n",
        "                  dir='./model_metrics/'):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        if output_layer_only:\n",
        "            # freeze all layers except the output layer; class-incremental\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "            # unfreeze output layer\n",
        "            for param in model.get_output_layer(task.id).parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        return self.optimize(model, task, fine_tune=True, dir=dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJYHj7fW6Td0"
      },
      "source": [
        "## Data Sources\n",
        "\n",
        "<!---  \n",
        "$module=data_source\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgXQAyGw-8jN"
      },
      "source": [
        "<!---  \n",
        "$module=preprocessing\n",
        "-->\n",
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=transforms.py\n",
        "-->"
      ],
      "metadata": {
        "id": "Esdiiq6SObub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KBQQlJ8--rF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SnapResize(transforms.Resize):\n",
        "    \"\"\"\n",
        "    Resizes a given img to the nearest power of 2\n",
        "    \"\"\"\n",
        "    def __call__(self, img):\n",
        "        width, height = img.size\n",
        "        new_width = 2 ** ((width - 1).bit_length())\n",
        "        new_height = 2 ** ((height - 1).bit_length())\n",
        "        return super(SnapResize, self).__call__((new_width, new_height))\n",
        "\n",
        "\n",
        "def cifar10_transforms(mean=(0.49139968, 0.48215841, 0.44653091),\n",
        "                        std=(0.24703223, 0.24348513, 0.26158784)):\n",
        "\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "\n",
        "def cifar100_transforms(mean=(0.50707516, 0.48654887, 0.44091784),\n",
        "                        std=(0.26733429, 0.25643846, 0.27615047)):\n",
        "\n",
        "\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "\n",
        "def mnist_transforms(mean=(0.1306604762738429),\n",
        "                        std=(0.30810780717887876)):\n",
        "\n",
        "\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "\n",
        "def imagenet_transforms(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]):\n",
        "    t_list = [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]\n",
        "\n",
        "    return transforms.Compose(t_list)\n",
        "\n",
        "\n",
        "def custom_images_transforms(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225]):\n",
        "    \"\"\"\n",
        "    Normalization defaults to ImageNet distribution as it is probable the\n",
        "    custom image dataset shares similarities with ImageNet given the\n",
        "    latter's sheer volume\n",
        "    \"\"\"\n",
        "    t_list = [\n",
        "        SnapResize(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]\n",
        "\n",
        "    return transforms.Compose(t_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQOEP1vrR-LU"
      },
      "source": [
        "### Vision DataSource\n",
        "\n",
        "<!---  \n",
        "$file=vision_datasource.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWAou-zPSMPZ"
      },
      "outputs": [],
      "source": [
        "class VisionDataSource:\n",
        "\n",
        "    class Dataset(Enum):\n",
        "        CIFAR10     = auto()\n",
        "        CIFAR100    = auto()\n",
        "        MNIST       = auto()\n",
        "        IMAGENET    = auto()\n",
        "        CUSTOM      = auto()\n",
        "\n",
        "    class SegmentableImageFolder(torchvision.datasets.ImageFolder):\n",
        "        def __init__(self, root, split, segment_size, segment_idx,\n",
        "                     transform=None, target_transform=None,\n",
        "                     loader=torchvision.datasets.folder.default_loader,\n",
        "                     is_valid_file=None, accumulate_classes=True):\n",
        "\n",
        "            assert split in ['train', 'val'], (\n",
        "                f'Invalid split {split} provided. Valid values are `train` '\n",
        "                'and `val`'\n",
        "            )\n",
        "\n",
        "            self.split = split\n",
        "            self.accumulate_classes = accumulate_classes\n",
        "            self.segment_start = segment_size * segment_idx\n",
        "            self.segment_end = self.segment_start + segment_size\n",
        "\n",
        "            split_root = os.path.join(root, split)\n",
        "\n",
        "            super(VisionDataSource.SegmentableImageFolder,\n",
        "                  self).__init__(split_root, transform, target_transform,\n",
        "                                 loader, is_valid_file)\n",
        "\n",
        "\n",
        "        def find_classes(self, directory):\n",
        "            def natural_sort(l):\n",
        "                convert = lambda text: int(text) \\\n",
        "                if text.isdigit() else text.lower()\n",
        "                alphanum_key = lambda key: [convert(c) \\\n",
        "                                            for c in re.split('([0-9]+)', key)]\n",
        "                return sorted(l, key=alphanum_key)\n",
        "\n",
        "            classes = natural_sort(entry.name \\\n",
        "                                   for entry in os.scandir(directory) \\\n",
        "                                   if entry.is_dir())\n",
        "            if not classes:\n",
        "                raise FileNotFoundError(f'Couldn\\'t find any class folder in ' +\n",
        "                                        f'{directory}.')\n",
        "\n",
        "            # filter classes\n",
        "            # classes = classes[self.segment_start:self.segment_end]\n",
        "            slice_start = 0 if self.accumulate_classes else self.segment_start\n",
        "            classes = classes[slice_start:self.segment_end]\n",
        "\n",
        "            # filter class_to_idx\n",
        "            class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "\n",
        "            return classes, class_to_idx\n",
        "\n",
        "\n",
        "    def __init__(self, path, dataset=None, transform=None, num_workers=4,\n",
        "                 autoload=False, train_data=None, val_data=None,\n",
        "                 segment_size=None, segment_idx=None):\n",
        "        assert dataset is None or isinstance(dataset,\n",
        "                                             VisionDataSource.Dataset), (\n",
        "            'The dataset provided is invalid! Make sure it '\n",
        "            'is of type `VisionDataSource.Dataset`'\n",
        "        )\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        self.num_workers = num_workers\n",
        "        self.train_data = train_data\n",
        "        self.val_data = val_data\n",
        "        self.segment_size = segment_size\n",
        "        self.segment_idx = segment_idx if segment_idx is not None else 0\n",
        "\n",
        "        if autoload:\n",
        "            self.load()\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # DATASET LOADERS\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"\n",
        "        Dataset loading\n",
        "        (downloads torchvision datasets if not previously downloaded)\n",
        "        \"\"\"\n",
        "\n",
        "        trns = self.transform\n",
        "        if self.dataset == VisionDataSource.Dataset.CIFAR10:\n",
        "            self.train_data = torchvision.datasets.CIFAR10(root=self.path,\n",
        "                                                           train=True,\n",
        "                                                           download=True,\n",
        "                                                           transform=trns)\n",
        "            self.val_data = torchvision.datasets.CIFAR10(root=self.path,\n",
        "                                                         train=False,\n",
        "                                                         download=True,\n",
        "                                                         transform=trns)\n",
        "\n",
        "        elif self.dataset == VisionDataSource.Dataset.CIFAR100:\n",
        "            self.train_data = torchvision.datasets.CIFAR100(root=self.path,\n",
        "                                                            train=True,\n",
        "                                                            download=True,\n",
        "                                                            transform=trns)\n",
        "            self.val_data = torchvision.datasets.CIFAR100(root=self.path,\n",
        "                                                          train=False,\n",
        "                                                          download=True,\n",
        "                                                          transform=trns)\n",
        "        elif self.dataset == VisionDataSource.Dataset.MNIST:\n",
        "            self.train_data = torchvision.datasets.MNIST(root=self.path,\n",
        "                                                         train=True,\n",
        "                                                         download=True,\n",
        "                                                         transform=trns)\n",
        "            self.val_data = torchvision.datasets.MNIST(root=self.path,\n",
        "                                                       train=False,\n",
        "                                                       download=True,\n",
        "                                                       transform=trns)\n",
        "        elif self.dataset == VisionDataSource.Dataset.IMAGENET:\n",
        "            self.train_data = torchvision.datasets.ImageNet(root=self.path,\n",
        "                                                            split='train',\n",
        "                                                            transform=trns)\n",
        "            self.val_data = torchvision.datasets.ImageNet(root=self.path,\n",
        "                                                          split='val',\n",
        "                                                          transform=trns)\n",
        "        elif self.dataset == VisionDataSource.Dataset.CUSTOM:\n",
        "            train_path = os.path.join(self.path, 'train')\n",
        "            val_path = os.path.join(self.path, 'val')\n",
        "            self.train_data = torchvision.dataset.ImageFolder(root=train_path,\n",
        "                                                              transform=trns)\n",
        "            self.val_data = torchvision.dataset.ImageFolder(root=val_path,\n",
        "                                                            transform=trns)\n",
        "\n",
        "        if self.segment_size is None:\n",
        "            self.segment_size = len(self.val_data.classes)\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # STATIC UTILITIES\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def class_segmentation_factory(path, dataset,\n",
        "                                   segment_size, segment_idx, transform):\n",
        "        trns = transform\n",
        "        ss, si = segment_size, segment_idx\n",
        "\n",
        "        train_data = VisionDataSource.SegmentableImageFolder(root=path,\n",
        "                                                             split='train',\n",
        "                                                             segment_size=ss,\n",
        "                                                             segment_idx=si,\n",
        "                                                             transform=trns)\n",
        "        val_data = VisionDataSource.SegmentableImageFolder(root=path,\n",
        "                                                           split='val',\n",
        "                                                           segment_size=ss,\n",
        "                                                           segment_idx=si,\n",
        "                                                           transform=trns)\n",
        "\n",
        "        return VisionDataSource(path, dataset=dataset, transform=transform,\n",
        "                                train_data=train_data, val_data=val_data,\n",
        "                                segment_size=ss, segment_idx=si)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def __convert_dataset_to_img(dataset, root_dir, split='train', ext='png'):\n",
        "\n",
        "        Logger.info(f'Converting compressed dataset to {ext} format')\n",
        "        Logger.info('This may take a while...')\n",
        "\n",
        "        # intentionally not joining the root_dir with `Config.BASE_PATH`\n",
        "        # saving locally on Colab instead to avoid clutter on GDrive when\n",
        "        # applicable\n",
        "        full_path = os.path.join(root_dir, split)\n",
        "\n",
        "        for idx, (image, label) in enumerate(dataset):\n",
        "            label_dir = os.path.join(full_path, str(label))\n",
        "            os.makedirs(label_dir, exist_ok=True)\n",
        "            image_path = os.path.join(label_dir, f'{idx}.{ext}')\n",
        "            if not isinstance(image, PIL.Image.Image):\n",
        "                image = torchvision.transforms.ToPILImage()(image)\n",
        "            image.save(image_path)\n",
        "\n",
        "        Logger.info(f'Successfully converted dataset! Saved to `{full_path}`')\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def __torchvision_downloader(cls, path, convert_to_img):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        train_data = cls(root=path, train=True, download=True)\n",
        "        val_data = cls(root=path, train=False, download=True)\n",
        "\n",
        "        if convert_to_img:\n",
        "            # convert dataset to image format and save for consistency\n",
        "            # to allow segmentation and consistent handling later\n",
        "\n",
        "            # delete downloaded compressed dataset\n",
        "            recursive_dir_delete(path)\n",
        "\n",
        "            # save segmentable dataset\n",
        "            VisionDataSource.__convert_dataset_to_img(train_data,\n",
        "                                                      path,\n",
        "                                                      split='train')\n",
        "            VisionDataSource.__convert_dataset_to_img(val_data,\n",
        "                                                      path,\n",
        "                                                      split='val')\n",
        "\n",
        "    @staticmethod\n",
        "    def download_cifar10(path, force_overwrite=False,\n",
        "                         allow_segmentation=False):\n",
        "        \"\"\"\n",
        "        allow_segmentation is an inefficient implementation as it converts each\n",
        "        sample to an image from its original compressed format (higher space\n",
        "        and time complexity)\n",
        "        \"\"\"\n",
        "        if not should_overwrite_path(path, force_overwrite):\n",
        "            return\n",
        "\n",
        "        # download CIFAR-10 dataset\n",
        "        VisionDataSource.__torchvision_downloader(\n",
        "            torchvision.datasets.CIFAR10, path, allow_segmentation\n",
        "        )\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def download_cifar100(path, force_overwrite=False,\n",
        "                          allow_segmentation=False):\n",
        "        \"\"\"\n",
        "        allow_segmentation is an inefficient implementation as it converts each\n",
        "        sample to an image from its original compressed format (higher space\n",
        "        and time complexity)\n",
        "        \"\"\"\n",
        "        if not should_overwrite_path(path, force_overwrite):\n",
        "            return\n",
        "\n",
        "        # download CIFAR-10 dataset\n",
        "        VisionDataSource.__torchvision_downloader(\n",
        "            torchvision.datasets.CIFAR100, path, allow_segmentation\n",
        "        )\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def download_mnist(path, force_overwrite=False, allow_segmentation=False):\n",
        "        \"\"\"\n",
        "        allow_segmentation is an inefficient implementation as it converts each\n",
        "        sample to an image from its original compressed format (higher space\n",
        "        and time complexity)\n",
        "        \"\"\"\n",
        "        if not should_overwrite_path(path, force_overwrite):\n",
        "            return\n",
        "\n",
        "        # download MNIST dataset\n",
        "        VisionDataSource.__torchvision_downloader(\n",
        "            torchvision.datasets.MNIST, path, allow_segmentation\n",
        "        )\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_data_distribution(path, transform=[], batch_size=256):\n",
        "        \"\"\"\n",
        "        Iterates through the dataset to calculates the mean/std, and\n",
        "        infer the channel dimensions.\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            Logger.warning(f'The given path \"{path}\" does not exist!')\n",
        "            return\n",
        "\n",
        "        # load dataset\n",
        "        t_comp = transforms.Compose(transform + [transforms.ToTensor()])\n",
        "        dataset = torchvision.datasets.ImageFolder(root=path,\n",
        "                                                   transform=t_comp)\n",
        "\n",
        "        Logger.progress('Computing mean and standard deviation for the dataset')\n",
        "        Logger.progress('This may take a while...')\n",
        "\n",
        "        # temporary DataLoader required to loop through the dataset\n",
        "        loader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=2)\n",
        "\n",
        "        # var[X] = E[X**2] - E[X]**2\n",
        "        channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
        "\n",
        "        for batch in loader:\n",
        "            images, labels = batch\n",
        "            # (B, C, H, W)\n",
        "            channels_sum += torch.mean(images, dim=[0, 2, 3])\n",
        "            channels_sqrd_sum += torch.mean(images ** 2, dim=[0, 2, 3])\n",
        "            num_batches += 1\n",
        "\n",
        "            # if self.channels is None:\n",
        "            #     self.channels = images[0].shape[1]\n",
        "\n",
        "        mean = channels_sum / num_batches\n",
        "        std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
        "\n",
        "        Logger.progress(f'Computed mean: {mean}, std: {std}')\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # UTILITIES\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        params = ['path', 'segment_size', 'segment_idx', 'num_workers']\n",
        "\n",
        "        transforms = [str(t) for t in self.transform.transforms]\n",
        "        dataset = ''\n",
        "        if self.dataset is not None:\n",
        "            dataset = self.dataset.name\n",
        "\n",
        "        return DataSourceMetadata(path=self.path,\n",
        "                                  segment_size=self.segment_size,\n",
        "                                  segment_idx=self.segment_idx,\n",
        "                                  num_workers=self.num_workers,\n",
        "                                  transforms=transforms,\n",
        "                                  dataset=dataset)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # OPERATORS\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.metadata)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.metadata)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, VisionDataSource):\n",
        "            return False\n",
        "\n",
        "        return self.metadata == other.metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives\n",
        "\n",
        "<!---  \n",
        "$module=objective\n",
        "-->"
      ],
      "metadata": {
        "id": "O6z_5zfsw4Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=base_objective.py\n",
        "-->"
      ],
      "metadata": {
        "id": "QomutldoPF_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseObjective(abc.ABC):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        assert hasattr(self.__class__, '_DEFAULTS'), (\n",
        "            '`BaseObjective` subclass must define a static `_DEFAULTS` dict!'\n",
        "        )\n",
        "\n",
        "        # objectives' criteria\n",
        "        self._objs = {}\n",
        "\n",
        "\n",
        "    def add_criterion(self, metric,\n",
        "                      polarity=None, score_weight=None,\n",
        "                      thresholds_enabled=None,\n",
        "                      min_threshold=None, target_threshold=None):\n",
        "\n",
        "        cls = self.__class__\n",
        "        assert metric in cls._DEFAULTS, (\n",
        "            'An invalid `metric` was provided. Please make sure that the '\n",
        "            'passed metric exists in the `_DEFAULTS` dict.'\n",
        "        )\n",
        "\n",
        "        defaults = cls._DEFAULTS[metric]\n",
        "\n",
        "        key = defaults['key']\n",
        "\n",
        "        # do not use `or` for defaulting as some of these values are false-like\n",
        "        obj_pol = defaults['polarity'] if polarity is None else polarity\n",
        "\n",
        "        weight = defaults['weight'] if score_weight is None else score_weight\n",
        "\n",
        "        t_enabled = defaults['thresholds_enabled'] \\\n",
        "        if thresholds_enabled is None else thresholds_enabled\n",
        "\n",
        "        min_t = defaults['min_threshold'] if min_threshold is None \\\n",
        "        else min_threshold\n",
        "\n",
        "        target_t = defaults['target_threshold'] if target_threshold is None \\\n",
        "        else target_threshold\n",
        "\n",
        "        self._objs[metric] = {\n",
        "            'key': key,\n",
        "            'polarity': obj_pol,\n",
        "            'weight': weight,\n",
        "            'thresholds_enabled': t_enabled,\n",
        "            'min_threshold': min_t,\n",
        "            'target_threshold': target_t\n",
        "        }\n",
        "\n",
        "\n",
        "    def min_threshold_met(self, eval_metrics):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            eval_metrics (:class:`~EvaluationMetrics`): the records' list to \\\n",
        "            assess the threshold over\n",
        "\n",
        "        Returns:\n",
        "            :obj:`bool`: whether or not all applicable minimum thresholds were \\\n",
        "            reached. If no threshold is enabled, the function returns `True`\n",
        "        \"\"\"\n",
        "\n",
        "        # check against idx -1; last recorded epoch\n",
        "        agg_metrics = eval_metrics.aggregate()[-1]\n",
        "\n",
        "        for criterion in self._objs.values():\n",
        "            key = criterion['key']\n",
        "\n",
        "            if not criterion['thresholds_enabled']:\n",
        "                # skip criterion\n",
        "                continue\n",
        "\n",
        "            if key not in agg_metrics:\n",
        "                # skip criterion\n",
        "                continue\n",
        "\n",
        "            if criterion['polarity'] < 0:\n",
        "                # minimization problem\n",
        "                if agg_metrics[key] < criterion['min_threshold']:\n",
        "                    return False\n",
        "            else:\n",
        "                # maximization problem\n",
        "                if agg_metrics[key] > criterion['min_threshold']:\n",
        "                    return False\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def target_threshold_met(self, eval_metrics):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            eval_metrics (:class:`~EvaluationMetrics`): the records' list to \\\n",
        "            assess the threshold over\n",
        "\n",
        "        Returns:\n",
        "            :obj:`bool`: whether or not all applicable target thresholds were \\\n",
        "            reached. If no threshold is enabled, the function returns `False`\n",
        "        \"\"\"\n",
        "\n",
        "        # check against idx -1; last recorded epoch\n",
        "        agg_metrics = eval_metrics.aggregate()[-1]\n",
        "        any_assessed = False\n",
        "\n",
        "        for criterion in self._objs.values():\n",
        "            key = criterion['key']\n",
        "\n",
        "            if not criterion['thresholds_enabled']:\n",
        "                # skip criterion\n",
        "                continue\n",
        "\n",
        "            if key not in agg_metrics:\n",
        "                # skip criterion\n",
        "                continue\n",
        "\n",
        "            any_assessed = True   # at least one threshold assessed\n",
        "\n",
        "            if criterion['polarity'] < 0:\n",
        "                # minimization problem\n",
        "                if agg_metrics[key] < criterion['target_threshold']:\n",
        "                    return False\n",
        "            else:\n",
        "                # maximization problem\n",
        "                if agg_metrics[key] > criterion['target_threshold']:\n",
        "                    return False\n",
        "\n",
        "        return any_assessed\n",
        "\n",
        "\n",
        "    @property\n",
        "    def score_weights(self):\n",
        "        return {o['key']: o['weight'] * o['polarity'] \\\n",
        "                for o in self._objs.values()}\n",
        "\n",
        "    @property\n",
        "    def metadata_list(self):\n",
        "        ret_list = [ObjectiveMetadata(o_type=type(self).__name__,\n",
        "                                      metric_key=o['key'],\n",
        "                                      polarity=o['polarity'],\n",
        "                                      score_weight=o['weight'],\n",
        "                                      thresholds_enabled=\\\n",
        "                                      o['thresholds_enabled'],\n",
        "                                      min_threshold=o['min_threshold'],\n",
        "                                      target_threshold=o['target_threshold']) \\\n",
        "                    for o in self._objs.values()]\n",
        "        return ret_list\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.metadata_list)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        \"\"\"\n",
        "        Using __eq__ is much faster than comparing __hash__ values\n",
        "        (dependent on __str__ values of datasource + search_space which\n",
        "        potentially encapsulates a lot of operations)\n",
        "        \"\"\"\n",
        "        if not isinstance(other, self.__class__):\n",
        "            return False\n",
        "\n",
        "        return self.metadata_list == other.metadata_list\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.metadata_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "lNfWBOujxETU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Classification Objective\n",
        "\n",
        "<!---  \n",
        "$file=image_classification_objective.py\n",
        "-->"
      ],
      "metadata": {
        "id": "V8SV_GSXw9Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ICObjective(BaseObjective):\n",
        "    \"\"\"\n",
        "    Image Classification Thresholds\n",
        "    \"\"\"\n",
        "\n",
        "    class Metric(Enum):\n",
        "        # Validation Accuracy\n",
        "        VAL_ACC                 = auto()\n",
        "        # Parameter Count\n",
        "        MODEL_SIZE              = auto()\n",
        "        # MFLOPs\n",
        "        COMP_PERF               = auto()\n",
        "        # Training Accuracy Convergence Rate\n",
        "        # (second derivative of train. acc.)\n",
        "        TRAIN_ACC_CONV          = auto()\n",
        "        # Training Loss Convergence Rate\n",
        "        # (second derivative of train. loss)\n",
        "        TRAIN_LOSS_CONV          = auto()\n",
        "\n",
        "\n",
        "    _DEFAULTS = {\n",
        "        Metric.VAL_ACC: {\n",
        "            'key': 'val_avg_acc',\n",
        "            'polarity': 1,\n",
        "            'weight': 0.8,\n",
        "            # disable thresholds by default\n",
        "            'thresholds_enabled': False,\n",
        "            'min_threshold': 0.0,\n",
        "            'target_threshold': 1.0\n",
        "        },\n",
        "        Metric.MODEL_SIZE: {\n",
        "            'key': 'total_params',\n",
        "            'polarity': -1,\n",
        "            'weight': 0.1,\n",
        "            # disable thresholds by default\n",
        "            'thresholds_enabled': False,\n",
        "            'min_threshold': np.inf,\n",
        "            'target_threshold': 0.0\n",
        "        },\n",
        "        Metric.COMP_PERF: {\n",
        "            'key': 'mflops',\n",
        "            'polarity': -1,\n",
        "            'weight': 0.1,\n",
        "            # disable thresholds by default\n",
        "            'thresholds_enabled': False,\n",
        "            'min_threshold': np.inf,\n",
        "            'target_threshold': 0.0\n",
        "        },\n",
        "        Metric.TRAIN_ACC_CONV: {\n",
        "            # this value is typically negative\n",
        "            # (training accuracy typically decelerates)\n",
        "            'key': 'train_acc_conv_rate',\n",
        "            'polarity': 1,\n",
        "            'weight': 0.2,\n",
        "            # disable thresholds by default\n",
        "            'thresholds_enabled': False,\n",
        "            'min_threshold': -1.0,\n",
        "            'target_threshold': 1.0\n",
        "        },\n",
        "        Metric.TRAIN_LOSS_CONV: {\n",
        "            # this value is also typically negative\n",
        "            # (training loss typically decelerates)\n",
        "            'key': 'train_loss_conv_rate',\n",
        "            'polarity': -1,\n",
        "            'weight': 0.1,\n",
        "            # disable thresholds by default\n",
        "            'thresholds_enabled': False,\n",
        "            'min_threshold': -1.0,\n",
        "            'target_threshold': 1.0\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ICObjective, self).__init__()\n",
        "\n"
      ],
      "metadata": {
        "id": "Td_zNLdKw8n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3SJVEJ4ONyd"
      },
      "source": [
        "## Task\n",
        "\n",
        "<!---  \n",
        "$module=task\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!---  \n",
        "$file=base_task.py\n",
        "-->"
      ],
      "metadata": {
        "id": "u0Ac3le_PcEt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xicTqkn0OQfG"
      },
      "outputs": [],
      "source": [
        "class BaseTask(abc.ABC):\n",
        "\n",
        "    def __init__(self, id, version, name, datasource, search_space,\n",
        "                 scoring_func, importance_weight, objective, finetune_lr):\n",
        "        self.id = id\n",
        "        self.version = version\n",
        "        self.name = name\n",
        "        self.modality = self.__class__._MODALITY\n",
        "        self.datasource = datasource\n",
        "        self.search_space = search_space\n",
        "        self.scoring_func = scoring_func\n",
        "        self.importance_wight = importance_weight\n",
        "        self.objective = objective\n",
        "        self.finetune_lr = finetune_lr\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def loaders(self):\n",
        "        raise NotImplementedError('Abstract method was not implemented')\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return TaskMetadata(t_type=type(self).__name__,\n",
        "                            id=self.id,\n",
        "                            version=self.version,\n",
        "                            name=self.name,\n",
        "                            modality=self.modality,\n",
        "                            objectives_metadata=self.objective.metadata_list,\n",
        "                            search_space_metadata=self.search_space.metadata,\n",
        "                            datasource_metadata=self.datasource.metadata)\n",
        "\n",
        "    def save(self, dir='./tasks/', filename=None):\n",
        "\n",
        "        def_filename = f'task_{self.id}v.{self.version}-{self.name}.json'\n",
        "\n",
        "        self.metadata.save(dir=dir,\n",
        "                           filename=filename if filename else def_filename)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.metadata)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        \"\"\"\n",
        "        Using __eq__ is much faster than comparing __hash__ values\n",
        "        (dependent on __str__ values of datasource + search_space which\n",
        "        potentially encapsulates a lot of operations)\n",
        "        \"\"\"\n",
        "        if not isinstance(other, self.__class__):\n",
        "            return False\n",
        "\n",
        "        return self.metadata == other.metadata\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.metadata)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXZTd13dRzw0"
      },
      "source": [
        "### Vision Task\n",
        "\n",
        "<!---  \n",
        "$file=vision_task.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "vxUsC0wsR4cz"
      },
      "outputs": [],
      "source": [
        "class VisionTask(BaseTask):\n",
        "    \"\"\"\n",
        "    Data structure containing all task-related attributes, including the\n",
        "    dataset and the search space. This allows for task-specific search spaces\n",
        "    \"\"\"\n",
        "\n",
        "    _MODALITY = 'Image Classification'\n",
        "\n",
        "    def __init__(self, id, version, name, datasource, search_space,\n",
        "                 train_batch_size=128, val_batch_size=128,\n",
        "                 nas_epochs=10, candidate_epochs=10, callbacks=[],\n",
        "                 learning_rate=0.001, finetune_lr=0.0003, scoring_func=None,\n",
        "                 importance_weight=1.0, objective=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset (:class:`~VisionDataSource`): the dataset used to evaluate \\\n",
        "            the candidates\n",
        "            root_path (:obj:`str`): the source path of data. \\\n",
        "            If this argument is not provided for other data sources, they are \\\n",
        "            downloaded through :class:`torchvision`\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(datasource, VisionDataSource), (\n",
        "            'Invalid dataset provided. Please ensure the data source is of '\n",
        "            'type `VisionDataSource`'\n",
        "        )\n",
        "\n",
        "        assert datasource.train_data is not None, (\n",
        "            'Dataset was not loaded. Please load the Data prior to '\n",
        "            'initializing the task object'\n",
        "        )\n",
        "\n",
        "        # defaults\n",
        "        sc_func = scoring_func or default_img_classification_scoring\n",
        "        obj = objective\n",
        "        if objective is None:\n",
        "            obj = ICObjective()\n",
        "            # default criterion\n",
        "            obj.add_criterion(metric=ICObjective.Metric.VAL_ACC,\n",
        "                              thresholds_enabled=False)\n",
        "\n",
        "        super(VisionTask, self).__init__(id=id,\n",
        "                                         version=version,\n",
        "                                         name=name,\n",
        "                                         datasource=datasource,\n",
        "                                         search_space=search_space,\n",
        "                                         scoring_func=sc_func,\n",
        "                                         importance_weight=importance_weight,\n",
        "                                         objective=obj,\n",
        "                                         finetune_lr=finetune_lr)\n",
        "\n",
        "        self.datasource = datasource\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.val_batch_size = val_batch_size\n",
        "        self.nas_epochs = nas_epochs\n",
        "        self.candidate_epochs = candidate_epochs\n",
        "        self.callbacks = callbacks\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.classes = self.datasource.val_data.classes\n",
        "        self.num_classes = len(self.classes)\n",
        "\n",
        "\n",
        "\n",
        "    # def __filter_classes(self, train_data, val_data, ratio):\n",
        "    #     num_classes = len(set(train_data.targets))\n",
        "    #     filter_count = int(num_classes * ratio)\n",
        "    #     selected_indices = torch.randperm(num_classes)[:filter_count]\n",
        "\n",
        "    #     # filter train/val subsets\n",
        "    #     train_indices = []\n",
        "    #     for i, label in enumerate(train_data.targets):\n",
        "    #         if label in selected_indices:\n",
        "    #             train_indices.append(i)\n",
        "    #     train_data = Subset(train_data, train_indices)   # in-place\n",
        "\n",
        "    #     val_indices = []\n",
        "    #     for i, label in enumerate(val_data.targets):\n",
        "    #         if label in selected_indices:\n",
        "    #             val_indices.append(i)\n",
        "    #     val_data = Subset(val_data, val_indices)         # in-place\n",
        "\n",
        "\n",
        "    # def normalize_data(self):\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     TODO: I'm not a big fan of re-initializing the entire dataset.\n",
        "    #     Computing the mean/std requires the intialization of a `DataLoader`\n",
        "    #     with the given transforms (as some of which may be resizing the inputs\n",
        "    #     which affects the mean/std), looping through the dataset to calculate\n",
        "    #     the expectation/variance -> std, then using the computed values to\n",
        "    #     re-init the dataset with the normalization transform.\n",
        "    #     Look into ways to compute the mean and std with less overhead and\n",
        "    #     without knowing the values a priori (for domain-agnostic purposes).\n",
        "\n",
        "    #     \"\"\"\n",
        "    #     assert hasattr(self, 'mean') and hasattr(self, 'std'), (\n",
        "    #         '`mean` and `std` were not computed! '\n",
        "    #         'Ensure that `__compute_data_attributes` is called'\n",
        "    #     )\n",
        "\n",
        "    #     self.transforms = transforms.Compose([self.train_data.transform,\n",
        "    #                                           transforms.Normalize(\n",
        "    #                                               mean=self.mean,\n",
        "    #                                               std=self.std\n",
        "    #                                           )])\n",
        "\n",
        "    #     Logger.info('Reinitializing data with normalization transforms...')\n",
        "\n",
        "    #     # re-init dataset with new transforms\n",
        "    #     self.__init_dataset()\n",
        "\n",
        "\n",
        "    def loaders(self):\n",
        "        train_loader = DataLoader(self.datasource.train_data,\n",
        "                                  batch_size=self.train_batch_size,\n",
        "                                  shuffle=True)\n",
        "        val_loader = DataLoader(self.datasource.val_data,\n",
        "                                batch_size=self.val_batch_size,\n",
        "                                shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "\n",
        "    ## @staticmethod\n",
        "    # def class_segmentation_factory(task, segment_size):\n",
        "    #     \"\"\"\n",
        "    #     Factory method to segment a given task into multiple class-wise\n",
        "    #     tasks. Although the resulting segments are distinct `Task` objects,\n",
        "    #     they all share the same name/id.\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     if task.num_classes % segment_size != 0:\n",
        "    #         Logger.warning((\n",
        "    #             'Number of classes is not divisible by the provided ',\n",
        "    #             'segmentation count. Flooring the value... ',\n",
        "    #             f'{task.num_classes % segment_size} classes will not be ',\n",
        "    #             'considered!\\n'\n",
        "    #         ))\n",
        "\n",
        "    #     seg_range = range(task.num_classes // segment_size)\n",
        "\n",
        "    #     train_data, val_data = task.train_data, task.val_data\n",
        "\n",
        "    #     # temp delete loaded dataset to avoid deep-copying them\n",
        "    #     del task.train_data\n",
        "    #     del task.val_data\n",
        "\n",
        "    #     task_list = []\n",
        "    #     inc_id = task.id + 1\n",
        "    #     for segment_idx in seg_range:\n",
        "    #         # train_classes only include the segment's classes\n",
        "    #         train_classes = list(range(segment_idx * segment_size,\n",
        "    #                                    (segment_idx + 1) * segment_size))\n",
        "    #         # val_classes include all segments' classes up to segment_idx\n",
        "    #         val_classes = list(range((segment_idx + 1) * segment_size))\n",
        "\n",
        "    #         seg_task = deepcopy(task)\n",
        "\n",
        "    #         seg_task.num_classes = val_classes\n",
        "    #         seg_task.id = inc_id\n",
        "    #         inc_id += 1\n",
        "\n",
        "    #         seg_task.train_data = Subset(train_data,\n",
        "    #                 [idx for idx in range(len(train_data)) \\\n",
        "    #                 if train_data.targets[idx] in train_classes]\n",
        "    #         )\n",
        "\n",
        "    #         seg_task.val_data = Subset(val_data,\n",
        "    #                 [idx for idx in range(len(val_data)) \\\n",
        "    #                 if val_data.targets[idx] in val_classes]\n",
        "    #         )\n",
        "\n",
        "    #         task_list.append(seg_task)\n",
        "\n",
        "    #     del train_data\n",
        "    #     del val_data\n",
        "    #     gc.collect()\n",
        "\n",
        "    #     return task_list\n",
        "\n",
        "    @property\n",
        "    def shapes(self):\n",
        "        \"\"\"\n",
        "        Gets the input shape (`tuple(batch_size, channels, height, width)`) and\n",
        "        the output shape (`tuple(batch_size, num_classes)`)\n",
        "\n",
        "        Returns:\n",
        "            tuple: ( `(int:batch_size, int:channels, int:height, int:width),\n",
        "                      (int:batch_size, int:num_classes)` )\n",
        "        \"\"\"\n",
        "\n",
        "        def get_spatial_dimensions():\n",
        "            # [Deprecated]: transforms are now applied during initialization\n",
        "            # if hasattr(self, 'transforms') and self.transforms is not None:\n",
        "            #     for t in self.transforms.transforms:\n",
        "            #         if isinstance(t, transforms.Resize):\n",
        "            #             return (self.channels, *t.size)\n",
        "\n",
        "            # self.train_data is a list of tuples (tensor, label int)\n",
        "            return self.datasource.train_data[0][0].size()\n",
        "\n",
        "        return (\n",
        "            (self.train_batch_size, *(get_spatial_dimensions())),\n",
        "            (self.train_batch_size, self.num_classes)\n",
        "         )\n",
        "\n",
        "    ## @property\n",
        "    # def distributions(self):\n",
        "    #     \"\"\"\n",
        "    #     \"\"\"\n",
        "    #     assert hasattr(self, 'train_dist') and hasattr(self, 'val_dist'), (\n",
        "    #         '`__compute_data_attributes()` needs to be called to initialize '\n",
        "    #         '`self.train_dist` and `self.val_dist`'\n",
        "    #     )\n",
        "\n",
        "    #     return (self.train_dist, self.val_dist)\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        ss_metadata = self.search_space.metadata\n",
        "        ds_metadata = self.datasource.metadata\n",
        "        obj_metadata = self.objective.metadata_list\n",
        "        shapes = self.shapes\n",
        "        return VisionTaskMetadata(t_type=type(self).__name__,\n",
        "                                  id=self.id,\n",
        "                                  version=self.version,\n",
        "                                  name=self.name,\n",
        "                                  modality=self.modality,\n",
        "                                  objectives_metadata=obj_metadata,\n",
        "                                  search_space_metadata=ss_metadata,\n",
        "                                  datasource_metadata=ds_metadata,\n",
        "                                  train_batch_size=self.train_batch_size,\n",
        "                                  val_batch_size=self.val_batch_size,\n",
        "                                  learning_rate=self.learning_rate,\n",
        "                                  nas_epochs=self.nas_epochs,\n",
        "                                  candidate_epochs=self.candidate_epochs,\n",
        "                                  in_shape=shapes[0],\n",
        "                                  out_shape=shapes[1],\n",
        "                                  classes=self.classes)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.metadata)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        \"\"\"\n",
        "        Using __eq__ is much faster than comparing __hash__ values\n",
        "        (dependent on __str__ values of datasource + search_space which\n",
        "        potentially encapsulates a lot of operations)\n",
        "        \"\"\"\n",
        "        if not isinstance(other, VisionTask):\n",
        "            return False\n",
        "\n",
        "        return self.metadata == other.metadata\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaFATou4R1-M"
      },
      "source": [
        "### Task Manager\n",
        "\n",
        "<!---  \n",
        "$file=task_manager.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8tcdZk-6WKQ"
      },
      "outputs": [],
      "source": [
        "class TaskManager:\n",
        "\n",
        "    def __init__(self, task_list=[]):\n",
        "        self.tasks = task_list\n",
        "\n",
        "\n",
        "    def add_task(self, task):\n",
        "        assert isinstance(task, BaseTask), (\n",
        "            'Invalid task provided. Please ensure that the argument is of type'\n",
        "            ' `BaseTask`'\n",
        "        )\n",
        "\n",
        "        # [Deprecated] - duplicate IDs are now valid for class-/domain-increment\n",
        "        # ids = [t.id for t in self.tasks]\n",
        "        # if task.id in ids:\n",
        "        #     Logger.warning(f'Task ID \"{task.id}\" was already used. Ignoring.')\n",
        "        #     return\n",
        "\n",
        "        self.tasks.append(task)\n",
        "\n",
        "\n",
        "    def fixed_scheduler(self):\n",
        "        for task in self.tasks:\n",
        "            yield task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawa8bt7YaPG"
      },
      "source": [
        "## Neural Architecture Search\n",
        "\n",
        "<!---  \n",
        "$module=neural_architecture_search\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3CVyKtoa9O"
      },
      "source": [
        "### NAS\n",
        "\n",
        "<!---  \n",
        "$file=nas.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdrv6QQlX37M"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NAS:\n",
        "\n",
        "    def __init__(self,\n",
        "                 search_algorithm,\n",
        "                 evaluation_strategy,\n",
        "                 visualize_candidates=True,\n",
        "                 **kwargs):\n",
        "\n",
        "        self.sys_metrics = SystemMetrics()\n",
        "        self.visualize_candidates = visualize_candidates\n",
        "\n",
        "        # SEARCH ALGORITHM INIT\n",
        "        self.optimizer = search_algorithm\n",
        "\n",
        "        # EVALUATION STRATEGY INIT\n",
        "        self.evaluator = evaluation_strategy\n",
        "\n",
        "\n",
        "    def optimize_candidates(self, models, nas_epoch, task):\n",
        "        for model in models:\n",
        "            # WL hash\n",
        "            hash = model.wl_hash\n",
        "\n",
        "            # visualize model pre-training (computationally expensive,\n",
        "            # consider using only when debugging)\n",
        "            if self.visualize_candidates:\n",
        "                model.visualize(dir='./plots',\n",
        "                                show_plot=True)\n",
        "\n",
        "            # log architecture and parameters' info pre-training\n",
        "            Logger.info(f'\\nEvaluating Arch {nas_epoch+1}/'\n",
        "                        f'{task.nas_epochs} \"{hash}\"')\n",
        "            Logger.info(f'\\nModel: {model.metadata.pretty_print()}')\n",
        "            Logger.info(f'{model.learnable_params} learnable parameters out of '\n",
        "                        f'{model.total_params} total parameters')\n",
        "\n",
        "            # evaluate candidate (model training/validation)\n",
        "            self.evaluator.optimize(model, task)\n",
        "\n",
        "            # candidate evaluation complete\n",
        "            Logger.success(f'Completed candidate \"{hash}\" evaluation')\n",
        "            Logger.separator()\n",
        "\n",
        "            # pass evaluation feedback to optimizer\n",
        "            self.optimizer.add_results(model.metrics.aggregate())\n",
        "\n",
        "\n",
        "    def free_mem(self):\n",
        "        \"\"\"\n",
        "        This should be the only point of garbage collection as\n",
        "        :func:`gc.collect` can be computationally expensive. `del` calls\n",
        "        throughout the NAS runs will be freed here\n",
        "        \"\"\"\n",
        "        gc.collect()\n",
        "\n",
        "        if self.evaluator.device.type != 'cpu':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    def reproduce(self, serialized_graph):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        model = Network.deserialize(serialized_graph)\n",
        "        self.evaluate(model, 0)\n",
        "\n",
        "\n",
        "    def run(self, task, dir='./nas_results/'):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(task, BaseTask), (\n",
        "            'Assigned task must be of type `BaseTask`. '\n",
        "            f'Type {type(task)} was given instead.'\n",
        "        )\n",
        "\n",
        "        # save task details prior to runnning NAS in case of abortion\n",
        "        # intermediary model data and training logs are saved during evaluation\n",
        "        task.save()\n",
        "\n",
        "        # assign given task to the optimizer\n",
        "        self.optimizer.assign_task(task)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # NAS Loop\n",
        "        # if a minimum threshold is set for a task, the loop is\n",
        "        # infinite until the minimum is met, otherwise stop at `nas_epochs`\n",
        "        nas_epoch = 0\n",
        "        while True:\n",
        "            # sample candidate(s) from the search space\n",
        "            models = self.optimizer.sample()\n",
        "            if models is None or not len(models):\n",
        "                # could not find a valid architecture after a number of\n",
        "                # attempts\n",
        "                continue\n",
        "\n",
        "            # train sampled network\n",
        "            self.optimize_candidates(models, nas_epoch, task)\n",
        "\n",
        "            # top candidate selection\n",
        "            self.optimizer.candidate_selection(models=models,\n",
        "                                               tasks=[task])\n",
        "\n",
        "            # housekeeping; gc triggered at the end of every NAS epoch only\n",
        "            # as it is inefficient if initiated more frequently\n",
        "            self.free_mem()\n",
        "\n",
        "            # perform NAS break checks\n",
        "            nas_epoch += 1\n",
        "\n",
        "            metrics = self.optimizer.top_metrics.records[-1]    # last eval\n",
        "\n",
        "            if nas_epoch >= task.nas_epochs \\\n",
        "            and task.objective.min_threshold_met(metrics):\n",
        "                break\n",
        "\n",
        "            if task.objective.target_threshold_met(metrics):\n",
        "                Logger.success('Target threshold reached!')\n",
        "                break\n",
        "\n",
        "            if nas_epoch >= task.nas_epochs:\n",
        "                # NAS epochs reached but minimum not yet met; issue warning\n",
        "                Logger.warning(\n",
        "                    f'Minimum threshold for task {task.id} v.{task.version} is '\n",
        "                    'not met! Overriding the given NAS epochs\\' count'\n",
        "                )\n",
        "\n",
        "        # NAS loop complete, save results\n",
        "        self.optimizer.metrics.save(filename='results.csv',\n",
        "                                    dir=dir)\n",
        "\n",
        "        self.sys_metrics.add_record(task_id=task.id,\n",
        "                                    task_version=task.version,\n",
        "                                    task_name=task.name,\n",
        "                                    nas_epoch=nas_epoch,\n",
        "                                    sys_usage=get_system_usage())\n",
        "\n",
        "        self.sys_metrics.save(filename='sys_usage.csv')\n",
        "\n",
        "        # NAS optimization complete\n",
        "        top_train_acc = max(self.optimizer.top_metrics['train_avg_acc'])\n",
        "        top_val_acc = max(self.optimizer.top_metrics['val_avg_acc'])\n",
        "        Logger.success(f'NAS optimization for task: {str(task.id)} '\n",
        "                    f'v.{task.version} | '\n",
        "                    f'{str(task.name)} is complete! Top1 train_acc: '\n",
        "                    f'{top_train_acc}, Top1 val_acc: {top_val_acc}')\n",
        "        Logger.separator()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv4MmPCwodCE"
      },
      "source": [
        "### CNAS\n",
        "\n",
        "<!---  \n",
        "$file=cnas.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGA_Bze9oDXR"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ContinualNAS(NAS):\n",
        "\n",
        "    def __init__(self,\n",
        "                 search_algorithm,\n",
        "                 evaluation_strategy,\n",
        "                 visualize_candidates=True):\n",
        "\n",
        "        self.__encountered_tasks = []\n",
        "\n",
        "        super(ContinualNAS, self).__init__(search_algorithm,\n",
        "                                           evaluation_strategy,\n",
        "                                           visualize_candidates)\n",
        "\n",
        "    def run(self, task_manager, dir='./nas_results/'):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(task_manager, TaskManager), (\n",
        "            'Assigned `task_manager` must be of type `TaskManager`. '\n",
        "            f'Type {type(task)} was given instead.'\n",
        "        )\n",
        "        assert isinstance(self.optimizer, CLOptimizerProtocol), (\n",
        "            'The assigned Search Algorithm must conform to '\n",
        "            '`CLOptimizerProtocol` in order to run NAS with Continual '\n",
        "            'Learning capabilities'\n",
        "        )\n",
        "        assert isinstance(self.evaluator, CLEvaluatorProtocol), (\n",
        "            'The assigned Search Algorithm must conform to '\n",
        "            '`CLEvaluatorProtocol` in order to run NAS with Continual '\n",
        "            'Learning capabilities'\n",
        "        )\n",
        "\n",
        "        for task in task_manager.fixed_scheduler():\n",
        "\n",
        "            # save task details prior to runnning NAS in case of abortion\n",
        "            # intermediary model data and training logs are saved during\n",
        "            # evaluation\n",
        "            task.save()\n",
        "            self.__encountered_tasks.append(task)\n",
        "\n",
        "            Logger.info((\n",
        "                f'Initiating Task {task.id} v.{task.version} - {task.name} '\n",
        "                f'({str(task.metadata)})'\n",
        "            ))\n",
        "\n",
        "            # ------------------------------------------------------------------\n",
        "            # ASSIGN AND FIT NEW TASK\n",
        "\n",
        "            # Assign new task & check if fine-tuning is required (class/domain-\n",
        "            # incremental scenarios; if new classes are added or task-boundary\n",
        "            # has shifted)\n",
        "            self.optimizer.assign_task(task)\n",
        "            top_candidate, should_finetune = self.optimizer.fit()\n",
        "\n",
        "            if should_finetune:\n",
        "                Logger.progress('Task fitting successful, fine-tuning...')\n",
        "                # state_dict = top_candidate.state_dict()     # preserve weights\n",
        "\n",
        "                # fine-tune model for the current task\n",
        "                self.evaluator.fine_tune(top_candidate, task)\n",
        "\n",
        "                # assess the fine-tuning to see if NAS changes are needed\n",
        "                nas_needed = False\n",
        "                for e_task in self.__encountered_tasks:\n",
        "                    metrics = self.evaluator.evaluate(top_candidate, e_task)\n",
        "                    if not e_task.objective.min_threshold_met(metrics):\n",
        "                        nas_needed = True\n",
        "                        break\n",
        "\n",
        "                if not nas_needed:\n",
        "                    # move on to the next task\n",
        "                    # del state_dict    # [no longer instantiated]\n",
        "                    Logger.success(f'{task.id} v.{task.version} - \"{task.name}\"'\n",
        "                                   f' is complete (fine-tuned)!')\n",
        "                    Logger.separator()\n",
        "\n",
        "                    f_name = f'{task.id}-{task.version}_results.csv'\n",
        "                    self.optimizer.metrics.save(filename=f_name,\n",
        "                                                dir=dir)\n",
        "\n",
        "                    self.sys_metrics.add_record(task_id=task.id,\n",
        "                                                task_version=task.version,\n",
        "                                                task_name=task.name,\n",
        "                                                nas_epoch=nas_epoch,\n",
        "                                                sys_usage=get_system_usage())\n",
        "                    self.sys_metrics.save(filename='sys_usage.csv')\n",
        "\n",
        "                    continue\n",
        "\n",
        "            # ------------------------------------------------------------------\n",
        "            # NAS LOOP\n",
        "\n",
        "            # if a minimum threshold is set for a task, the loop is infinite\n",
        "            # until the minimum is met, otherwise stop at `nas_epochs`\n",
        "            nas_epoch = 0\n",
        "            while True:\n",
        "\n",
        "                # --------------------------------------------------------------\n",
        "                # SAMPLE CANDIDATE(S)\n",
        "\n",
        "                models = []\n",
        "                if top_candidate is not None:\n",
        "                    # augment candidate(s) to fit a new task\n",
        "\n",
        "                    # extensions are for class-/domain-incremental scenarios\n",
        "                    # where fine-tuning was not sufficient\n",
        "                    models = self.optimizer.augment(base_model=top_candidate)\n",
        "                else:\n",
        "                    # initial task optimization (sample from scratch)\n",
        "                    models = self.optimizer.sample()\n",
        "\n",
        "                if models is None or not len(models):\n",
        "                    # could not find a valid architecture after a number of\n",
        "                    # attempts\n",
        "                    continue\n",
        "\n",
        "                # --------------------------------------------------------------\n",
        "                # EVALUATE CANDIDATE(S)\n",
        "\n",
        "                # train sampled network\n",
        "                self.optimize_candidates(models, nas_epoch, task)\n",
        "\n",
        "                # top candidate selection\n",
        "                self.optimizer.candidate_selection(models=models,\n",
        "                                                   tasks=self.\\\n",
        "                                                   __encountered_tasks)\n",
        "\n",
        "                # housekeeping; gc triggered at the end of every NAS epoch only\n",
        "                # as it is inefficient if initiated more frequently\n",
        "                self.free_mem()\n",
        "\n",
        "\n",
        "                # --------------------------------------------------------------\n",
        "                # SAVE RESULTS\n",
        "\n",
        "                f_name = f'{task.id}-{task.version}_results.csv'\n",
        "                self.optimizer.metrics.save(filename=f_name,\n",
        "                                            dir=dir)\n",
        "\n",
        "                self.sys_metrics.add_record(task_id=task.id,\n",
        "                                            task_version=task.version,\n",
        "                                            task_name=task.name,\n",
        "                                            nas_epoch=nas_epoch,\n",
        "                                            sys_usage=get_system_usage())\n",
        "                self.sys_metrics.save(filename='sys_usage.csv')\n",
        "\n",
        "\n",
        "                # --------------------------------------------------------------\n",
        "                # ASSESS THRESHOLDS\n",
        "\n",
        "                # perform NAS break checks\n",
        "                nas_epoch += 1\n",
        "\n",
        "                metrics = self.optimizer.top_metrics.records[-1]    # last eval\n",
        "\n",
        "                if task.objective.target_threshold_met(metrics):\n",
        "                    Logger.success('Target threshold reached!')\n",
        "                    break\n",
        "\n",
        "                if nas_epoch >= task.nas_epochs:\n",
        "                    minimum_is_met = True\n",
        "                    # check if the minimum threshold is met across all tasks\n",
        "                    for t in self.__encountered_tasks:\n",
        "                        # Logger.debug(str(t))\n",
        "                        # Logger.debug(metrics)\n",
        "                        if not t.objective.min_threshold_met(metrics):\n",
        "                            minimum_is_met = False\n",
        "                            # NAS epochs reached but minimum thresholds not yet\n",
        "                            # met; issue warning\n",
        "                            Logger.warning(\n",
        "                                f'Minimum threshold for task {t.id} is not '\n",
        "                                'met! Overriding the given NAS epochs\\' count'\n",
        "                            )\n",
        "                            # break     # skip breaking -> warn about all tasks\n",
        "\n",
        "                    if minimum_is_met:\n",
        "                        # minimum threshold is met and NAS epochs reached\n",
        "                        break\n",
        "\n",
        "\n",
        "            # ------------------------------------------------------------------\n",
        "            # END OF NAS\n",
        "\n",
        "            # Reset XAI parameters\n",
        "            if self.evaluator.xai is not None:\n",
        "                self.evaluator.xai.reset()\n",
        "\n",
        "            # NAS optimization complete\n",
        "            top_train_acc = max(self.optimizer.top_metrics['train_avg_acc'])\n",
        "            top_val_acc = max(self.optimizer.top_metrics['val_avg_acc'])\n",
        "            Logger.success(f'NAS optimization for task: {str(task.id)} '\n",
        "                        f'v.{task.version} '\n",
        "                        f'| {str(task.name)} is complete! Top1 train_acc: '\n",
        "                        f'{top_train_acc}, Top1 val_acc: {top_val_acc}')\n",
        "            Logger.separator()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goPbp_Gzoh7F"
      },
      "source": [
        "## Main\n",
        "\n",
        "<!---  \n",
        "$ignore-cell=True\n",
        "$root-level=True\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBeq1SZEprXi"
      },
      "outputs": [],
      "source": [
        "if Config.MOUNT_GDRIVE:\n",
        "    # gate GDrive mounting unless manually specified\n",
        "    from google.colab import files, drive, runtime\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "    if Config.EXPERIMENT_NAME:\n",
        "        Config.BASE_PATH = os.path.join(Config.BASE_PATH,\n",
        "                                        Config.EXPERIMENT_NAME)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy-ows0NCrzk"
      },
      "source": [
        "### Run NAS\n",
        "\n",
        "<!---  \n",
        "$file=run_nas.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1um6XuECxqd"
      },
      "outputs": [],
      "source": [
        "\n",
        "OPERATIONS = [\n",
        "    Conv2d,\n",
        "    SepConv2d,\n",
        "    DilatedConv2d,\n",
        "    Identity,\n",
        "\n",
        "    MaxPool2d,\n",
        "    AvgPool2d,\n",
        "    # GlobalAvgPool2d, # flattens the spatial dimensions (keeps the channel\n",
        "                       # dim), hence invalidating most operations it precedes\n",
        "    TransformChannels,\n",
        "    ReduceResolution,\n",
        "    StridedConv2d,\n",
        "    StridedSepConv,\n",
        "\n",
        "    BatchNormalization,\n",
        "    LayerNormalization,\n",
        "    GroupNormalization,\n",
        "    InstanceNormalization,\n",
        "    Dropout,\n",
        "\n",
        "    ReLU,\n",
        "    Swish,\n",
        "    HSwish,\n",
        "    LeakyReLU,\n",
        "\n",
        "    # ConvBnReluBlock,\n",
        "    # ResidualBlock,\n",
        "    # InceptionBlock\n",
        "]\n",
        "\n",
        "config = Params.get_args()\n",
        "\n",
        "set_reproducible(random_seed=42)\n",
        "\n",
        "\n",
        "# Dataset\n",
        "cifar10_dataset = VisionDataSource(\n",
        "    path='./cifar10',\n",
        "    dataset=VisionDataSource.Dataset.CIFAR10,\n",
        "    transform=cifar10_transforms()\n",
        ")\n",
        "cifar10_dataset.load()\n",
        "\n",
        "mnist_dataset = VisionDataSource(\n",
        "    path='./mnist',\n",
        "    dataset=VisionDataSource.Dataset.MNIST,\n",
        "    transform=mnist_transforms()\n",
        ")\n",
        "mnist_dataset.load()\n",
        "\n",
        "\n",
        "# Search Space\n",
        "ss = LWSearchSpace(\n",
        "    num_vertices=7,\n",
        "    operations=OPERATIONS,\n",
        "    encoding='multi-branch'\n",
        ")\n",
        "\n",
        "\n",
        "# Tasks\n",
        "cifar10_task = VisionTask(\n",
        "    id=0,\n",
        "    version=0,\n",
        "    name='cifar10',\n",
        "    datasource=cifar10_dataset,\n",
        "    candidate_epochs=5,\n",
        "    nas_epochs=3,\n",
        "    search_space=ss\n",
        ")\n",
        "\n",
        "mnist_obj = ICObjective()\n",
        "mnist_obj.add_criterion(metric=ICObjective.Metric.VAL_ACC,\n",
        "                        min_threshold=0.6,\n",
        "                        target_threshold=0.9,\n",
        "                        thresholds_enabled=True)\n",
        "mnist_task = VisionTask(\n",
        "    id=1,\n",
        "    version=0,\n",
        "    name='mnist',\n",
        "    datasource=mnist_dataset,\n",
        "    candidate_epochs=3,\n",
        "    nas_epochs=10,\n",
        "    search_space=ss,\n",
        "    objective=mnist_obj,\n",
        "    callbacks=[\n",
        "        AdaptiveCutoffThreshold()\n",
        "    ]\n",
        ")\n",
        "input_shape, output_shape = cifar10_task.shapes\n",
        "Logger.info(f'Shapes: {input_shape} | {output_shape}')\n",
        "\n",
        "# Optimizer\n",
        "search_algorithm = RandomSearch()\n",
        "\n",
        "\n",
        "# XAI Interpreter\n",
        "xai_interpreter = DeepTaylorDecomposition(false_pred_count=32,\n",
        "                                          true_pred_count=32)\n",
        "\n",
        "\n",
        "# Evaluation Strategy\n",
        "evaluation_strategy = ImageClassificationEvaluator(\n",
        "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    save_training_logs=True,\n",
        "    verbose=True,\n",
        "    xai_interpreter=xai_interpreter\n",
        ")\n",
        "\n",
        "\n",
        "# Neural Architecture Search\n",
        "nas = NAS(search_algorithm=search_algorithm,\n",
        "          evaluation_strategy=evaluation_strategy)\n",
        "\n",
        "try:\n",
        "    nas.run(task=mnist_task)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    raise e\n",
        "finally:\n",
        "    runtime.unassign()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFt-6516o4LY"
      },
      "source": [
        "### Run CNAS\n",
        "\n",
        "<!---  \n",
        "$file=run_cnas.py\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UJQf-H5lo3rc",
        "outputId": "8a94192c-a269-403f-ba35-b15a9dba67e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4585742.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 133613.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:06<00:00, 243122.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5554090.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
            "\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:18<00:00, 9230245.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10/\n",
            "Files already downloaded and verified\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./cifar100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:18<00:00, 9134778.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar100/cifar-100-python.tar.gz to ./cifar100/\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m[26/06 09:44:34:892] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: __convert_dataset_to_img): Converting compressed dataset to png format\n",
            "\u001b[34m[26/06 09:44:34:934] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: __convert_dataset_to_img): This may take a while...\n",
            "\u001b[34m[26/06 09:44:54:054] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: __convert_dataset_to_img): Successfully converted dataset! Saved to `./cifar100/train`\n",
            "\u001b[34m[26/06 09:44:54:094] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: __convert_dataset_to_img): Converting compressed dataset to png format\n",
            "\u001b[34m[26/06 09:44:54:136] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: __convert_dataset_to_img): This may take a while...\n",
            "\u001b[34m[26/06 09:44:57:966] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: __convert_dataset_to_img): Successfully converted dataset! Saved to `./cifar100/val`\n",
            "\u001b[34m[26/06 09:44:58:040] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (0; cifar10) - Shapes: (128, 3, 32, 32) | (128, 10)\n",
            "\u001b[34m[26/06 09:44:58:070] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (1; mnist) - Shapes: (128, 1, 28, 28) | (128, 10)\n",
            "\u001b[34m[26/06 09:44:58:115] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_0) - Shapes: (128, 3, 32, 32) | (128, 10)\n",
            "\u001b[34m[26/06 09:44:58:173] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_1) - Shapes: (128, 3, 32, 32) | (128, 20)\n",
            "\u001b[34m[26/06 09:44:58:247] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_2) - Shapes: (128, 3, 32, 32) | (128, 30)\n",
            "\u001b[34m[26/06 09:44:58:339] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_3) - Shapes: (128, 3, 32, 32) | (128, 40)\n",
            "\u001b[34m[26/06 09:44:58:444] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_4) - Shapes: (128, 3, 32, 32) | (128, 50)\n",
            "\u001b[34m[26/06 09:44:58:564] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_5) - Shapes: (128, 3, 32, 32) | (128, 60)\n",
            "\u001b[34m[26/06 09:44:58:699] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_6) - Shapes: (128, 3, 32, 32) | (128, 70)\n",
            "\u001b[34m[26/06 09:44:58:847] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_7) - Shapes: (128, 3, 32, 32) | (128, 80)\n",
            "\u001b[34m[26/06 09:44:59:016] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_8) - Shapes: (128, 3, 32, 32) | (128, 90)\n",
            "\u001b[34m[26/06 09:44:59:194] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: visiontaskfactory): Task (2; cifar100_9) - Shapes: (128, 3, 32, 32) | (128, 100)\n",
            "\u001b[34m[26/06 09:44:59:273] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: run): Initiating Task 0 v.0 - cifar10 (type=VisionTask,id=0,version=0,name=cifar10,modality=Image Classification,objectives_metadata=[type=ICObjective,metric_key=val_avg_acc,polarity=1,score_weight=0.8,thresholds_enabled=False,min_threshold=0.6,target_threshold=0.9, type=ICObjective,metric_key=train_acc_conv_rate,polarity=1,score_weight=0.2,thresholds_enabled=False,min_threshold=-1.0,target_threshold=1.0],search_space_metadata=type=LWSearchSpace,num_vertices=6,encoding=multi-branch,operations_metadata=None,datasource_metadata=path=./cifar10/,segment_size=10,segment_idx=0,num_workers=4,transforms=['ToTensor()', 'Normalize(mean=(0.49139968, 0.48215841, 0.44653091), std=(0.24703223, 0.24348513, 0.26158784))'],dataset=CIFAR10,train_batch_size=128,val_batch_size=128,learning_rate=0.001,nas_epochs=3,candidate_epochs=2,in_shape=(128, 3, 32, 32),out_shape=(128, 10),classes=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m[26/06 09:45:00:226] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: optimize_candidates): \n",
            "Evaluating Arch 1/3 \"be9035feb34c8b5ce719fbef7411c5be\"\n",
            "\u001b[34m[26/06 09:45:00:266] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: optimize_candidates): \n",
            "Model: {\n",
            "    \"adj_matrix\": \"[[0, 1, 0, 1, 1], [0, 0, 1, 1, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]]\",\n",
            "    \"id\": \"0\",\n",
            "    \"learnable_params\": \"64426\",\n",
            "    \"mflops\": \"55.918592\",\n",
            "    \"model_hash\": \"703554390493850431\",\n",
            "    \"nodes\": \"[\\\"(InputStem(op=InputStem,id=0,_is_partial=False), {'signature': 'op=InputStem,_is_partial=False'})\\\", \\\"(Conv2d(op=Conv2d,id=1,in_shape=(128, 3, 32, 32),out_shape=(128, 64, 32, 32),filter_count=64,kernel_size=11,_is_partial=False), {'signature': 'op=Conv2d,in_shape=(128, 3, 32, 32),out_shape=(128, 64, 32, 32),filter_count=64,kernel_size=11,_is_partial=False'})\\\", \\\"(StridedConv2d(op=StridedConv2d,id=2,in_shape=(128, 64, 32, 32),out_shape=(128, 32, 15, 15),filter_count=32,kernel_size=3,stride=2,_is_partial=False), {'signature': 'op=StridedConv2d,in_shape=(128, 64, 32, 32),out_shape=(128, 32, 15, 15),filter_count=32,kernel_size=3,stride=2,_is_partial=False'})\\\", \\\"(ReLU(op=ReLU,id=3,in_shape=(128, 67, 32, 32),out_shape=(128, 67, 32, 32),_is_partial=False), {'signature': 'op=ReLU,in_shape=(128, 67, 32, 32),out_shape=(128, 67, 32, 32),_is_partial=False'})\\\", \\\"(OutputStem(op=OutputStem,id=4,_is_partial=False), {'signature': 'op=OutputStem,_is_partial=False'})\\\"]\",\n",
            "    \"serialized_graph\": \"{\\\"mflops\\\": 55.918592, \\\"adj_matrix\\\": [[0, 1, 0, 1, 1], [0, 0, 1, 1, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]], \\\"nodes\\\": [\\\"(InputStem(op=InputStem,id=0,_is_partial=False), {'signature': 'op=InputStem,_is_partial=False'})\\\", \\\"(Conv2d(op=Conv2d,id=1,in_shape=(128, 3, 32, 32),out_shape=(128, 64, 32, 32),filter_count=64,kernel_size=11,_is_partial=False), {'signature': 'op=Conv2d,in_shape=(128, 3, 32, 32),out_shape=(128, 64, 32, 32),filter_count=64,kernel_size=11,_is_partial=False'})\\\", \\\"(StridedConv2d(op=StridedConv2d,id=2,in_shape=(128, 64, 32, 32),out_shape=(128, 32, 15, 15),filter_count=32,kernel_size=3,stride=2,_is_partial=False), {'signature': 'op=StridedConv2d,in_shape=(128, 64, 32, 32),out_shape=(128, 32, 15, 15),filter_count=32,kernel_size=3,stride=2,_is_partial=False'})\\\", \\\"(ReLU(op=ReLU,id=3,in_shape=(128, 67, 32, 32),out_shape=(128, 67, 32, 32),_is_partial=False), {'signature': 'op=ReLU,in_shape=(128, 67, 32, 32),out_shape=(128, 67, 32, 32),_is_partial=False'})\\\", \\\"(OutputStem(op=OutputStem,id=4,_is_partial=False), {'signature': 'op=OutputStem,_is_partial=False'})\\\"], \\\"task_map\\\": {\\\"0\\\": [[[0, 1, 0, 1, 1], [0, 0, 1, 1, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]], [0, 1, 2, 3, 4], {\\\"1\\\": [0], \\\"3\\\": [0, 1], \\\"4\\\": [0, 1, 2, 3], \\\"2\\\": [1]}]}, \\\"tasks_metadata\\\": [{\\\"type\\\": \\\"VisionTask\\\", \\\"id\\\": 0, \\\"version\\\": 0, \\\"name\\\": \\\"cifar10\\\", \\\"modality\\\": \\\"Image Classification\\\", \\\"objectives_metadata\\\": [{\\\"type\\\": \\\"ICObjective\\\", \\\"metric_key\\\": \\\"val_avg_acc\\\", \\\"polarity\\\": 1, \\\"score_weight\\\": 0.8, \\\"thresholds_enabled\\\": false, \\\"min_threshold\\\": 0.6, \\\"target_threshold\\\": 0.9}, {\\\"type\\\": \\\"ICObjective\\\", \\\"metric_key\\\": \\\"train_acc_conv_rate\\\", \\\"polarity\\\": 1, \\\"score_weight\\\": 0.2, \\\"thresholds_enabled\\\": false, \\\"min_threshold\\\": -1.0, \\\"target_threshold\\\": 1.0}], \\\"search_space_metadata\\\": {\\\"type\\\": \\\"LWSearchSpace\\\", \\\"num_vertices\\\": 6, \\\"encoding\\\": \\\"multi-branch\\\", \\\"operations_metadata\\\": null}, \\\"datasource_metadata\\\": {\\\"path\\\": \\\"./cifar10/\\\", \\\"segment_size\\\": 10, \\\"segment_idx\\\": 0, \\\"num_workers\\\": 4, \\\"transforms\\\": [\\\"ToTensor()\\\", \\\"Normalize(mean=(0.49139968, 0.48215841, 0.44653091), std=(0.24703223, 0.24348513, 0.26158784))\\\"], \\\"dataset\\\": \\\"CIFAR10\\\"}, \\\"train_batch_size\\\": 128, \\\"val_batch_size\\\": 128, \\\"learning_rate\\\": 0.001, \\\"nas_epochs\\\": 3, \\\"candidate_epochs\\\": 2, \\\"in_shape\\\": [128, 3, 32, 32], \\\"out_shape\\\": [128, 10], \\\"classes\\\": [\\\"airplane\\\", \\\"automobile\\\", \\\"bird\\\", \\\"cat\\\", \\\"deer\\\", \\\"dog\\\", \\\"frog\\\", \\\"horse\\\", \\\"ship\\\", \\\"truck\\\"]}]}\",\n",
            "    \"task_map\": \"{0: ([[0, 1, 0, 1, 1], [0, 0, 1, 1, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]], [0, 1, 2, 3, 4], {1: [0], 3: [0, 1], 4: [0, 1, 2, 3], 2: [1]})}\",\n",
            "    \"tasks_metadata\": \"[type=VisionTask,id=0,version=0,name=cifar10,modality=Image Classification,objectives_metadata=[type=ICObjective,metric_key=val_avg_acc,polarity=1,score_weight=0.8,thresholds_enabled=False,min_threshold=0.6,target_threshold=0.9, type=ICObjective,metric_key=train_acc_conv_rate,polarity=1,score_weight=0.2,thresholds_enabled=False,min_threshold=-1.0,target_threshold=1.0],search_space_metadata=type=LWSearchSpace,num_vertices=6,encoding=multi-branch,operations_metadata=None,datasource_metadata=path=./cifar10/,segment_size=10,segment_idx=0,num_workers=4,transforms=['ToTensor()', 'Normalize(mean=(0.49139968, 0.48215841, 0.44653091), std=(0.24703223, 0.24348513, 0.26158784))'],dataset=CIFAR10,train_batch_size=128,val_batch_size=128,learning_rate=0.001,nas_epochs=3,candidate_epochs=2,in_shape=(128, 3, 32, 32),out_shape=(128, 10),classes=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']]\",\n",
            "    \"total_params\": \"64426\",\n",
            "    \"version\": \"1\",\n",
            "    \"wl_hash\": \"be9035feb34c8b5ce719fbef7411c5be\"\n",
            "}\n",
            "\u001b[34m[26/06 09:45:00:301] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: optimize_candidates): 64426 learnable parameters out of 64426 total parameters\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m[26/06 09:45:18.236] \u001b[1mPROGRESS\u001b[0m\u001b[0m: Task (0 v.0: \"cifar10\") | Epoch 1/2, Batch 391/391 - Loss: 1.8848798759758014, Acc.: 0.30934\n",
            "\u001b[36m[26/06 09:45:21.221] \u001b[1mPROGRESS\u001b[0m\u001b[0m: Task (0 v.0: \"cifar10\") | Epoch 1/2, Batch 79/79 - Loss: 1.729546693306935, Acc.: 0.3697\n",
            "\u001b[36m[26/06 09:45:37.157] \u001b[1mPROGRESS\u001b[0m\u001b[0m: Task (0 v.0: \"cifar10\") | Epoch 2/2, Batch 391/391 - Loss: 1.709366231623208, Acc.: 0.37836\n",
            "\u001b[36m[26/06 09:45:40.026] \u001b[1mPROGRESS\u001b[0m\u001b[0m: Task (0 v.0: \"cifar10\") | Epoch 2/2, Batch 79/79 - Loss: 1.6659172335757484, Acc.: 0.3888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m[26/06 09:45:40.100] \u001b[1mPROGRESS\u001b[0m\u001b[0m: XAI: DTD -- Scores {3: -4.669689417369227e-07, 2: -6.343586846924154e-06, 1: -3.1698089060228085e-06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mCompleted candidate \"be9035feb34c8b5ce719fbef7411c5be\" evaluation\u001b[0m\n",
            "\n",
            "\u001b[32m▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m[26/06 09:45:41:280] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: optimize_candidates): \n",
            "Evaluating Arch 2/3 \"1f3553bce3e7051abbeb8fe95f3b0eb6\"\n",
            "\u001b[34m[26/06 09:45:41:317] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: optimize_candidates): \n",
            "Model: {\n",
            "    \"adj_matrix\": \"[[0, 1, 1, 1, 0, 1], [0, 0, 1, 1, 0, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0]]\",\n",
            "    \"id\": \"1\",\n",
            "    \"learnable_params\": \"44584\",\n",
            "    \"mflops\": \"1818.581832\",\n",
            "    \"model_hash\": \"4667960581227275240\",\n",
            "    \"nodes\": \"[\\\"(InputStem(op=InputStem,id=0,_is_partial=False), {'signature': 'op=InputStem,_is_partial=False'})\\\", \\\"(SepConv2d(op=SepConv2d,id=1,in_shape=(128, 3, 32, 32),out_shape=(128, 64, 32, 32),filter_count=64,kernel_size=3,_is_partial=False), {'signature': 'op=SepConv2d,in_shape=(128, 3, 32, 32),out_shape=(128, 64, 32, 32),filter_count=64,kernel_size=3,_is_partial=False'})\\\", \\\"(AvgPool2d(op=AvgPool2d,id=2,in_shape=(128, 67, 32, 32),out_shape=(128, 67, 15, 15),kernel_size=3,stride=2,padding=0,_is_partial=False), {'signature': 'op=AvgPool2d,in_shape=(128, 67, 32, 32),out_shape=(128, 67, 15, 15),kernel_size=3,stride=2,padding=0,_is_partial=False'})\\\", \\\"(Dropout(op=Dropout,id=3,in_shape=(128, 134, 32, 32),out_shape=(128, 134, 32, 32),p=0.25,_is_partial=False), {'signature': 'op=Dropout,in_shape=(128, 134, 32, 32),out_shape=(128, 134, 32, 32),p=0.25,_is_partial=False'})\\\", \\\"(Identity(op=Identity,id=4,in_shape=(128, 67, 15, 15),out_shape=(128, 67, 15, 15),_is_partial=False), {'signature': 'op=Identity,in_shape=(128, 67, 15, 15),out_shape=(128, 67, 15, 15),_is_partial=False'})\\\", \\\"(OutputStem(op=OutputStem,id=5,_is_partial=False), {'signature': 'op=OutputStem,_is_partial=False'})\\\"]\",\n",
            "    \"serialized_graph\": \"{\\\"mflops\\\": 1818.581832, \\\"adj_matrix\\\": [[0, 1, 1, 1, 0, 1], [0, 0, 1, 1, 0, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0]], \\\"nodes\\\": [\\\"(InputStem(op=InputStem,id=0,_is_partial=False), {'signature': 'op=InputStem,_is_partial=False'})\\\", \\\"(SepConv2d(op=SepConv2d,id=1,in_shape=(128, 3, 32, 32),out_shape=(128, 64, 32, 32),filter_count=64,kernel_size=3,_is_partial=False), {'signature': 'op=SepConv2d,in_shape=(128, 3, 32, 32),out_shape=(128, 64, 32, 32),filter_count=64,kernel_size=3,_is_partial=False'})\\\", \\\"(AvgPool2d(op=AvgPool2d,id=2,in_shape=(128, 67, 32, 32),out_shape=(128, 67, 15, 15),kernel_size=3,stride=2,padding=0,_is_partial=False), {'signature': 'op=AvgPool2d,in_shape=(128, 67, 32, 32),out_shape=(128, 67, 15, 15),kernel_size=3,stride=2,padding=0,_is_partial=False'})\\\", \\\"(Dropout(op=Dropout,id=3,in_shape=(128, 134, 32, 32),out_shape=(128, 134, 32, 32),p=0.25,_is_partial=False), {'signature': 'op=Dropout,in_shape=(128, 134, 32, 32),out_shape=(128, 134, 32, 32),p=0.25,_is_partial=False'})\\\", \\\"(Identity(op=Identity,id=4,in_shape=(128, 67, 15, 15),out_shape=(128, 67, 15, 15),_is_partial=False), {'signature': 'op=Identity,in_shape=(128, 67, 15, 15),out_shape=(128, 67, 15, 15),_is_partial=False'})\\\", \\\"(OutputStem(op=OutputStem,id=5,_is_partial=False), {'signature': 'op=OutputStem,_is_partial=False'})\\\"], \\\"task_map\\\": {\\\"0\\\": [[[0, 1, 1, 1, 0, 1], [0, 0, 1, 1, 0, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0]], [0, 1, 2, 3, 4, 5], {\\\"1\\\": [0], \\\"2\\\": [0, 1], \\\"3\\\": [0, 1, 2], \\\"5\\\": [0, 1, 2, 3, 4], \\\"4\\\": [2]}]}, \\\"tasks_metadata\\\": [{\\\"type\\\": \\\"VisionTask\\\", \\\"id\\\": 0, \\\"version\\\": 0, \\\"name\\\": \\\"cifar10\\\", \\\"modality\\\": \\\"Image Classification\\\", \\\"objectives_metadata\\\": [{\\\"type\\\": \\\"ICObjective\\\", \\\"metric_key\\\": \\\"val_avg_acc\\\", \\\"polarity\\\": 1, \\\"score_weight\\\": 0.8, \\\"thresholds_enabled\\\": false, \\\"min_threshold\\\": 0.6, \\\"target_threshold\\\": 0.9}, {\\\"type\\\": \\\"ICObjective\\\", \\\"metric_key\\\": \\\"train_acc_conv_rate\\\", \\\"polarity\\\": 1, \\\"score_weight\\\": 0.2, \\\"thresholds_enabled\\\": false, \\\"min_threshold\\\": -1.0, \\\"target_threshold\\\": 1.0}], \\\"search_space_metadata\\\": {\\\"type\\\": \\\"LWSearchSpace\\\", \\\"num_vertices\\\": 6, \\\"encoding\\\": \\\"multi-branch\\\", \\\"operations_metadata\\\": null}, \\\"datasource_metadata\\\": {\\\"path\\\": \\\"./cifar10/\\\", \\\"segment_size\\\": 10, \\\"segment_idx\\\": 0, \\\"num_workers\\\": 4, \\\"transforms\\\": [\\\"ToTensor()\\\", \\\"Normalize(mean=(0.49139968, 0.48215841, 0.44653091), std=(0.24703223, 0.24348513, 0.26158784))\\\"], \\\"dataset\\\": \\\"CIFAR10\\\"}, \\\"train_batch_size\\\": 128, \\\"val_batch_size\\\": 128, \\\"learning_rate\\\": 0.001, \\\"nas_epochs\\\": 3, \\\"candidate_epochs\\\": 2, \\\"in_shape\\\": [128, 3, 32, 32], \\\"out_shape\\\": [128, 10], \\\"classes\\\": [\\\"airplane\\\", \\\"automobile\\\", \\\"bird\\\", \\\"cat\\\", \\\"deer\\\", \\\"dog\\\", \\\"frog\\\", \\\"horse\\\", \\\"ship\\\", \\\"truck\\\"]}]}\",\n",
            "    \"task_map\": \"{0: ([[0, 1, 1, 1, 0, 1], [0, 0, 1, 1, 0, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0]], [0, 1, 2, 3, 4, 5], {1: [0], 2: [0, 1], 3: [0, 1, 2], 5: [0, 1, 2, 3, 4], 4: [2]})}\",\n",
            "    \"tasks_metadata\": \"[type=VisionTask,id=0,version=0,name=cifar10,modality=Image Classification,objectives_metadata=[type=ICObjective,metric_key=val_avg_acc,polarity=1,score_weight=0.8,thresholds_enabled=False,min_threshold=0.6,target_threshold=0.9, type=ICObjective,metric_key=train_acc_conv_rate,polarity=1,score_weight=0.2,thresholds_enabled=False,min_threshold=-1.0,target_threshold=1.0],search_space_metadata=type=LWSearchSpace,num_vertices=6,encoding=multi-branch,operations_metadata=None,datasource_metadata=path=./cifar10/,segment_size=10,segment_idx=0,num_workers=4,transforms=['ToTensor()', 'Normalize(mean=(0.49139968, 0.48215841, 0.44653091), std=(0.24703223, 0.24348513, 0.26158784))'],dataset=CIFAR10,train_batch_size=128,val_batch_size=128,learning_rate=0.001,nas_epochs=3,candidate_epochs=2,in_shape=(128, 3, 32, 32),out_shape=(128, 10),classes=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']]\",\n",
            "    \"total_params\": \"44584\",\n",
            "    \"version\": \"1\",\n",
            "    \"wl_hash\": \"1f3553bce3e7051abbeb8fe95f3b0eb6\"\n",
            "}\n",
            "\u001b[34m[26/06 09:45:41:350] \u001b[1mINFO\u001b[0m\u001b[0m \u001b[3m(caller: optimize_candidates): 44584 learnable parameters out of 44584 total parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m[26/06 09:46:00.575] \u001b[1mPROGRESS\u001b[0m\u001b[0m: Task (0 v.0: \"cifar10\") | Epoch 1/2, Batch 391/391 - Loss: 2.100776663826555, Acc.: 0.21136\n",
            "\u001b[36m[26/06 09:46:03.931] \u001b[1mPROGRESS\u001b[0m\u001b[0m: Task (0 v.0: \"cifar10\") | Epoch 1/2, Batch 79/79 - Loss: 2.0647481860993784, Acc.: 0.2245\n",
            "\u001b[36m[26/06 09:46:22.580] \u001b[1mPROGRESS\u001b[0m\u001b[0m: Task (0 v.0: \"cifar10\") | Epoch 2/2, Batch 382/391 - Loss: 2.0153920208401694, Acc.: 0.23194126308900523"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-237fa543b539>\u001b[0m in \u001b[0;36m<cell line: 148>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mnas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-f3ba9e31b2a8>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, task_manager, dir)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# train sampled network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnas_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# top candidate selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-2641499e1756>\u001b[0m in \u001b[0;36moptimize_candidates\u001b[0;34m(self, models, nas_epoch, task)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# evaluate candidate (model training/validation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# candidate evaluation complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-4269611560ac>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, model, task, fine_tune, dir)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-4269611560ac>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, model, epoch, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;31m# calculate/update metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 batch_res = self._calculate_batch_metrics(split='train',\n\u001b[0m\u001b[1;32m    136\u001b[0m                                                           \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                                                           \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-4269611560ac>\u001b[0m in \u001b[0;36m_calculate_batch_metrics\u001b[0;34m(self, split, loss, outputs, labels, n_batches, top_k)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'running_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "OPERATIONS = [\n",
        "    Conv2d,\n",
        "    SepConv2d,\n",
        "    DilatedConv2d,\n",
        "    Identity,\n",
        "\n",
        "    MaxPool2d,\n",
        "    AvgPool2d,\n",
        "    # GlobalAvgPool2d, # flattens the spatial dimensions (keeps the channel\n",
        "                       # dim), hence invalidating most operations it precedes\n",
        "    TransformChannels,\n",
        "    ReduceResolution,\n",
        "    StridedConv2d,\n",
        "    StridedSepConv,\n",
        "\n",
        "    BatchNormalization,\n",
        "    LayerNormalization,\n",
        "    GroupNormalization,\n",
        "    InstanceNormalization,\n",
        "    Dropout,\n",
        "\n",
        "    ReLU,\n",
        "    Swish,\n",
        "    HSwish,\n",
        "    LeakyReLU,\n",
        "\n",
        "    # ConvBnReluBlock,\n",
        "    # ResidualBlock,\n",
        "    # InceptionBlock\n",
        "]\n",
        "\n",
        "config = Params.get_args()\n",
        "\n",
        "# set_reproducible(random_seed=42)\n",
        "\n",
        "# Datasets\n",
        "mnist_dataset = VisionDataSource(path='./mnist/',\n",
        "                                 dataset=VisionDataSource.Dataset.MNIST,\n",
        "                                 transform=mnist_transforms(),\n",
        "                                 autoload=True)\n",
        "mnist_obj = ICObjective()\n",
        "mnist_obj.add_criterion(metric=ICObjective.Metric.VAL_ACC,\n",
        "                        min_threshold=0.6,\n",
        "                        target_threshold=0.9,\n",
        "                        thresholds_enabled=False,\n",
        "                        score_weight=0.7)   # score weights will be normalized;\n",
        "                                            # they are just relative to each\n",
        "                                            # other\n",
        "mnist_obj.add_criterion(metric=ICObjective.Metric.MODEL_SIZE,\n",
        "                        score_weight=0.1)\n",
        "mnist_obj.add_criterion(metric=ICObjective.Metric.TRAIN_ACC_CONV,\n",
        "                        score_weight=0.2)\n",
        "\n",
        "cifar10_dataset = VisionDataSource(path='./cifar10/',\n",
        "                                   dataset=VisionDataSource.Dataset.CIFAR10,\n",
        "                                   transform=cifar10_transforms(),\n",
        "                                   autoload=True)\n",
        "cifar10_obj = ICObjective()\n",
        "cifar10_obj.add_criterion(metric=ICObjective.Metric.VAL_ACC,\n",
        "                          min_threshold=0.6,\n",
        "                          target_threshold=0.9,\n",
        "                          thresholds_enabled=False,\n",
        "                          score_weight=0.8)\n",
        "cifar10_obj.add_criterion(metric=ICObjective.Metric.TRAIN_ACC_CONV,\n",
        "                          score_weight=0.2,\n",
        "                          thresholds_enabled=False)\n",
        "\n",
        "VisionDataSource.download_cifar100(path='./cifar100/',\n",
        "                                   force_overwrite=False,\n",
        "                                   allow_segmentation=True)\n",
        "\n",
        "# Search Space\n",
        "ss = LWSearchSpace(\n",
        "    num_vertices=6,\n",
        "    operations=OPERATIONS,\n",
        "    encoding='multi-branch',\n",
        ")\n",
        "\n",
        "ss.register_sampling_hook('spatial_ops_boost', spatial_ops_boost_hook)\n",
        "\n",
        "# Tasks\n",
        "def visiontaskfactory(id, version, name, datasource, obj=None):\n",
        "    task = VisionTask(id=id, version=version, name=name, datasource=datasource,\n",
        "                      search_space=ss,\n",
        "                      nas_epochs=3, candidate_epochs=2, objective=obj,\n",
        "                      callbacks=[])\n",
        "    input_shape, output_shape = task.shapes\n",
        "\n",
        "    Logger.info(f'Task ({id}; {name}) - Shapes: {input_shape} | {output_shape}')\n",
        "\n",
        "    return task\n",
        "\n",
        "\n",
        "task_manager = TaskManager()\n",
        "task_manager.add_task(visiontaskfactory(0, 0, 'cifar10', cifar10_dataset,\n",
        "                                        cifar10_obj))\n",
        "task_manager.add_task(visiontaskfactory(1, 0, 'mnist', mnist_dataset,\n",
        "                                        mnist_obj))\n",
        "\n",
        "\n",
        "c100_ds = VisionDataSource.Dataset.CIFAR100\n",
        "for seg_idx in range(10):\n",
        "    # segment_size = 10\n",
        "    tns = cifar100_transforms()\n",
        "    seg_task = VisionDataSource.class_segmentation_factory('./cifar100',\n",
        "                                                           dataset=c100_ds,\n",
        "                                                           segment_size=10,\n",
        "                                                           segment_idx=seg_idx,\n",
        "                                                           transform=tns)\n",
        "\n",
        "    cifar100_obj = ICObjective()\n",
        "    cifar100_obj.add_criterion(metric=ICObjective.Metric.VAL_ACC,\n",
        "                               min_threshold=0.15,\n",
        "                               target_threshold=0.9,\n",
        "                               thresholds_enabled=True,\n",
        "                               score_weight=0.8)\n",
        "    cifar100_obj.add_criterion(metric=ICObjective.Metric.TRAIN_ACC_CONV,\n",
        "                               score_weight=0.1)\n",
        "    cifar100_obj.add_criterion(metric=ICObjective.Metric.COMP_PERF,\n",
        "                               score_weight=0.1)\n",
        "    task_manager.add_task(visiontaskfactory(2, seg_idx,\n",
        "                                            f'cifar100_{seg_idx}', seg_task,\n",
        "                                            cifar100_obj))\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "search_algorithm = RandomSearch()\n",
        "\n",
        "\n",
        "# XAI Interpreter\n",
        "xai_interpreter = DeepTaylorDecomposition(false_pred_count=32,\n",
        "                                          true_pred_count=32)\n",
        "\n",
        "\n",
        "# Evaluation Strategy\n",
        "evaluation_strategy = ImageClassificationEvaluator(\n",
        "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    save_training_logs=True,\n",
        "    verbose=True,\n",
        "    xai_interpreter=xai_interpreter\n",
        ")\n",
        "\n",
        "\n",
        "# Neural Architecture Search\n",
        "nas = ContinualNAS(search_algorithm=search_algorithm,\n",
        "                   evaluation_strategy=evaluation_strategy)\n",
        "\n",
        "try:\n",
        "    nas.run(task_manager=task_manager)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    raise e\n",
        "#finally:\n",
        "    # runtime.unassign()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PukRgOgokw6K"
      },
      "outputs": [],
      "source": [
        "nas.optimizer._best_candidate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BufTyqH5hVUH",
        "outputId": "2f0a01dd-a078-48be-c5bf-e1b8c2f5dfa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [2, 3], [3, 4, 6], [5, 6], [3, 5], [], [5]]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adj_matrix = np.array([[0, 1, 1, 1, 1, 0, 0], [0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 1, 1, 0, 1], [0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0]]);\n",
        "\n",
        "pred, succ = predecessor_successor_lists(adj_matrix)\n",
        "\n",
        "succ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGHGivXapqsF"
      },
      "source": [
        "## Collate Results\n",
        "\n",
        "<!---  \n",
        "$ignore-module=True\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro-Ylu2-Vcwr",
        "outputId": "f0569178-b3f0-4acb-81d5-b2c0bc4286c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/gdrive/MyDrive/PhD/NASKit/plots/ (stored 0%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/plots/model_1_v2.svg (deflated 79%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/plots/model_1_v1.svg (deflated 75%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/plots/model_0_v1.svg (deflated 75%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/ (stored 0%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/2900676297973850959.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-7843224659262453008.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-6305422713374061575.txt (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/7773903318826398693.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/727984449068491063.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/8272421828646875578.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-7672706893139725666.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/2892597892134494562.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/5382523853876667529.txt (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-3554645095639943724.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/3803518802104321190.txt (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-8660100710531261794.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-5698059951799188933.txt (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/5027606544715454066.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-7017528319913500831.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-7905695743861407380.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/3483362321972991473.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-7355255431593127284.txt (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/3657524623320052953.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/5362765708955278113.txt (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-4181285638118614876.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-928078371529506829.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-5157096887869663006.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-7811466165861169140.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/186700792040321004.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-7103144295778107269.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-671227291577171851.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/1850541646177882453.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/2025593368459283781.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-5411559479292748204.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/48741163131411745.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/3396664403846055038.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/4497338904805763947.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-7974659229229233165.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/5257501173376523259.txt (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/training_logs/-1350850652007784063.txt (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/ (stored 0%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/2900676297973850959.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/727984449068491063.csv (deflated 80%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-4181285638118614876.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/5382523853876667529.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/3803518802104321190.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-7843224659262453008.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/8272421828646875578.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-5698059951799188933.csv (deflated 75%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/1850541646177882453.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-6305422713374061575.csv (deflated 76%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-3554645095639943724.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/4497338904805763947.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/3657524623320052953.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/7773903318826398693.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-5411559479292748204.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-1350850652007784063.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/3396664403846055038.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-928078371529506829.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/2025593368459283781.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-8660100710531261794.csv (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-7672706893139725666.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-7103144295778107269.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/186700792040321004.csv (deflated 81%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/5027606544715454066.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-671227291577171851.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/2892597892134494562.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-5157096887869663006.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/3483362321972991473.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-7905695743861407380.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/5257501173376523259.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-7811466165861169140.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-7974659229229233165.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/48741163131411745.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/model_metrics/-7017528319913500831.csv (deflated 82%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/tasks/ (stored 0%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/tasks/task_1-mnist.json (deflated 39%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/tasks/task_0-cifar10.json (deflated 39%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/nas_results/ (stored 0%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/nas_results/1_results.csv (deflated 95%)\n",
            "  adding: content/gdrive/MyDrive/PhD/NASKit/nas_results/0_results.csv (deflated 84%)\n"
          ]
        }
      ],
      "source": [
        "!find /content -maxdepth 1 -type d \\( -name training_logs -o -name nas_results -o -name plots -o -name model_metrics -o -name tasks \\) | zip -r results.zip -@"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L99qUqW2pikO"
      },
      "source": [
        "## Scratchpad\n",
        "\n",
        "<!---  \n",
        "$ignore-module=True\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from scipy.stats import wasserstein_distance, entropy\n",
        "\n",
        "def dataset_embedding(dataset):\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "    print('loaded; prepping')\n",
        "    data = next(iter(loader))[0].numpy()  # Assuming the dataset tuples are (data, target)\n",
        "    print('reshape')\n",
        "    data = data.reshape(data.shape[0], -1)  # Flatten the data\n",
        "\n",
        "    print('mean + variance starting...')\n",
        "    # Calculate mean and variance\n",
        "    means = np.mean(data, axis=0)\n",
        "    variances = np.var(data, axis=0)\n",
        "    print('histograms starting...')\n",
        "    # Calculate histograms and Wasserstein distance (EMD)\n",
        "    hist_distances = []\n",
        "    for i in range(data.shape[1]):\n",
        "        hist1, _ = np.histogram(data[:, i], bins=10, range=(data[:, i].min(), data[:, i].max()))\n",
        "        hist2, _ = np.histogram(data[:, i], bins=10, range=(data[:, i].min(), data[:, i].max()))\n",
        "        hist_distances.append(wasserstein_distance(hist1, hist2))\n",
        "\n",
        "    print('\\n\\nhist', hist_distances)\n",
        "    # Calculate correlation matrix and its Frobenius norm\n",
        "    print('correlation matrix starting...')\n",
        "    correlation_matrix = np.corrcoef(data, rowvar=False)\n",
        "    print('\\n\\ncorr', correlation_matrix)\n",
        "    print('norm corr starting...')\n",
        "    norm_correlation = np.linalg.norm(correlation_matrix - np.eye(correlation_matrix.shape[0]))\n",
        "\n",
        "    print('\\n\\nnorm corr', norm_correlation)\n",
        "    # Combine all embeddings into a single vector\n",
        "\n",
        "    print('concat starting...')\n",
        "    embedding = np.concatenate((means, variances, hist_distances, [norm_correlation]))\n",
        "    return embedding\n",
        "\n",
        "# Example usage with a PyTorch dataset\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load a sample dataset (e.g., MNIST)\n",
        "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_embedding = dataset_embedding(mnist)\n",
        "print(\"Dataset Embedding:\", mnist_embedding)\n",
        "print(\"sum\", sum(mnist_embedding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3u_Phfjss35",
        "outputId": "01ec5546-0b66-452d-d9f3-95e2fb2ffea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded; prepping\n",
            "reshape\n",
            "mean + variance starting...\n",
            "histograms starting...\n",
            "\n",
            "\n",
            "hist [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "correlation matrix starting...\n",
            "\n",
            "\n",
            "corr [[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "norm corr starting...\n",
            "\n",
            "\n",
            "norm corr nan\n",
            "concat starting...\n",
            "Dataset Embedding: [ 0.  0.  0. ...  0.  0. nan]\n",
            "sum nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoBMfOOl7k8k"
      },
      "outputs": [],
      "source": [
        "# g = rs.search()\n",
        "# g.visualize(show_plot=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxexX-uRGDk0"
      },
      "outputs": [],
      "source": [
        "# l = [list(g.successors(node)) for node in g.nodes()]\n",
        "# l[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sVsF8SVHHHT"
      },
      "outputs": [],
      "source": [
        "# print(nx.shortest_path_length(g, list(g.nodes())[0]).values())\n",
        "# print([node.depth for node in g.nodes()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jSWYUB1cJvK"
      },
      "outputs": [],
      "source": [
        "# g.compile_network(rs.input_shape, rs.num_classes)\n",
        "# g.get_architecture()\n",
        "# Logger.debug(g.compiled_ops)\n",
        "# g.visualize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf9C-pGPMLAX"
      },
      "outputs": [],
      "source": [
        "# g = nx.DiGraph()\n",
        "\n",
        "# g.add_edges_from([[1, 2], [1,3], [2,4], [4, 5]])\n",
        "# nx.weisfeiler_lehman_graph_hash(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z6EKhwQ7mtR"
      },
      "outputs": [],
      "source": [
        "# arr = nx.adjacency_matrix(g).toarray().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CEsFIb_irvlk",
        "outputId": "394aba72-85a1-4410-e7f7-4fe0fe390138"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'03e1277d2b6d6ab299c74c2ec9252343'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# g2 = nx.DiGraph()\n",
        "\n",
        "# g2.add_nodes_from([(1, {'index': 3}), (2, {'index': 1}), (3, {'index': 4}), (4, {'index': 2}), (5, {'index': 5})])\n",
        "# g2.add_edges_from([[1, 2], [1,3], [2,4], [4, 5]])\n",
        "# nx.weisfeiler_lehman_graph_hash(g2, node_attr='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfCVeRAixIkl"
      },
      "outputs": [],
      "source": [
        "# g2 = nx.DiGraph()\n",
        "\n",
        "# g2.add_nodes_from([(1, {'index': 3}), (2, {'index': 4}), (3, {'index': 1}), (4, {'index': 2}), (5, {'index': 5})])\n",
        "# g2.add_edges_from([[1, 2], [1,3], [2,4], [4, 5]])\n",
        "# nx.weisfeiler_lehman_graph_hash(g2, node_attr='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2RnTD5s2wMP"
      },
      "outputs": [],
      "source": [
        "# g.operations[0].flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaKqrtEC6Eg2"
      },
      "outputs": [],
      "source": [
        "# traced_model = torch.jit.trace(g, torch.rand(1, 3, 32, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSnk1yvEAcD_"
      },
      "outputs": [],
      "source": [
        "# scripted_model = torch.jit.script(traced_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "144c9PiCDCoZ"
      },
      "outputs": [],
      "source": [
        "# torch.jit.save(scripted_model, \"scripted_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuLfxANFD29j"
      },
      "outputs": [],
      "source": [
        "# !pip install -q netron\n",
        "# !curl --output smartreply.zip https://storage.googleapis.com/download.tensorflow.org/models/tflite/smartreply_1.0_2017_11_01.zip\n",
        "# !unzip smartreply.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fid_iv56E4hG"
      },
      "outputs": [],
      "source": [
        "# import netron\n",
        "# import portpicker\n",
        "# from google.colab import output\n",
        "\n",
        "# port = portpicker.pick_unused_port()\n",
        "\n",
        "# # Read the model file and start the netron browser.\n",
        "# with output.temporary():\n",
        "#     netron.start()\n",
        "#     netron.start('scripted_model.pt', port, browse=False)\n",
        "\n",
        "# output.serve_kernel_port_as_iframe(port, height='800')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM0QjQoQFG0a"
      },
      "outputs": [],
      "source": [
        "# type(traced_model.graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7llEZKUUDrV"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# LRP RULES\n",
        "# \"\"\"\n",
        "\n",
        "# def lrp_conv2d(self, R, layer):\n",
        "#     # Get weights and activations from the layer\n",
        "#     weight = layer.weight.data\n",
        "#     activation = layer.input  # Assuming you saved the input to the layer during forward pass\n",
        "\n",
        "#     # Compute the Z values (activation * weight) and normalize them\n",
        "#     Z = F.conv2d(activation, weight, stride=layer.stride, padding=layer.padding, groups=layer.groups)\n",
        "#     Z += 1e-9  # To avoid division by zero\n",
        "#     S = R / Z  # Redistribution ratio\n",
        "\n",
        "#     # Compute the gradient with respect to the input\n",
        "#     C = torch.nn.grad.conv2d_input(activation.shape, weight, S, stride=layer.stride, padding=layer.padding, groups=layer.groups)\n",
        "#     return C\n",
        "\n",
        "\n",
        "# def lrp_maxpool2d(self, R, layer):\n",
        "#     # Get the activation from the layer\n",
        "#     activation = layer.input  # Assuming you saved the input to the layer during forward pass\n",
        "\n",
        "#     # Perform max pooling on the activation to find the max values\n",
        "#     max_pool = F.max_pool2d(activation, layer.kernel_size, layer.stride, layer.padding, return_indices=True)\n",
        "#     max_activations, indices = max_pool\n",
        "\n",
        "#     # Route relevance to the max locations\n",
        "#     relevance = torch.zeros_like(activation)\n",
        "#     relevance = relevance.scatter_(1, indices, R)\n",
        "\n",
        "#     return relevance\n",
        "\n",
        "\n",
        "# def lrp_batchnorm(self, R, layer):\n",
        "#     # Get the necessary parameters from the layer\n",
        "#     activation = layer.input  # Assuming you saved the input to the layer during forward pass\n",
        "#     mean = layer.running_mean\n",
        "#     var = layer.running_var\n",
        "#     weight = layer.weight\n",
        "#     bias = layer.bias\n",
        "\n",
        "#     # Normalize the input\n",
        "#     normalized_input = (activation - mean[None, :, None, None]) / torch.sqrt(var[None, :, None, None] + layer.eps)\n",
        "\n",
        "#     # Calculate the Z values\n",
        "#     Z = normalized_input * weight[None, :, None, None] + bias[None, :, None, None]\n",
        "#     Z += 1e-9  # To avoid division by zero\n",
        "#     S = R / Z  # Redistribution ratio\n",
        "\n",
        "#     # Propagate the relevance to the input\n",
        "#     C = S * normalized_input\n",
        "#     return C\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "toc_visible": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}